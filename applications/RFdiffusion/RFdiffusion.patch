diff --git a/config/inference/base.yaml b/config/inference/base.yaml
index fac858e..80ff699 100644
--- a/config/inference/base.yaml
+++ b/config/inference/base.yaml
@@ -21,6 +21,7 @@ inference:
   trb_save_ckpt_path: null
   schedule_directory_path: null
   model_directory_path: null
+  precision: null
 
 contigmap:
   contigs: null
diff --git a/env/SE3nv.yml b/env/SE3nv.yml
index a51bcce..ab28e9c 100644
--- a/env/SE3nv.yml
+++ b/env/SE3nv.yml
@@ -1,18 +1,16 @@
 name: SE3nv
 channels:
-  - defaults
   - conda-forge
-  - pytorch
-  - dglteam
-  - nvidia
 dependencies:
-  - python=3.9
-  - pytorch=1.9
-  - torchaudio
-  - torchvision
-  - cudatoolkit=11.1
-  - dgl-cuda11.1
-  - pip
+  - python=3.11.0
+  - pip=24.0
   - pip:
-    - hydra-core
-    - pyrsistent
+    - intel-extension-for-pytorch==2.2.0
+    - pandas==2.2.2
+    - torch==2.2.0
+    - hydra-core==1.3.2
+    - pyrsistent==0.20.0
+    - torchdata==0.7.1
+    - pydantic==2.7.1
+    - dgl==2.1.0
+    - numpy==1.26.0
diff --git a/protein.py b/protein.py
new file mode 100644
index 0000000..ebaf048
--- /dev/null
+++ b/protein.py
@@ -0,0 +1,412 @@
+#!/usr/bin/env python
+# Copyright 2025 Intel Corporation
+# SPDX-License-Identifier: MIT License
+# Author: Rahamathullah
+# Email shaikx.rahamathullah@intel.com
+
+# encoding: utf-8
+import os, sys
+import time
+import signal
+import argparse
+import random
+import string
+import json
+import numpy as np
+import matplotlib.pyplot as plt
+from string import ascii_uppercase, ascii_lowercase
+from rfdiffusion.inference.utils import parse_pdb
+from ColabDesign.colabdesign.rf.utils import get_ca, fix_contigs, fix_partial_contigs, fix_pdb, sym_it
+from colabdesign.shared.protein import pdb_to_string
+import subprocess
+def get_pdb(pdb_code=None):
+  print("----get_pdb")
+  if pdb_code is None or pdb_code == "":
+    upload_dict = files.upload()
+    pdb_string = upload_dict[list(upload_dict.keys())[0]]
+    with open("tmp.pdb","wb") as out: out.write(pdb_string)
+    return "tmp.pdb"
+  elif os.path.isfile(pdb_code):
+    return pdb_code
+  elif len(pdb_code) == 4:
+    if not os.path.isfile(f"{pdb_code}.pdb1"):
+      os.system(f"wget -qnc https://files.rcsb.org/download/{pdb_code}.pdb1.gz")
+      os.system(f"gunzip {pdb_code}.pdb1.gz")
+    return f"{pdb_code}.pdb1"
+  else:
+    os.system(f"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v3.pdb")
+    return f"AF-{pdb_code}-F1-model_v3.pdb"
+
+def run_ananas(pdb_str, path, sym=None):
+  print("----run_ananas")
+  pdb_filename = f"outputs/{path}/ananas_input.pdb"
+  out_filename = f"outputs/{path}/ananas.json"
+  with open(pdb_filename,"w") as handle:
+    handle.write(pdb_str)
+
+  cmd = f"./ananas {pdb_filename} -u -j {out_filename}"
+  if sym is None: os.system(cmd)
+  else: os.system(f"{cmd} {sym}")
+
+  # parse results
+  try:
+    out = json.loads(open(out_filename,"r").read())
+    results,AU = out[0], out[-1]["AU"]
+    group = AU["group"]
+    chains = AU["chain names"]
+    rmsd = results["Average_RMSD"]
+    print(f"AnAnaS detected {group} symmetry at RMSD:{rmsd:.3}")
+
+    C = np.array(results['transforms'][0]['CENTER'])
+    A = [np.array(t["AXIS"]) for t in results['transforms']]
+
+    # apply symmetry and filter to the asymmetric unit
+    new_lines = []
+    for line in pdb_str.split("\n"):
+      if line.startswith("ATOM"):
+        chain = line[21:22]
+        if chain in chains:
+          x = np.array([float(line[i:(i+8)]) for i in [30,38,46]])
+          if group[0] == "c":
+            x = sym_it(x,C,A[0])
+          if group[0] == "d":
+            x = sym_it(x,C,A[1],A[0])
+          coord_str = "".join(["{:8.3f}".format(a) for a in x])
+          new_lines.append(line[:30]+coord_str+line[54:])
+      else:
+        new_lines.append(line)
+    return results, "\n".join(new_lines)
+
+  except:
+    return None, pdb_str
+
+"""def run(command, steps, num_designs=1): #, visual="none"):
+
+  #os.system(f'{command} > /dev/null ')
+  #return 1, 1
+  print("-run")
+  def run_command_and_get_pid(command):
+    print("-------run_command_and_get_pid")
+    pid_file = '/dev/shm/pid'
+    os.system(f'nohup {command} > /dev/null & echo $! > {pid_file}')
+    with open(pid_file, 'r') as f:
+      pid = int(f.read().strip())
+    os.remove(pid_file)
+  #  pid = process.pid
+    return pid
+  def is_process_running(pid):
+    try:
+      os.kill(pid, 0)
+    except OSError:
+      return False
+    else:
+      return True
+
+  # clear previous run
+  for n in range(steps):
+    if os.path.isfile(f"/dev/shm/{n}.pdb"):
+      os.remove(f"/dev/shm/{n}.pdb")
+
+  pid = run_command_and_get_pid(command)
+  try:
+    fail = False
+    for _ in range(num_designs):
+
+      # for each step check if output generated
+      for n in range(steps):
+        wait = True
+        while wait and not fail:
+          time.sleep(0.1)
+          if os.path.isfile(f"/dev/shm/{n}.pdb"):
+            pdb_str = open(f"/dev/shm/{n}.pdb").read()
+            if pdb_str[-3:] == "TER":
+              wait = False
+            elif not is_process_running(pid):
+              print('elif not')
+              fail = True
+          elif not is_process_running(pid):
+            print('elif not-2')
+            fail = True
+    while is_process_running(pid):
+      time.sleep(0.1)
+  except Exception as e:
+    print(f"Exception in run(): {e}")
+    try:
+      process.kill()
+    except:
+      pass
+  except KeyboardInterrupt:
+    os.kill(pid, signal.SIGTERM)
+    #progress.bar_style = 'danger'
+    #progress.description = "stopped"""
+############## external code #####################
+""" def run(command, steps, num_designs=1, visual="none"):
+
+  def run_command_and_get_pid(command):
+    print("Launching process")
+    process = subprocess.Popen(command, shell=True)
+    return process.pid, process
+
+  def is_process_running(process):
+    try:
+      os.kill(pid, 0)
+    except OSError:
+      return False
+    else:
+      return True
+
+  # Clear previous run
+  for n in range(steps):
+    if os.path.isfile(f"/dev/shm/{n}.pdb"):
+      os.remove(f"/dev/shm/{n}.pdb")
+
+  pid, process = run_command_and_get_pid(command)
+  try:
+    fail = False
+    for _ in range(num_designs):
+      for n in range(steps):
+        wait = True
+        while wait and not fail:
+          time.sleep(0.1)
+          if os.path.isfile(f"/dev/shm/{n}.pdb"):
+            with open(f"/dev/shm/{n}.pdb") as f:
+              pdb_str = f.read()
+              if pdb_str.strip().endswith("TER"):
+                wait = False
+              elif not is_process_running(pid):
+                fail = True
+          elif not is_process_running(pid):
+            fail = True
+
+    # Wait for process to end completely before exiting
+    process.wait()
+
+  except KeyboardInterrupt:
+    process.terminate()
+    print("Terminated by user.")"""
+
+def run(command, steps, num_designs=1, visual="none"):
+
+  def run_command_and_get_process(command):
+    return subprocess.Popen(command, shell=True)
+
+  def is_process_running(process):
+    return process.poll() is None
+
+  # Clear previous outputs
+  for n in range(steps):
+    pdb_path = f"/dev/shm/{n}.pdb"
+    if os.path.exists(pdb_path):
+      os.remove(pdb_path)
+
+  process = run_command_and_get_process(command)
+
+  try:
+    fail = False
+    for _ in range(num_designs):
+      for n in range(steps):
+        wait = True
+        while wait and not fail:
+          time.sleep(0.1)
+          pdb_path = f"/dev/shm/{n}.pdb"
+          if os.path.isfile(pdb_path):
+            with open(pdb_path) as f:
+              pdb_str = f.read()
+            if pdb_str.strip().endswith("TER"):
+              wait = False
+            elif not is_process_running(process):
+              fail = True
+          elif not is_process_running(process):
+            fail = True
+
+    # Wait for process to finish fully
+    process.wait()
+
+  except KeyboardInterrupt:
+    print("Interrupted. Terminating process.")
+    process.terminate()
+    process.wait()
+
+def run_diffusion(contigs, path,pdb=None, iterations=50,
+                  symmetry="none", order=1, hotspot=None,
+                  chains=None, add_potential=False,
+                  num_designs=1): #, visual="none"):
+
+  full_path = f"outputs/{path}"
+  #print("-----run_diffusion")
+  #full_path = os.path.join(path)
+  print('full_path_1', full_path)
+  os.makedirs(full_path, exist_ok=True)
+  opts = [f"inference.output_prefix={full_path}",
+          f"inference.num_designs={num_designs}"]
+  if chains == "": chains = None
+  # determine symmetry type
+  if symmetry in ["auto","cyclic","dihedral"]:
+    if symmetry == "auto":
+      sym, copies = None, 1
+    else:
+      sym, copies = {"cyclic":(f"c{order}",order),
+                     "dihedral":(f"d{order}",order*2)}[symmetry]
+  else:
+    symmetry = None
+    sym, copies = None, 1
+
+  # determine mode
+  contigs = contigs.replace(","," ").replace(":"," ").split()
+  is_fixed, is_free = False, False
+  fixed_chains = []
+  for contig in contigs:
+    for x in contig.split("/"):
+      a = x.split("-")[0]
+      if a[0].isalpha():
+        is_fixed = True
+        if a[0] not in fixed_chains:
+          fixed_chains.append(a[0])
+      if a.isnumeric():
+        is_free = True
+  if len(contigs) == 0 or not is_free:
+    mode = "partial"
+  elif is_fixed:
+    mode = "fixed"
+  else:
+    mode = "free"
+
+  # fix input contigs
+  if mode in ["partial","fixed"]:
+    pdb_str = pdb_to_string(get_pdb(pdb), chains=chains)
+    if symmetry == "auto":
+      a, pdb_str = run_ananas(pdb_str, path)
+      if a is None:
+        print(f'ERROR: no symmetry detected')
+        symmetry = None
+        sym, copies = None, 1
+      else:
+        if a["group"][0] == "c":
+          symmetry = "cyclic"
+          sym, copies = a["group"], int(a["group"][1:])
+        elif a["group"][0] == "d":
+          symmetry = "dihedral"
+          sym, copies = a["group"], 2 * int(a["group"][1:])
+        else:
+          print(f'ERROR: the detected symmetry ({a["group"]}) not currently supported')
+          symmetry = None
+          sym, copies = None, 1
+
+    elif mode == "fixed":
+      pdb_str = pdb_to_string(pdb_str, chains=fixed_chains)
+
+    pdb_filename = f"{full_path}/input.pdb"
+    with open(pdb_filename, "w") as handle:
+      handle.write(pdb_str)
+
+    parsed_pdb = parse_pdb(pdb_filename)
+    opts.append(f"inference.input_pdb={pdb_filename}")
+    if mode in ["partial"]:
+      iterations = int(80 * (iterations / 200))
+      opts.append(f"diffuser.partial_T={iterations}")
+      contigs = fix_partial_contigs(contigs, parsed_pdb)
+    else:
+      opts.append(f"diffuser.T={iterations}")
+      contigs = fix_contigs(contigs, parsed_pdb)
+  else:
+    opts.append(f"diffuser.T={iterations}")
+    parsed_pdb = None
+    contigs = fix_contigs(contigs, parsed_pdb)
+
+  if hotspot is not None and hotspot != "":
+    opts.append(f"ppi.hotspot_res=[{hotspot}]")
+
+  # setup symmetry
+  if sym is not None:
+    sym_opts = ["--config-name symmetry", f"inference.symmetry={sym}"]
+    if add_potential:
+      sym_opts += ["'potentials.guiding_potentials=[\"type:olig_contacts,weight_intra:1,weight_inter:0.1\"]'",
+                   "potentials.olig_intra_all=True","potentials.olig_inter_all=True",
+                   "potentials.guide_scale=2","potentials.guide_decay=quadratic"]
+    opts = sym_opts + opts
+    contigs = sum([contigs] * copies,[])
+
+  opts.append(f"'contigmap.contigs=[{' '.join(contigs)}]'")
+  opts += ["+inference.dump_pdb=True","+inference.dump_pdb_path='/dev/shm'"]
+
+  print("mode:", mode)
+  print("output:", full_path)
+  print("contigs:", contigs)
+
+  final_contigs_file = os.path.join(full_path, "final_contigs.txt")
+  """with open(final_contigs_file, 'w') as f:
+    for c in contigs:
+      f.write(f"'{c}'\n")"""
+  with open(final_contigs_file, 'w') as f:
+    if isinstance(contigs, list):
+        if len(contigs) == 1:
+            # Single contig string, write as-is without quotes
+            f.write(f"{contigs[0]}\n")
+        else:
+            # Multiple contigs, join with comma and space
+            f.write(", ".join(contigs) + "\n")
+    else:
+        # In case contigs is not a list (fallback)
+        f.write(str(contigs) + "\n")
+
+  opts_str = " ".join(opts)
+  cmd = f"./scripts/run_inference.py {opts_str}"
+  print(cmd)
+
+  # RUN
+  #print("-----before run")
+  run(cmd, iterations, num_designs) #, visual=visual)
+  #print("------after run")
+  # fix pdbs
+  for n in range(num_designs):
+    pdbs = [f"{full_path}_{n}.pdb"]
+    for pdb in pdbs:
+        with open(pdb, "r") as handle:
+          pdb_str = handle.read()
+        with open(pdb, "w") as handle:
+          pdb_str = handle.write(fix_pdb(pdb_str, contigs))
+  print('final rfdiffusion output generated')
+  return contigs, copies
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--name", type=str, required=True)
+    parser.add_argument("--pdb", type=str, required=True)
+    #parser.add_argument("--path", type=str, required=True)
+    parser.add_argument("--contigs", type=str, required=True)
+    parser.add_argument("--iterations", type=int, default=50)
+    parser.add_argument("--hotspot", type=str)
+    parser.add_argument("--num_designs", type=int, default=1)
+    #parser.add_argument("--visual", type=str, default="none")
+    parser.add_argument("--symmetry", type=str, default="none")
+    parser.add_argument("--order", type=int, default=1)
+    parser.add_argument("--chains", type=str)
+    parser.add_argument("--add_potential", action="store_true")
+    o = parser.parse_args()
+
+    path = o.name
+    while os.path.exists(f"outputs/{path}_0.pdb"):
+        path = o.name + "_" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))
+
+    flags = {
+        "contigs": o.contigs,
+        "pdb": o.pdb,
+        "order": o.order,
+        "iterations": o.iterations,
+        "symmetry": o.symmetry,
+        "hotspot": o.hotspot,
+        "path": path,
+        "chains": o.chains,
+        "add_potential": o.add_potential,
+        "num_designs": o.num_designs,
+        #"visual": o.visual
+    }
+
+    for k, v in flags.items():
+        if isinstance(v, str):
+            flags[k] = v.replace("'", "").replace('"', '')
+
+    contigs, copies = run_diffusion(**flags)
+
+if __name__ == "__main__":
+    main()
diff --git a/rfdiffusion/inference/model_runners.py b/rfdiffusion/inference/model_runners.py
index 9a33d98..8fe54e7 100644
--- a/rfdiffusion/inference/model_runners.py
+++ b/rfdiffusion/inference/model_runners.py
@@ -14,6 +14,7 @@ import torch.nn.functional as nn
 from rfdiffusion import util
 from hydra.core.hydra_config import HydraConfig
 import os
+import intel_extension_for_pytorch as ipex
 
 from rfdiffusion.model_input_logger import pickle_function_call
 import sys
@@ -35,13 +36,13 @@ class Sampler:
         """
         self.initialized = False
         self.initialize(conf)
-        
+
     def initialize(self, conf: DictConfig) -> None:
         """
         Initialize sampler.
         Args:
             conf: Configuration
-        
+
         - Selects appropriate model from input
         - Assembles Config from model checkpoint and command line overrides
 
@@ -55,7 +56,7 @@ class Sampler:
 
         # Assign config to Sampler
         self._conf = conf
-
+        
         ################################
         ### Select Appropriate Model ###
         ################################
@@ -104,6 +105,12 @@ class Sampler:
             self.assemble_config_from_chk()
             # Now actually load the model weights into RF
             self.model = self.load_model()
+            if self._conf.inference.precision == "bfloat16":
+                dtype = torch.bfloat16
+                self.model = ipex.optimize(self.model, dtype =dtype)
+            else:
+                dtype = torch.float32
+                self.model = ipex.optimize(self.model, dtype= dtype)
         else:
             self.assemble_config_from_chk()
 
@@ -144,7 +151,7 @@ class Sampler:
             self.symmetry = None
 
         self.allatom = ComputeAllAtomCoords().to(self.device)
-        
+
         if self.inf_conf.input_pdb is None:
             # set default pdb
             script_dir=os.path.dirname(os.path.realpath(__file__))
@@ -161,7 +168,7 @@ class Sampler:
             self.t_step_input = int(self.diffuser_conf.partial_T)
         else:
             self.t_step_input = int(self.diffuser_conf.T)
-        
+
     @property
     def T(self):
         '''
@@ -191,7 +198,7 @@ class Sampler:
         Actions:
             - Replaces all -model and -diffuser items
             - Throws a warning if there are items in -model and -diffuser that aren't in the checkpoint
-        
+
         This throws an error if there is a flag in the checkpoint 'config_dict' that isn't in the inference config.
         This should ensure that whenever a feature is added in the training setup, it is accounted for in the inference script.
 
@@ -209,17 +216,17 @@ class Sampler:
                     self._conf[cat][key] = self.ckpt['config_dict'][cat][key]
                 except:
                     pass
-        
+
         # add overrides back in again
         for override in overrides:
             if override.split(".")[0] in ['model','diffuser','preprocess']:
-                print(f'WARNING: You are changing {override.split("=")[0]} from the value this model was trained with. Are you sure you know what you are doing?') 
+                print(f'WARNING: You are changing {override.split("=")[0]} from the value this model was trained with. Are you sure you know what you are doing?')
                 mytype = type(self._conf[override.split(".")[0]][override.split(".")[1].split("=")[0]])
                 self._conf[override.split(".")[0]][override.split(".")[1].split("=")[0]] = mytype(override.split("=")[1])
 
     def load_model(self):
         """Create RosettaFold model from preloaded checkpoint."""
-        
+
         # Read input dimensions from checkpoint.
         self.d_t1d=self._conf.preprocess.d_t1d
         self.d_t2d=self._conf.preprocess.d_t2d
@@ -228,6 +235,12 @@ class Sampler:
             pickle_dir = pickle_function_call(model, 'forward', 'inference')
             print(f'pickle_dir: {pickle_dir}')
         model = model.eval()
+        if self._conf.inference.precision == "bfloat16":
+            dtype = torch.bfloat16
+            model = ipex.optimize(model, dtype=dtype)
+        else:
+            dtype = torch.float32
+            model = ipex.optimize(model,dtype=dtype)
         self._log.info(f'Loading checkpoint.')
         model.load_state_dict(self.ckpt['model_state_dict'], strict=True)
         return model
@@ -253,15 +266,15 @@ class Sampler:
     def sample_init(self, return_forward_trajectory=False):
         """
         Initial features to start the sampling process.
-        
+
         Modify signature and function body for different initialization
         based on the config.
-        
+
         Returns:
             xt: Starting positions with a portion of them randomly sampled.
             seq_t: Starting sequence with a portion of them set to unknown.
         """
-        
+
         #######################
         ### Parse input pdb ###
         #######################
@@ -278,7 +291,7 @@ class Sampler:
         self.mappings = self.contig_map.get_mappings()
         self.mask_seq = torch.from_numpy(self.contig_map.inpaint_seq)[None,:]
         self.mask_str = torch.from_numpy(self.contig_map.inpaint_str)[None,:]
-        self.binderlen =  len(self.contig_map.inpaint)     
+        self.binderlen =  len(self.contig_map.inpaint)
 
         ####################
         ### Get Hotspots ###
@@ -310,7 +323,7 @@ class Sampler:
 
         self.diffusion_mask = self.mask_str
         self.chain_idx=['A' if i < self.binderlen else 'B' for i in range(L_mapped)]
-        
+
         ####################################
         ### Generate initial coordinates ###
         ####################################
@@ -338,7 +351,7 @@ class Sampler:
             atom_mask_mapped = torch.full((L_mapped, 27), False)
             atom_mask_mapped[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0]
 
-        # Diffuse the contig-mapped coordinates 
+        # Diffuse the contig-mapped coordinates
         if self.diffuser_conf.partial_T:
             assert self.diffuser_conf.partial_T <= self.diffuser_conf.T, "Partial_T must be less than T"
             self.t_step_input = int(self.diffuser_conf.partial_T)
@@ -352,10 +365,10 @@ class Sampler:
 
         seq_t = torch.full((1,L_mapped), 21).squeeze() # 21 is the mask token
         seq_t[contig_map.hal_idx0] = seq_orig[contig_map.ref_idx0]
-        
+
         # Unmask sequence if desired
         if self._conf.contigmap.provide_seq is not None:
-            seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()] 
+            seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()]
 
         seq_t[~self.mask_seq.squeeze()] = 21
         seq_t    = torch.nn.functional.one_hot(seq_t, num_classes=22).float() # [L,22]
@@ -379,7 +392,7 @@ class Sampler:
         if self.symmetry is not None:
             xt, seq_t = self.symmetry.apply_symmetry(xt, seq_t)
         self._log.info(f'Sequence init: {seq2chars(torch.argmax(seq_t, dim=-1))}')
-        
+
         self.msa_prev = None
         self.pair_prev = None
         self.state_prev = None
@@ -407,17 +420,17 @@ class Sampler:
         return xt, seq_t
 
     def _preprocess(self, seq, xyz_t, t, repack=False):
-        
+
         """
         Function to prepare inputs to diffusion model
-        
-            seq (L,22) one-hot sequence 
+
+            seq (L,22) one-hot sequence
 
             msa_masked (1,1,L,48)
 
             msa_full (1,1,L,25)
-        
-            xyz_t (L,14,3) template crds (diffused) 
+
+            xyz_t (L,14,3) template crds (diffused)
 
             t1d (1,L,28) this is the t1d before tacking on the chi angles:
                 - seq + unknown/mask (21)
@@ -427,7 +440,7 @@ class Sampler:
                 - contacting residues: for ppi. Target residues in contact with binder (1)
                 - empty feature (legacy) (1)
                 - ss (H, E, L, MASK) (4)
-            
+
             t2d (1, L, L, 45)
                 - last plane is block adjacency
     """
@@ -456,7 +469,7 @@ class Sampler:
 
         ###########
         ### t1d ###
-        ########### 
+        ###########
 
         # Here we need to go from one hot with 22 classes to one hot with 21 classes (last plane is missing token)
         t1d = torch.zeros((1,1,L,21))
@@ -466,9 +479,9 @@ class Sampler:
             if seqt1d[idx,21] == 1:
                 seqt1d[idx,20] = 1
                 seqt1d[idx,21] = 0
-        
+
         t1d[:,:,:,:21] = seqt1d[None,None,:,:21]
-        
+
 
         # Set timestep feature to 1 where diffusion mask is True, else 1-t/T
         timefeature = torch.zeros((L)).float()
@@ -477,7 +490,7 @@ class Sampler:
         timefeature = timefeature[None,None,...,None]
 
         t1d = torch.cat((t1d, timefeature), dim=-1).float()
-        
+
         #############
         ### xyz_t ###
         #############
@@ -493,8 +506,8 @@ class Sampler:
         ### t2d ###
         ###########
         t2d = xyz_to_t2d(xyz_t)
-        
-        ###########      
+
+        ###########
         ### idx ###
         ###########
         idx = torch.tensor(self.contig_map.rf)[None]
@@ -519,7 +532,7 @@ class Sampler:
         t1d = t1d.to(self.device)
         t2d = t2d.to(self.device)
         alpha_t = alpha_t.to(self.device)
-        
+
         ######################
         ### added_features ###
         ######################
@@ -541,7 +554,7 @@ class Sampler:
             t1d=torch.cat((t1d, torch.zeros_like(t1d[...,:1]), hotspot_tens[None,None,...,None].to(self.device)), dim=-1)
 
         return msa_masked, msa_full, seq[None], torch.squeeze(xyz_t, dim=0), idx, t1d, t2d, xyz_t, alpha_t
-        
+
     def sample_step(self, *, t, x_t, seq_init, final_step):
         '''Generate the next pose that the model should be supplied at timestep t-1.
 
@@ -550,7 +563,7 @@ class Sampler:
             seq_t (torch.tensor): (L,22) The sequence at the beginning of this timestep
             x_t (torch.tensor): (L,14,3) The residue positions at the beginning of this timestep
             seq_init (torch.tensor): (L,22) The initialized sequence used in updating the sequence.
-            
+
         Returns:
             px0: (L,14,3) The model's prediction of x0.
             x_t_1: (L,14,3) The updated positions of the next step.
@@ -587,14 +600,14 @@ class Sampler:
                                 return_infer=True,
                                 motif_mask=self.diffusion_mask.squeeze().to(self.device))
 
-        # prediction of X0 
+        # prediction of X0
         _, px0  = self.allatom(torch.argmax(seq_in, dim=-1), px0, alpha)
         px0    = px0.squeeze()[:,:14]
-        
+
         #####################
         ### Get next pose ###
         #####################
-        
+
         if t > final_step:
             seq_t_1 = nn.one_hot(seq_init,num_classes=22).to(self.device)
             x_t_1, px0 = self.denoiser.get_next_pose(
@@ -643,7 +656,7 @@ class SelfConditioning(Sampler):
         ##################################
         ######## Str Self Cond ###########
         ##################################
-        if (t < self.diffuser.T) and (t != self.diffuser_conf.partial_T):   
+        if (t < self.diffuser.T) and (t != self.diffuser_conf.partial_T):
             zeros = torch.zeros(B,1,L,24,3).float().to(xyz_t.device)
             xyz_t = torch.cat((self.prev_pred.unsqueeze(1),zeros), dim=-2) # [B,T,L,27,3]
             t2d_44   = xyz_to_t2d(xyz_t) # [B,T,L,L,44]
@@ -659,9 +672,16 @@ class SelfConditioning(Sampler):
         ####################
         ### Forward Pass ###
         ####################
+        if self._conf.inference.precision == "bfloat16":
+            dtype = torch.bfloat16
+            enable = True
+        else:
+            dtype = torch.float32
+            enable = False
 
         with torch.no_grad():
-            msa_prev, pair_prev, px0, state_prev, alpha, logits, plddt = self.model(msa_masked,
+            with torch.autocast(enabled = enable, dtype = dtype, device_type = "cpu"):
+                msa_prev, pair_prev, px0, state_prev, alpha, logits, plddt = self.model(msa_masked,
                                 msa_full,
                                 seq_in,
                                 xt_in,
@@ -675,7 +695,7 @@ class SelfConditioning(Sampler):
                                 state_prev = None,
                                 t=torch.tensor(t),
                                 return_infer=True,
-                                motif_mask=self.diffusion_mask.squeeze().to(self.device))   
+                                motif_mask=self.diffusion_mask.squeeze().to(self.device))
 
             if self.symmetry is not None and self.inf_conf.symmetric_self_cond:
                 px0 = self.symmetrise_prev_pred(px0=px0,seq_in=seq_in, alpha=alpha)[:,:,:3]
@@ -685,7 +705,7 @@ class SelfConditioning(Sampler):
         # prediction of X0
         _, px0  = self.allatom(torch.argmax(seq_in, dim=-1), px0, alpha)
         px0    = px0.squeeze()[:,:14]
-        
+
         ###########################
         ### Generate Next Input ###
         ###########################
@@ -725,7 +745,7 @@ class SelfConditioning(Sampler):
         return px0_sym
 
 class ScaffoldedSampler(SelfConditioning):
-    """ 
+    """
     Model Runner for Scaffold-Constrained diffusion
     """
     def __init__(self, conf: DictConfig):
@@ -764,7 +784,33 @@ class ScaffoldedSampler(SelfConditioning):
         else:
             self.target = None
             self.target_pdb=False
+    def initialize(self, conf: DictConfig):
+        super().initialize(conf)
+        # initialize BlockAdjacency sampling class
+        self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)
+
+
+        #################################################
+        ### Initialize target, if doing binder design ###
+        #################################################
 
+        if conf.scaffoldguided.target_pdb:
+            self.target = iu.Target(conf.scaffoldguided, conf.ppi.hotspot_res)
+            self.target_pdb = self.target.get_target()
+            if conf.scaffoldguided.target_ss is not None:
+                self.target_ss = torch.load(conf.scaffoldguided.target_ss).long()
+                self.target_ss = torch.nn.functional.one_hot(self.target_ss, num_classes=4)
+                if self._conf.scaffoldguided.contig_crop is not None:
+                    self.target_ss=self.target_ss[self.target_pdb['crop_mask']]
+            if conf.scaffoldguided.target_adj is not None:
+                self.target_adj = torch.load(conf.scaffoldguided.target_adj).long()
+                self.target_adj=torch.nn.functional.one_hot(self.target_adj, num_classes=3)
+                if self._conf.scaffoldguided.contig_crop is not None:
+                        self.target_adj=self.target_adj[self.target_pdb['crop_mask']]
+                        self.target_adj=self.target_adj[:,self.target_pdb['crop_mask']]
+        else:
+            self.target = None
+            self.target_pdb=False
     def sample_init(self):
         """
         Wrapper method for taking secondary structure + adj, and outputting xt, seq_t
@@ -778,9 +824,9 @@ class ScaffoldedSampler(SelfConditioning):
 
         ##############################
         ### Auto-contig generation ###
-        ##############################    
+        ##############################
 
-        if self.contig_conf.contigs is None: 
+        if self.contig_conf.contigs is None:
             # process target
             xT = torch.full((self.L, 27,3), np.nan)
             xT = get_init_xyz(xT[None,None]).squeeze()
@@ -808,7 +854,7 @@ class ScaffoldedSampler(SelfConditioning):
                 contig = []
                 for idx,i in enumerate(self.target_pdb['pdb_idx'][:-1]):
                     if idx==0:
-                        start=i[1]               
+                        start=i[1]
                     if i[1] + 1 != self.target_pdb['pdb_idx'][idx+1][1] or i[0] != self.target_pdb['pdb_idx'][idx+1][0]:
                         contig.append(f'{i[0]}{start}-{i[1]}/0 ')
                         start = self.target_pdb['pdb_idx'][idx+1][1]
@@ -851,18 +897,18 @@ class ScaffoldedSampler(SelfConditioning):
             assert L_mapped==self.adj.shape[0]
             diffusion_mask = self.mask_str
             self.diffusion_mask = diffusion_mask
-            
+
             xT = torch.full((1,1,L_mapped,27,3), np.nan)
             xT[:, :, contig_map.hal_idx0, ...] = xyz_27[contig_map.ref_idx0,...]
             xT = get_init_xyz(xT).squeeze()
             atom_mask = torch.full((L_mapped, 27), False)
             atom_mask[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0]
- 
+
         ####################
         ### Get hotspots ###
         ####################
         self.hotspot_0idx=iu.get_idx0_hotspots(self.mappings, self.ppi_conf, self.binderlen)
-        
+
         #########################
         ### Set up potentials ###
         #########################
@@ -905,17 +951,17 @@ class ScaffoldedSampler(SelfConditioning):
 
         xT = torch.clone(fa_stack[-1].squeeze()[:,:14,:])
         return xT, seq_T
-    
+
     def _preprocess(self, seq, xyz_t, t):
         msa_masked, msa_full, seq, xyz_prev, idx_pdb, t1d, t2d, xyz_t, alpha_t = super()._preprocess(seq, xyz_t, t, repack=False)
-        
+
         ###################################
         ### Add Adj/Secondary Structure ###
         ###################################
 
         assert self.preprocess_conf.d_t1d == 28, "The checkpoint you're using hasn't been trained with sec-struc/block adjacency features"
         assert self.preprocess_conf.d_t2d == 47, "The checkpoint you're using hasn't been trained with sec-struc/block adjacency features"
-       
+
         #####################
         ### Handle Target ###
         #####################
@@ -930,7 +976,7 @@ class ScaffoldedSampler(SelfConditioning):
         t1d=torch.cat((t1d, full_ss[None,None].to(self.device)), dim=-1)
 
         t1d = t1d.float()
-        
+
         ###########
         ### t2d ###
         ###########
diff --git a/rfdiffusion/inference/symmetry.py b/rfdiffusion/inference/symmetry.py
index 864a5ab..463363b 100644
--- a/rfdiffusion/inference/symmetry.py
+++ b/rfdiffusion/inference/symmetry.py
@@ -73,7 +73,7 @@ class SymGen:
             self.apply_symmetry = self._apply_octahedral
 
         elif global_sym.lower() in saved_symmetries:
-            # Using a saved symmetry 
+            # Using a saved symmetry
             self._log.info('Initializing %s symmetry order.'%global_sym)
             self._init_from_symrots_file(global_sym)
 
@@ -108,7 +108,7 @@ class SymGen:
             start_i = subunit_len * i
             end_i = subunit_len * (i+1)
             coords_out[start_i:end_i] = torch.einsum(
-                'bnj,kj->bnk', coords_out[:subunit_len], self.sym_rots[i])
+                'bnj,kj->bnk', coords_out[:subunit_len].to(dtype=torch.float32), self.sym_rots[i].to(dtype=torch.float32))
             seq_out[start_i:end_i]  = seq_out[:subunit_len]
         return coords_out, seq_out
 
@@ -174,7 +174,7 @@ class SymGen:
                 center = torch.mean(subunit_chain[:, 1, :], axis=0)
                 subunit_chain -= center[None, None, :]
                 rotated_axis = torch.einsum(
-                    'nj,kj->nk', base_axis, self.sym_rots[i]) 
+                    'nj,kj->nk', base_axis, self.sym_rots[i])
                 subunit_chain += rotated_axis[:, None, :]
 
             coords_out[start_i:end_i] = subunit_chain
@@ -185,7 +185,7 @@ class SymGen:
     ## symmetry from file #
     #######################
     def _init_from_symrots_file(self, name):
-        """ _init_from_symrots_file initializes using 
+        """ _init_from_symrots_file initializes using
         ./inference/sym_rots.npz
 
         Args:
@@ -203,11 +203,11 @@ class SymGen:
             if str(k) == name: symms = v
         assert symms is not None, "%s not found in %s"%(name, fn)
 
-        
+
         self.sym_rots =  [torch.tensor(v_i, dtype=torch.float32) for v_i in symms]
         self.order = len(self.sym_rots)
 
-        # Return if identity is the first rotation  
+        # Return if identity is the first rotation
         if not np.isclose(((self.sym_rots[0]-np.eye(3))**2).sum(), 0):
 
             # Move identity to be the first rotation
diff --git a/rfdiffusion/inference/utils.py b/rfdiffusion/inference/utils.py
index 43e7e99..61298c4 100644
--- a/rfdiffusion/inference/utils.py
+++ b/rfdiffusion/inference/utils.py
@@ -640,7 +640,6 @@ def process_target(pdb_path, parse_hetatom=False, center=True):
         out["info_het"] = target_struct["info_het"]
     return out
 
-
 def get_idx0_hotspots(mappings, ppi_conf, binderlen):
     """
     Take pdb-indexed hotspot resudes and the length of the binder, and makes the 0-indexed tensor of hotspots
@@ -654,12 +653,15 @@ def get_idx0_hotspots(mappings, ppi_conf, binderlen):
             ), "Hotspot residues need to be provided in pdb-indexed form. E.g. A100,A103"
             hotspots = [(i[0], int(i[1:])) for i in ppi_conf.hotspot_res]
             hotspot_idx = []
-            for i, res in enumerate(mappings["receptor_con_ref_pdb_idx"]):
+            if "receptor_con_ref_pdb_idx" in mappings:
+              (idx,idx0) = (mappings["receptor_con_ref_pdb_idx"],mappings["receptor_con_hal_idx0"])
+            else:
+              (idx,idx0) = (mappings["con_ref_pdb_idx"],mappings["con_hal_idx0"])
+            for i, res in enumerate(idx):
                 if res in hotspots:
-                    hotspot_idx.append(mappings["receptor_con_hal_idx0"][i])
+                    hotspot_idx.append(idx0[i])
     return hotspot_idx
 
-
 class BlockAdjacency:
     """
     Class for handling PPI design inference with ss/block_adj inputs.
@@ -688,8 +690,8 @@ class BlockAdjacency:
              conf.scaffold_list as conf
              conf.inference.num_designs for sanity checking
         """
-       
-        self.conf=conf 
+
+        self.conf=conf
         # either list or path to .txt file with list of scaffolds
         if self.conf.scaffoldguided.scaffold_list is not None:
             if type(self.conf.scaffoldguided.scaffold_list) == list:
@@ -874,13 +876,13 @@ class BlockAdjacency:
         """
         Wrapper method for pulling an item from the list, and preparing ss and block adj features
         """
-        
+
         # Handle determinism. Useful for integration tests
         if self.conf.inference.deterministic:
             torch.manual_seed(self.num_completed)
             np.random.seed(self.num_completed)
             random.seed(self.num_completed)
-  
+
         if self.systematic:
             # reset if num designs > num_scaffolds
             if self.item_n >= len(self.scaffold_list):
diff --git a/scripts/run_inference.py b/scripts/run_inference.py
index 2a3bf36..f58c0bd 100755
--- a/scripts/run_inference.py
+++ b/scripts/run_inference.py
@@ -27,7 +27,22 @@ from hydra.core.hydra_config import HydraConfig
 import numpy as np
 import random
 import glob
+from omegaconf import DictConfig
+from rfdiffusion.inference import model_runners
 
+def sampler_selector(conf: DictConfig):
+    if conf.scaffoldguided.scaffoldguided:
+        sampler = model_runners.ScaffoldedSampler(conf)
+    else:
+        if conf.inference.model_runner == "default":
+            sampler = model_runners.Sampler(conf)
+        elif conf.inference.model_runner == "SelfConditioning":
+            sampler = model_runners.SelfConditioning(conf)
+        elif conf.inference.model_runner == "ScaffoldedSampler":
+            sampler = model_runners.ScaffoldedSampler(conf)
+        else:
+            raise ValueError(f"Unrecognized sampler {conf.model_runner}")
+    return sampler
 
 def make_deterministic(seed=0):
     torch.manual_seed(seed)
@@ -38,18 +53,10 @@ def make_deterministic(seed=0):
 @hydra.main(version_base=None, config_path="../config/inference", config_name="base")
 def main(conf: HydraConfig) -> None:
     log = logging.getLogger(__name__)
+    torch.cuda.is_available = lambda : False
     if conf.inference.deterministic:
         make_deterministic()
 
-    # Check for available GPU and print result of check
-    if torch.cuda.is_available():
-        device_name = torch.cuda.get_device_name(torch.cuda.current_device())
-        log.info(f"Found GPU with device_name {device_name}. Will run RFdiffusion on {device_name}")
-    else:
-        log.info("////////////////////////////////////////////////")
-        log.info("///// NO GPU DETECTED! Falling back to CPU /////")
-        log.info("////////////////////////////////////////////////")
-
     # Initialize sampler and target/contig.
     sampler = iu.sampler_selector(conf)
 
@@ -87,8 +94,16 @@ def main(conf: HydraConfig) -> None:
         seq_stack = []
         plddt_stack = []
 
-        x_t = torch.clone(x_init)
-        seq_t = torch.clone(seq_init)
+        if conf.inference.precision == "bfloat16":
+            dtype=torch.bfloat16
+            x_t = torch.clone(x_init)
+            seq_t = torch.clone(seq_init)
+            x_t = x_t.to(dtype=dtype)
+            seq_t = seq_t.to(dtype=dtype)
+        else:
+            x_t = torch.clone(x_init)
+            seq_t = torch.clone(seq_init)
+
         # Loop over number of reverse diffusion time steps.
         for t in range(int(sampler.t_step_input), sampler.inf_conf.final_step - 1, -1):
             px0, x_t, seq_t, plddt = sampler.sample_step(
@@ -147,9 +162,7 @@ def main(conf: HydraConfig) -> None:
         trb = dict(
             config=OmegaConf.to_container(sampler._conf, resolve=True),
             plddt=plddt_stack.cpu().numpy(),
-            device=torch.cuda.get_device_name(torch.cuda.current_device())
-            if torch.cuda.is_available()
-            else "CPU",
+            device='cpu',
             time=time.time() - start_time,
         )
         if hasattr(sampler, "contig_map"):
diff --git a/setup_rfdiffusion.sh b/setup_rfdiffusion.sh
new file mode 100644
index 0000000..3626fc2
--- /dev/null
+++ b/setup_rfdiffusion.sh
@@ -0,0 +1,109 @@
+# Copyright 2025 Intel Corporation
+# SPDX-License-Identifier: MIT License
+# Author: Rahamathullah
+# Email shaikx.rahamathullah@intel.com
+
+#!/bin/bash
+
+set -e
+SCRIPT_PATH="${BASH_SOURCE:-$0}"
+ABS_SCRIPT_PATH="$(realpath "${SCRIPT_PATH}")"
+#echo "Value of ABS_SCRIPT_PATH: ${ABS_SCRIPT_PATH}"
+ABS_DIRECTORY="$(dirname "${ABS_SCRIPT_PATH}")"
+# Default Conda installation directory
+CONDA_INSTALL_DIR=$(realpath ./miniforge3)
+
+# Parse command line arguments
+while (( "$#" )); do
+  case "$1" in
+    -p)
+      CONDA_INSTALL_DIR=$2
+      CONDA_INSTALL_DIR=$(realpath "$CONDA_INSTALL_DIR")
+      shift 2
+      ;;
+    -*|--*=) # Unsupported flags
+      echo "Error: Unsupported flag $1" >&2
+      exit 1
+      ;;
+    *) # Preserve positional arguments
+      echo "Error: Unsupported argument $1" >&2
+      exit 1
+      ;;
+  esac
+done
+# Check if Miniforge3 exists and install if not found
+if [ ! -d "$CONDA_INSTALL_DIR" ]; then
+  echo "Miniforge3 is not installed. Installing..."
+  wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
+  bash Miniforge3-Linux-x86_64.sh -b -p "$CONDA_INSTALL_DIR"
+  echo "Miniforge3 installation complete."
+else
+  echo "Miniforge3 is already installed at: $CONDA_INSTALL_DIR"
+fi
+# Export Conda binary path
+export PATH="$CONDA_INSTALL_DIR/bin:$PATH"
+# Clone the RFdiffusion repository if it doesn't exist
+if [ ! -d "RFdiffusion" ]; then
+  git clone https://github.com/RosettaCommons/RFdiffusion.git
+else
+  echo "RFdiffusion repository already exists, skipping git clone."
+fi
+
+echo "$CONDA_INSTALL_DIR"
+# Apply patch (assuming patch file is RFdiffusion.patch and it should be applied in RFdiffusion directory)
+cd RFdiffusion
+git checkout 820bfdfaded8c260b962dc40a3171eae316b6ce0
+git log -1
+PATCH_FILE="$ABS_DIRECTORY/RFdiffusion.patch"
+echo $PATCH_FILE
+if [ -f "$PATCH_FILE" ]; then
+  # Check if the patch is already applied
+  if git apply --reverse --check "$PATCH_FILE" > /dev/null 2>&1; then
+    echo "Patch has already been applied. Skipping patch step."
+  else
+    git apply "$PATCH_FILE"
+    echo "Patch applied successfully."
+  fi
+else
+  echo "Error: Patch file not found at $PATCH_FILE" >&2
+  exit 1
+fi
+mkdir -p models
+cd models/
+wget https://files.ipd.uw.edu/pub/RFdiffusion/6f5902ac237024bdd0c176cb93063dc4/Base_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/e29311f6f1bf1af907f9ef9f44b8328b/Complex_base_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/60f09a193fb5e5ccdc4980417708dbab/Complex_Fold_base_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/74f51cfb8b440f50d70878e05361d8f0/InpaintSeq_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/76d00716416567174cdb7ca96e208296/InpaintSeq_Fold_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/5532d2e1f3a4738decd58b19d633b3c3/ActiveSite_ckpt.pt
+wget https://files.ipd.uw.edu/pub/RFdiffusion/12fc204edeae5b57713c5ad7dcb97d39/Base_epoch8_ckpt.pt
+# Optional:
+wget https://files.ipd.uw.edu/pub/RFdiffusion/f572d396fae9206628714fb2ce00f72e/Complex_beta_ckpt.pt
+# original structure prediction weights
+wget https://files.ipd.uw.edu/pub/RFdiffusion/1befcb9b28e2f778f53d47f18b7597fa/RF_structure_prediction_weights.pt
+cd ../
+# Create and activate the Conda environment using the YAML file, disabling plugins to avoid errors
+#CONDA_NO_PLUGINS=true
+if conda env list | grep -q "^SE3nv"; then
+	echo "Environment exists. Moving ahead without create the env. If the setup crashes, please remove manually."
+    else
+	echo "Creating conda env SE3nv.."
+	conda env create -f env/SE3nv.yml
+fi
+source $CONDA_INSTALL_DIR/bin/activate SE3nv
+#conda init
+#conda activate SE3nv
+
+# Install SE3Transformer requirements
+cd env/SE3Transformer
+pip install --no-cache-dir -r requirements.txt
+python setup.py install
+
+# Install the rfdiffusion module
+cd ../.. # Change into the root directory of the repository
+pip install -e .
+
+echo ""
+echo "Note:"
+echo "Conda (Miniforge3) is installed at $CONDA_INSTALL_DIR"
+echo "To manually activate conda env, do: source $CONDA_INSTALL_DIR/bin/activate SE3nv"
