diff --git a/config/inference/base.yaml b/config/inference/base.yaml
index fac858e..80ff699 100644
--- a/config/inference/base.yaml
+++ b/config/inference/base.yaml
@@ -21,6 +21,7 @@ inference:
   trb_save_ckpt_path: null
   schedule_directory_path: null
   model_directory_path: null
+  precision: null
 
 contigmap:
   contigs: null
diff --git a/env/SE3nv.yml b/env/SE3nv.yml
index a51bcce..ab28e9c 100644
--- a/env/SE3nv.yml
+++ b/env/SE3nv.yml
@@ -1,18 +1,16 @@
 name: SE3nv
 channels:
-  - defaults
   - conda-forge
-  - pytorch
-  - dglteam
-  - nvidia
 dependencies:
-  - python=3.9
-  - pytorch=1.9
-  - torchaudio
-  - torchvision
-  - cudatoolkit=11.1
-  - dgl-cuda11.1
-  - pip
+  - python=3.11.0
+  - pip=24.0
   - pip:
-    - hydra-core
-    - pyrsistent
+    - intel-extension-for-pytorch==2.2.0
+    - pandas==2.2.2
+    - torch==2.2.0
+    - hydra-core==1.3.2
+    - pyrsistent==0.20.0
+    - torchdata==0.7.1
+    - pydantic==2.7.1
+    - dgl==2.1.0
+    - numpy==1.26.0
diff --git a/rfdiffusion/inference/model_runners.py b/rfdiffusion/inference/model_runners.py
index 9a33d98..8fe54e7 100644
--- a/rfdiffusion/inference/model_runners.py
+++ b/rfdiffusion/inference/model_runners.py
@@ -14,6 +14,7 @@ import torch.nn.functional as nn
 from rfdiffusion import util
 from hydra.core.hydra_config import HydraConfig
 import os
+import intel_extension_for_pytorch as ipex
 
 from rfdiffusion.model_input_logger import pickle_function_call
 import sys
@@ -35,13 +36,13 @@ class Sampler:
         """
         self.initialized = False
         self.initialize(conf)
-        
+
     def initialize(self, conf: DictConfig) -> None:
         """
         Initialize sampler.
         Args:
             conf: Configuration
-        
+
         - Selects appropriate model from input
         - Assembles Config from model checkpoint and command line overrides
 
@@ -55,7 +56,7 @@ class Sampler:
 
         # Assign config to Sampler
         self._conf = conf
-
+        
         ################################
         ### Select Appropriate Model ###
         ################################
@@ -104,6 +105,12 @@ class Sampler:
             self.assemble_config_from_chk()
             # Now actually load the model weights into RF
             self.model = self.load_model()
+            if self._conf.inference.precision == "bfloat16":
+                dtype = torch.bfloat16
+                self.model = ipex.optimize(self.model, dtype =dtype)
+            else:
+                dtype = torch.float32
+                self.model = ipex.optimize(self.model, dtype= dtype)
         else:
             self.assemble_config_from_chk()
 
@@ -144,7 +151,7 @@ class Sampler:
             self.symmetry = None
 
         self.allatom = ComputeAllAtomCoords().to(self.device)
-        
+
         if self.inf_conf.input_pdb is None:
             # set default pdb
             script_dir=os.path.dirname(os.path.realpath(__file__))
@@ -161,7 +168,7 @@ class Sampler:
             self.t_step_input = int(self.diffuser_conf.partial_T)
         else:
             self.t_step_input = int(self.diffuser_conf.T)
-        
+
     @property
     def T(self):
         '''
@@ -191,7 +198,7 @@ class Sampler:
         Actions:
             - Replaces all -model and -diffuser items
             - Throws a warning if there are items in -model and -diffuser that aren't in the checkpoint
-        
+
         This throws an error if there is a flag in the checkpoint 'config_dict' that isn't in the inference config.
         This should ensure that whenever a feature is added in the training setup, it is accounted for in the inference script.
 
@@ -209,17 +216,17 @@ class Sampler:
                     self._conf[cat][key] = self.ckpt['config_dict'][cat][key]
                 except:
                     pass
-        
+
         # add overrides back in again
         for override in overrides:
             if override.split(".")[0] in ['model','diffuser','preprocess']:
-                print(f'WARNING: You are changing {override.split("=")[0]} from the value this model was trained with. Are you sure you know what you are doing?') 
+                print(f'WARNING: You are changing {override.split("=")[0]} from the value this model was trained with. Are you sure you know what you are doing?')
                 mytype = type(self._conf[override.split(".")[0]][override.split(".")[1].split("=")[0]])
                 self._conf[override.split(".")[0]][override.split(".")[1].split("=")[0]] = mytype(override.split("=")[1])
 
     def load_model(self):
         """Create RosettaFold model from preloaded checkpoint."""
-        
+
         # Read input dimensions from checkpoint.
         self.d_t1d=self._conf.preprocess.d_t1d
         self.d_t2d=self._conf.preprocess.d_t2d
@@ -228,6 +235,12 @@ class Sampler:
             pickle_dir = pickle_function_call(model, 'forward', 'inference')
             print(f'pickle_dir: {pickle_dir}')
         model = model.eval()
+        if self._conf.inference.precision == "bfloat16":
+            dtype = torch.bfloat16
+            model = ipex.optimize(model, dtype=dtype)
+        else:
+            dtype = torch.float32
+            model = ipex.optimize(model,dtype=dtype)
         self._log.info(f'Loading checkpoint.')
         model.load_state_dict(self.ckpt['model_state_dict'], strict=True)
         return model
@@ -253,15 +266,15 @@ class Sampler:
     def sample_init(self, return_forward_trajectory=False):
         """
         Initial features to start the sampling process.
-        
+
         Modify signature and function body for different initialization
         based on the config.
-        
+
         Returns:
             xt: Starting positions with a portion of them randomly sampled.
             seq_t: Starting sequence with a portion of them set to unknown.
         """
-        
+
         #######################
         ### Parse input pdb ###
         #######################
@@ -278,7 +291,7 @@ class Sampler:
         self.mappings = self.contig_map.get_mappings()
         self.mask_seq = torch.from_numpy(self.contig_map.inpaint_seq)[None,:]
         self.mask_str = torch.from_numpy(self.contig_map.inpaint_str)[None,:]
-        self.binderlen =  len(self.contig_map.inpaint)     
+        self.binderlen =  len(self.contig_map.inpaint)
 
         ####################
         ### Get Hotspots ###
@@ -310,7 +323,7 @@ class Sampler:
 
         self.diffusion_mask = self.mask_str
         self.chain_idx=['A' if i < self.binderlen else 'B' for i in range(L_mapped)]
-        
+
         ####################################
         ### Generate initial coordinates ###
         ####################################
@@ -338,7 +351,7 @@ class Sampler:
             atom_mask_mapped = torch.full((L_mapped, 27), False)
             atom_mask_mapped[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0]
 
-        # Diffuse the contig-mapped coordinates 
+        # Diffuse the contig-mapped coordinates
         if self.diffuser_conf.partial_T:
             assert self.diffuser_conf.partial_T <= self.diffuser_conf.T, "Partial_T must be less than T"
             self.t_step_input = int(self.diffuser_conf.partial_T)
@@ -352,10 +365,10 @@ class Sampler:
 
         seq_t = torch.full((1,L_mapped), 21).squeeze() # 21 is the mask token
         seq_t[contig_map.hal_idx0] = seq_orig[contig_map.ref_idx0]
-        
+
         # Unmask sequence if desired
         if self._conf.contigmap.provide_seq is not None:
-            seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()] 
+            seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()]
 
         seq_t[~self.mask_seq.squeeze()] = 21
         seq_t    = torch.nn.functional.one_hot(seq_t, num_classes=22).float() # [L,22]
@@ -379,7 +392,7 @@ class Sampler:
         if self.symmetry is not None:
             xt, seq_t = self.symmetry.apply_symmetry(xt, seq_t)
         self._log.info(f'Sequence init: {seq2chars(torch.argmax(seq_t, dim=-1))}')
-        
+
         self.msa_prev = None
         self.pair_prev = None
         self.state_prev = None
@@ -407,17 +420,17 @@ class Sampler:
         return xt, seq_t
 
     def _preprocess(self, seq, xyz_t, t, repack=False):
-        
+
         """
         Function to prepare inputs to diffusion model
-        
-            seq (L,22) one-hot sequence 
+
+            seq (L,22) one-hot sequence
 
             msa_masked (1,1,L,48)
 
             msa_full (1,1,L,25)
-        
-            xyz_t (L,14,3) template crds (diffused) 
+
+            xyz_t (L,14,3) template crds (diffused)
 
             t1d (1,L,28) this is the t1d before tacking on the chi angles:
                 - seq + unknown/mask (21)
@@ -427,7 +440,7 @@ class Sampler:
                 - contacting residues: for ppi. Target residues in contact with binder (1)
                 - empty feature (legacy) (1)
                 - ss (H, E, L, MASK) (4)
-            
+
             t2d (1, L, L, 45)
                 - last plane is block adjacency
     """
@@ -456,7 +469,7 @@ class Sampler:
 
         ###########
         ### t1d ###
-        ########### 
+        ###########
 
         # Here we need to go from one hot with 22 classes to one hot with 21 classes (last plane is missing token)
         t1d = torch.zeros((1,1,L,21))
@@ -466,9 +479,9 @@ class Sampler:
             if seqt1d[idx,21] == 1:
                 seqt1d[idx,20] = 1
                 seqt1d[idx,21] = 0
-        
+
         t1d[:,:,:,:21] = seqt1d[None,None,:,:21]
-        
+
 
         # Set timestep feature to 1 where diffusion mask is True, else 1-t/T
         timefeature = torch.zeros((L)).float()
@@ -477,7 +490,7 @@ class Sampler:
         timefeature = timefeature[None,None,...,None]
 
         t1d = torch.cat((t1d, timefeature), dim=-1).float()
-        
+
         #############
         ### xyz_t ###
         #############
@@ -493,8 +506,8 @@ class Sampler:
         ### t2d ###
         ###########
         t2d = xyz_to_t2d(xyz_t)
-        
-        ###########      
+
+        ###########
         ### idx ###
         ###########
         idx = torch.tensor(self.contig_map.rf)[None]
@@ -519,7 +532,7 @@ class Sampler:
         t1d = t1d.to(self.device)
         t2d = t2d.to(self.device)
         alpha_t = alpha_t.to(self.device)
-        
+
         ######################
         ### added_features ###
         ######################
@@ -541,7 +554,7 @@ class Sampler:
             t1d=torch.cat((t1d, torch.zeros_like(t1d[...,:1]), hotspot_tens[None,None,...,None].to(self.device)), dim=-1)
 
         return msa_masked, msa_full, seq[None], torch.squeeze(xyz_t, dim=0), idx, t1d, t2d, xyz_t, alpha_t
-        
+
     def sample_step(self, *, t, x_t, seq_init, final_step):
         '''Generate the next pose that the model should be supplied at timestep t-1.
 
@@ -550,7 +563,7 @@ class Sampler:
             seq_t (torch.tensor): (L,22) The sequence at the beginning of this timestep
             x_t (torch.tensor): (L,14,3) The residue positions at the beginning of this timestep
             seq_init (torch.tensor): (L,22) The initialized sequence used in updating the sequence.
-            
+
         Returns:
             px0: (L,14,3) The model's prediction of x0.
             x_t_1: (L,14,3) The updated positions of the next step.
@@ -587,14 +600,14 @@ class Sampler:
                                 return_infer=True,
                                 motif_mask=self.diffusion_mask.squeeze().to(self.device))
 
-        # prediction of X0 
+        # prediction of X0
         _, px0  = self.allatom(torch.argmax(seq_in, dim=-1), px0, alpha)
         px0    = px0.squeeze()[:,:14]
-        
+
         #####################
         ### Get next pose ###
         #####################
-        
+
         if t > final_step:
             seq_t_1 = nn.one_hot(seq_init,num_classes=22).to(self.device)
             x_t_1, px0 = self.denoiser.get_next_pose(
@@ -643,7 +656,7 @@ class SelfConditioning(Sampler):
         ##################################
         ######## Str Self Cond ###########
         ##################################
-        if (t < self.diffuser.T) and (t != self.diffuser_conf.partial_T):   
+        if (t < self.diffuser.T) and (t != self.diffuser_conf.partial_T):
             zeros = torch.zeros(B,1,L,24,3).float().to(xyz_t.device)
             xyz_t = torch.cat((self.prev_pred.unsqueeze(1),zeros), dim=-2) # [B,T,L,27,3]
             t2d_44   = xyz_to_t2d(xyz_t) # [B,T,L,L,44]
@@ -659,9 +672,16 @@ class SelfConditioning(Sampler):
         ####################
         ### Forward Pass ###
         ####################
+        if self._conf.inference.precision == "bfloat16":
+            dtype = torch.bfloat16
+            enable = True
+        else:
+            dtype = torch.float32
+            enable = False
 
         with torch.no_grad():
-            msa_prev, pair_prev, px0, state_prev, alpha, logits, plddt = self.model(msa_masked,
+            with torch.autocast(enabled = enable, dtype = dtype, device_type = "cpu"):
+                msa_prev, pair_prev, px0, state_prev, alpha, logits, plddt = self.model(msa_masked,
                                 msa_full,
                                 seq_in,
                                 xt_in,
@@ -675,7 +695,7 @@ class SelfConditioning(Sampler):
                                 state_prev = None,
                                 t=torch.tensor(t),
                                 return_infer=True,
-                                motif_mask=self.diffusion_mask.squeeze().to(self.device))   
+                                motif_mask=self.diffusion_mask.squeeze().to(self.device))
 
             if self.symmetry is not None and self.inf_conf.symmetric_self_cond:
                 px0 = self.symmetrise_prev_pred(px0=px0,seq_in=seq_in, alpha=alpha)[:,:,:3]
@@ -685,7 +705,7 @@ class SelfConditioning(Sampler):
         # prediction of X0
         _, px0  = self.allatom(torch.argmax(seq_in, dim=-1), px0, alpha)
         px0    = px0.squeeze()[:,:14]
-        
+
         ###########################
         ### Generate Next Input ###
         ###########################
@@ -725,7 +745,7 @@ class SelfConditioning(Sampler):
         return px0_sym
 
 class ScaffoldedSampler(SelfConditioning):
-    """ 
+    """
     Model Runner for Scaffold-Constrained diffusion
     """
     def __init__(self, conf: DictConfig):
@@ -764,7 +784,33 @@ class ScaffoldedSampler(SelfConditioning):
         else:
             self.target = None
             self.target_pdb=False
+    def initialize(self, conf: DictConfig):
+        super().initialize(conf)
+        # initialize BlockAdjacency sampling class
+        self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)
+
+
+        #################################################
+        ### Initialize target, if doing binder design ###
+        #################################################
 
+        if conf.scaffoldguided.target_pdb:
+            self.target = iu.Target(conf.scaffoldguided, conf.ppi.hotspot_res)
+            self.target_pdb = self.target.get_target()
+            if conf.scaffoldguided.target_ss is not None:
+                self.target_ss = torch.load(conf.scaffoldguided.target_ss).long()
+                self.target_ss = torch.nn.functional.one_hot(self.target_ss, num_classes=4)
+                if self._conf.scaffoldguided.contig_crop is not None:
+                    self.target_ss=self.target_ss[self.target_pdb['crop_mask']]
+            if conf.scaffoldguided.target_adj is not None:
+                self.target_adj = torch.load(conf.scaffoldguided.target_adj).long()
+                self.target_adj=torch.nn.functional.one_hot(self.target_adj, num_classes=3)
+                if self._conf.scaffoldguided.contig_crop is not None:
+                        self.target_adj=self.target_adj[self.target_pdb['crop_mask']]
+                        self.target_adj=self.target_adj[:,self.target_pdb['crop_mask']]
+        else:
+            self.target = None
+            self.target_pdb=False
     def sample_init(self):
         """
         Wrapper method for taking secondary structure + adj, and outputting xt, seq_t
@@ -778,9 +824,9 @@ class ScaffoldedSampler(SelfConditioning):
 
         ##############################
         ### Auto-contig generation ###
-        ##############################    
+        ##############################
 
-        if self.contig_conf.contigs is None: 
+        if self.contig_conf.contigs is None:
             # process target
             xT = torch.full((self.L, 27,3), np.nan)
             xT = get_init_xyz(xT[None,None]).squeeze()
@@ -808,7 +854,7 @@ class ScaffoldedSampler(SelfConditioning):
                 contig = []
                 for idx,i in enumerate(self.target_pdb['pdb_idx'][:-1]):
                     if idx==0:
-                        start=i[1]               
+                        start=i[1]
                     if i[1] + 1 != self.target_pdb['pdb_idx'][idx+1][1] or i[0] != self.target_pdb['pdb_idx'][idx+1][0]:
                         contig.append(f'{i[0]}{start}-{i[1]}/0 ')
                         start = self.target_pdb['pdb_idx'][idx+1][1]
@@ -851,18 +897,18 @@ class ScaffoldedSampler(SelfConditioning):
             assert L_mapped==self.adj.shape[0]
             diffusion_mask = self.mask_str
             self.diffusion_mask = diffusion_mask
-            
+
             xT = torch.full((1,1,L_mapped,27,3), np.nan)
             xT[:, :, contig_map.hal_idx0, ...] = xyz_27[contig_map.ref_idx0,...]
             xT = get_init_xyz(xT).squeeze()
             atom_mask = torch.full((L_mapped, 27), False)
             atom_mask[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0]
- 
+
         ####################
         ### Get hotspots ###
         ####################
         self.hotspot_0idx=iu.get_idx0_hotspots(self.mappings, self.ppi_conf, self.binderlen)
-        
+
         #########################
         ### Set up potentials ###
         #########################
@@ -905,17 +951,17 @@ class ScaffoldedSampler(SelfConditioning):
 
         xT = torch.clone(fa_stack[-1].squeeze()[:,:14,:])
         return xT, seq_T
-    
+
     def _preprocess(self, seq, xyz_t, t):
         msa_masked, msa_full, seq, xyz_prev, idx_pdb, t1d, t2d, xyz_t, alpha_t = super()._preprocess(seq, xyz_t, t, repack=False)
-        
+
         ###################################
         ### Add Adj/Secondary Structure ###
         ###################################
 
         assert self.preprocess_conf.d_t1d == 28, "The checkpoint you're using hasn't been trained with sec-struc/block adjacency features"
         assert self.preprocess_conf.d_t2d == 47, "The checkpoint you're using hasn't been trained with sec-struc/block adjacency features"
-       
+
         #####################
         ### Handle Target ###
         #####################
@@ -930,7 +976,7 @@ class ScaffoldedSampler(SelfConditioning):
         t1d=torch.cat((t1d, full_ss[None,None].to(self.device)), dim=-1)
 
         t1d = t1d.float()
-        
+
         ###########
         ### t2d ###
         ###########
diff --git a/rfdiffusion/inference/symmetry.py b/rfdiffusion/inference/symmetry.py
index 864a5ab..463363b 100644
--- a/rfdiffusion/inference/symmetry.py
+++ b/rfdiffusion/inference/symmetry.py
@@ -73,7 +73,7 @@ class SymGen:
             self.apply_symmetry = self._apply_octahedral
 
         elif global_sym.lower() in saved_symmetries:
-            # Using a saved symmetry 
+            # Using a saved symmetry
             self._log.info('Initializing %s symmetry order.'%global_sym)
             self._init_from_symrots_file(global_sym)
 
@@ -108,7 +108,7 @@ class SymGen:
             start_i = subunit_len * i
             end_i = subunit_len * (i+1)
             coords_out[start_i:end_i] = torch.einsum(
-                'bnj,kj->bnk', coords_out[:subunit_len], self.sym_rots[i])
+                'bnj,kj->bnk', coords_out[:subunit_len].to(dtype=torch.float32), self.sym_rots[i].to(dtype=torch.float32))
             seq_out[start_i:end_i]  = seq_out[:subunit_len]
         return coords_out, seq_out
 
@@ -174,7 +174,7 @@ class SymGen:
                 center = torch.mean(subunit_chain[:, 1, :], axis=0)
                 subunit_chain -= center[None, None, :]
                 rotated_axis = torch.einsum(
-                    'nj,kj->nk', base_axis, self.sym_rots[i]) 
+                    'nj,kj->nk', base_axis, self.sym_rots[i])
                 subunit_chain += rotated_axis[:, None, :]
 
             coords_out[start_i:end_i] = subunit_chain
@@ -185,7 +185,7 @@ class SymGen:
     ## symmetry from file #
     #######################
     def _init_from_symrots_file(self, name):
-        """ _init_from_symrots_file initializes using 
+        """ _init_from_symrots_file initializes using
         ./inference/sym_rots.npz
 
         Args:
@@ -203,11 +203,11 @@ class SymGen:
             if str(k) == name: symms = v
         assert symms is not None, "%s not found in %s"%(name, fn)
 
-        
+
         self.sym_rots =  [torch.tensor(v_i, dtype=torch.float32) for v_i in symms]
         self.order = len(self.sym_rots)
 
-        # Return if identity is the first rotation  
+        # Return if identity is the first rotation
         if not np.isclose(((self.sym_rots[0]-np.eye(3))**2).sum(), 0):
 
             # Move identity to be the first rotation
diff --git a/rfdiffusion/util.py b/rfdiffusion/util.py
index 19c30f5..fb3b552 100644
--- a/rfdiffusion/util.py
+++ b/rfdiffusion/util.py
@@ -267,6 +267,152 @@ def cross_product_matrix(u):
     matrix[:, :, 2, 1] = u[..., 0]
     return matrix
 
+def writepdb_str(
+    atoms, seq, binderlen=None, idx_pdb=None, bfacts=None, chain_idx=None
+):
+    import io
+    f = io.StringIO()
+    ctr = 1
+    scpu = seq.cpu().squeeze()
+    atomscpu = atoms.cpu().squeeze()
+    if bfacts is None:
+        bfacts = torch.zeros(atomscpu.shape[0])
+    if idx_pdb is None:
+        idx_pdb = 1 + torch.arange(atomscpu.shape[0])
+
+    Bfacts = torch.clamp(bfacts.cpu(), 0, 1)
+    for i, s in enumerate(scpu):
+        if chain_idx is None:
+            if binderlen is not None:
+                if i < binderlen:
+                    chain = "A"
+                else:
+                    chain = "B"
+            elif binderlen is None:
+                chain = "A"
+        else:
+            chain = chain_idx[i]
+        if len(atomscpu.shape) == 2:
+            f.write(
+                "%-6s%5s %4s %3s %s%4d    %8.3f%8.3f%8.3f%6.2f%6.2f\n"
+                % (
+                    "ATOM",
+                    ctr,
+                    " CA ",
+                    num2aa[s],
+                    chain,
+                    idx_pdb[i],
+                    atomscpu[i, 0],
+                    atomscpu[i, 1],
+                    atomscpu[i, 2],
+                    1.0,
+                    Bfacts[i],
+                )
+            )
+            ctr += 1
+        elif atomscpu.shape[1] == 3:
+            for j, atm_j in enumerate([" N  ", " CA ", " C  "]):
+                f.write(
+                    "%-6s%5s %4s %3s %s%4d    %8.3f%8.3f%8.3f%6.2f%6.2f\n"
+                    % (
+                        "ATOM",
+                        ctr,
+                        atm_j,
+                        num2aa[s],
+                        chain,
+                        idx_pdb[i],
+                        atomscpu[i, j, 0],
+                        atomscpu[i, j, 1],
+                        atomscpu[i, j, 2],
+                        1.0,
+                        Bfacts[i],
+                    )
+                )
+                ctr += 1
+        elif atomscpu.shape[1] == 4:
+            for j, atm_j in enumerate([" N  ", " CA ", " C  ", " O  "]):
+                f.write(
+                    "%-6s%5s %4s %3s %s%4d    %8.3f%8.3f%8.3f%6.2f%6.2f\n"
+                    % (
+                        "ATOM",
+                        ctr,
+                        atm_j,
+                        num2aa[s],
+                        chain,
+                        idx_pdb[i],
+                        atomscpu[i, j, 0],
+                        atomscpu[i, j, 1],
+                        atomscpu[i, j, 2],
+                        1.0,
+                        Bfacts[i],
+                    )
+                )
+                ctr += 1
+
+        else:
+            natoms = atomscpu.shape[1]
+            if natoms != 14 and natoms != 27:
+                print("bad size!", atoms.shape)
+                assert False
+            atms = aa2long[s]
+            # his prot hack
+            if (
+                s == 8
+                and torch.linalg.norm(atomscpu[i, 9, :] - atomscpu[i, 5, :]) < 1.7
+            ):
+                atms = (
+                    " N  ",
+                    " CA ",
+                    " C  ",
+                    " O  ",
+                    " CB ",
+                    " CG ",
+                    " NE2",
+                    " CD2",
+                    " CE1",
+                    " ND1",
+                    None,
+                    None,
+                    None,
+                    None,
+                    " H  ",
+                    " HA ",
+                    "1HB ",
+                    "2HB ",
+                    " HD2",
+                    " HE1",
+                    " HD1",
+                    None,
+                    None,
+                    None,
+                    None,
+                    None,
+                    None,
+                )  # his_d
+
+            for j, atm_j in enumerate(atms):
+                if (
+                    j < natoms and atm_j is not None
+                ):  # and not torch.isnan(atomscpu[i,j,:]).any()):
+                    f.write(
+                        "%-6s%5s %4s %3s %s%4d    %8.3f%8.3f%8.3f%6.2f%6.2f\n"
+                        % (
+                            "ATOM",
+                            ctr,
+                            atm_j,
+                            num2aa[s],
+                            chain,
+                            idx_pdb[i],
+                            atomscpu[i, j, 0],
+                            atomscpu[i, j, 1],
+                            atomscpu[i, j, 2],
+                            1.0,
+                            Bfacts[i],
+                        )
+                    )
+                    ctr += 1
+
+    return f.getvalue()
 
 # writepdb
 def writepdb(
@@ -713,6 +859,61 @@ def writepdb_multi(
                 ctr += 1
 
         f.write("ENDMDL\n")
+def writepdb_multi_str(
+    atoms_stack,
+    bfacts,
+    seq_stack,
+    backbone_only=False,
+    chain_ids=None,
+    use_hydrogens=True,
+):
+    """
+    Return a multi-model PDB string for structural states instead of writing to a file.
+    """
+    lines = []
+
+    if seq_stack.ndim != 2:
+        T = atoms_stack.shape[0]
+        seq_stack = torch.tile(seq_stack, (T, 1))
+    seq_stack = seq_stack.cpu()
+
+    for atoms, scpu in zip(atoms_stack, seq_stack):
+        ctr = 1
+        atomscpu = atoms.cpu()
+        Bfacts = torch.clamp(bfacts.cpu(), 0, 1)
+        for i, s in enumerate(scpu):
+            atms = aa2long[s.item()]
+            for j, atm_j in enumerate(atms):
+                if backbone_only and j >= N_BACKBONE_ATOMS:
+                    break
+                if not use_hydrogens and j >= N_HEAVY:
+                    break
+                if (atm_j is None) or (torch.all(torch.isnan(atomscpu[i, j]))):
+                    continue
+                chain_id = "A"
+                if chain_ids is not None:
+                    chain_id = chain_ids[i]
+                lines.append(
+                    "%-6s%5d %4s %3s %s%4d    %8.3f%8.3f%8.3f%6.2f%6.2f" % (
+                        "ATOM",
+                        ctr,
+                        atm_j,
+                        num2aa[s.item()],
+                        chain_id,
+                        i + 1,
+                        atomscpu[i, j, 0].item(),
+                        atomscpu[i, j, 1].item(),
+                        atomscpu[i, j, 2].item(),
+                        1.0,
+                        Bfacts[i].item(),
+                    )
+                )
+                ctr += 1
+
+        lines.append("ENDMDL")
+
+    return "\n".join(lines)
+
 
 def calc_rmsd(xyz1, xyz2, eps=1e-6):
     """
diff --git a/scripts/run_inference.py b/scripts/run_inference.py
index 2a3bf36..f58c0bd 100755
--- a/scripts/run_inference.py
+++ b/scripts/run_inference.py
@@ -27,7 +27,22 @@ from hydra.core.hydra_config import HydraConfig
 import numpy as np
 import random
 import glob
+from omegaconf import DictConfig
+from rfdiffusion.inference import model_runners
 
+def sampler_selector(conf: DictConfig):
+    if conf.scaffoldguided.scaffoldguided:
+        sampler = model_runners.ScaffoldedSampler(conf)
+    else:
+        if conf.inference.model_runner == "default":
+            sampler = model_runners.Sampler(conf)
+        elif conf.inference.model_runner == "SelfConditioning":
+            sampler = model_runners.SelfConditioning(conf)
+        elif conf.inference.model_runner == "ScaffoldedSampler":
+            sampler = model_runners.ScaffoldedSampler(conf)
+        else:
+            raise ValueError(f"Unrecognized sampler {conf.model_runner}")
+    return sampler
 
 def make_deterministic(seed=0):
     torch.manual_seed(seed)
@@ -38,18 +53,10 @@ def make_deterministic(seed=0):
 @hydra.main(version_base=None, config_path="../config/inference", config_name="base")
 def main(conf: HydraConfig) -> None:
     log = logging.getLogger(__name__)
+    torch.cuda.is_available = lambda : False
     if conf.inference.deterministic:
         make_deterministic()
 
-    # Check for available GPU and print result of check
-    if torch.cuda.is_available():
-        device_name = torch.cuda.get_device_name(torch.cuda.current_device())
-        log.info(f"Found GPU with device_name {device_name}. Will run RFdiffusion on {device_name}")
-    else:
-        log.info("////////////////////////////////////////////////")
-        log.info("///// NO GPU DETECTED! Falling back to CPU /////")
-        log.info("////////////////////////////////////////////////")
-
     # Initialize sampler and target/contig.
     sampler = iu.sampler_selector(conf)
 
@@ -87,8 +94,16 @@ def main(conf: HydraConfig) -> None:
         seq_stack = []
         plddt_stack = []
 
-        x_t = torch.clone(x_init)
-        seq_t = torch.clone(seq_init)
+        if conf.inference.precision == "bfloat16":
+            dtype=torch.bfloat16
+            x_t = torch.clone(x_init)
+            seq_t = torch.clone(seq_init)
+            x_t = x_t.to(dtype=dtype)
+            seq_t = seq_t.to(dtype=dtype)
+        else:
+            x_t = torch.clone(x_init)
+            seq_t = torch.clone(seq_init)
+
         # Loop over number of reverse diffusion time steps.
         for t in range(int(sampler.t_step_input), sampler.inf_conf.final_step - 1, -1):
             px0, x_t, seq_t, plddt = sampler.sample_step(
@@ -147,9 +162,7 @@ def main(conf: HydraConfig) -> None:
         trb = dict(
             config=OmegaConf.to_container(sampler._conf, resolve=True),
             plddt=plddt_stack.cpu().numpy(),
-            device=torch.cuda.get_device_name(torch.cuda.current_device())
-            if torch.cuda.is_available()
-            else "CPU",
+            device='cpu',
             time=time.time() - start_time,
         )
         if hasattr(sampler, "contig_map"):
