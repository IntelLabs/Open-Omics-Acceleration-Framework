diff --git a/colabdesign/__init__.py b/colabdesign/__init__.py
index 4fa3969..58a43f7 100644
--- a/colabdesign/__init__.py
+++ b/colabdesign/__init__.py
@@ -6,11 +6,7 @@ if int(jax.__version__.split(".")[1]) > 3:
 import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
-from colabdesign.shared.utils import clear_mem
 from colabdesign.af.model import mk_af_model
-from colabdesign.tr.model import mk_tr_model
-from colabdesign.mpnn.model import mk_mpnn_model
 
 # backward compatability
 mk_design_model = mk_afdesign_model = mk_af_model
-mk_trdesign_model = mk_tr_model
\ No newline at end of file
diff --git a/colabdesign/af/__init__.py b/colabdesign/af/__init__.py
index bc92f61..58a43f7 100644
--- a/colabdesign/af/__init__.py
+++ b/colabdesign/af/__init__.py
@@ -6,8 +6,7 @@ if int(jax.__version__.split(".")[1]) > 3:
 import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
-from colabdesign.shared.utils import clear_mem
 from colabdesign.af.model import mk_af_model
 
 # backward compatability
-mk_design_model = mk_afdesign_model = mk_af_model
\ No newline at end of file
+mk_design_model = mk_afdesign_model = mk_af_model
diff --git a/colabdesign/af/design.py b/colabdesign/af/design.py
deleted file mode 100644
index 732b3d5..0000000
--- a/colabdesign/af/design.py
+++ /dev/null
@@ -1,561 +0,0 @@
-import random, os
-import jax
-import jax.numpy as jnp
-import numpy as np
-from colabdesign.af.alphafold.common import residue_constants
-from colabdesign.shared.utils import copy_dict, update_dict, Key, dict_to_str, to_float, softmax, categorical, to_list, copy_missing
-
-####################################################
-# AF_DESIGN - design functions
-####################################################
-#\
-# \_af_design
-# |\
-# | \_restart
-#  \
-#   \_design
-#    \_step
-#     \_run
-#      \_recycle
-#       \_single
-#
-####################################################
-
-class _af_design:
-
-  def restart(self, seed=None, opt=None, weights=None,
-              seq=None, mode=None, keep_history=False, reset_opt=True, **kwargs):   
-    '''
-    restart the optimization
-    ------------
-    note: model.restart() resets the [opt]ions and weights to their defaults
-    use model.set_opt(..., set_defaults=True) and model.set_weights(..., set_defaults=True)
-    or model.restart(reset_opt=False) to avoid this
-    ------------
-    seed=0 - set seed for reproducibility
-    reset_opt=False - do NOT reset [opt]ions/weights to defaults
-    keep_history=True - do NOT clear the trajectory/[opt]ions/weights
-    '''
-    # reset [opt]ions
-    if reset_opt and not keep_history:
-      copy_missing(self.opt, self._opt)
-      self.opt = copy_dict(self._opt)
-      if hasattr(self,"aux"): del self.aux
-    
-    if not keep_history:
-      # initialize trajectory
-      self._tmp = {"traj":{"seq":[],"xyz":[],"plddt":[],"pae":[]},
-                   "log":[],"best":{}}
-
-    # update options/settings (if defined)
-    self.set_opt(opt)
-    self.set_weights(weights)
-  
-    # initialize sequence
-    self.set_seed(seed)
-    self.set_seq(seq=seq, mode=mode, **kwargs)
-
-    # reset optimizer
-    self._k = 0
-    self.set_optimizer()
-
-  def _get_model_nums(self, num_models=None, sample_models=None, models=None):
-    '''decide which model params to use'''
-    if num_models is None: num_models = self.opt["num_models"]
-    if sample_models is None: sample_models = self.opt["sample_models"]
-
-    ns_name = self._model_names
-    ns = list(range(len(ns_name)))
-    if models is not None:
-      models = models if isinstance(models,list) else [models]
-      ns = [ns[n if isinstance(n,int) else ns_name.index(n)] for n in models]
-
-    m = min(num_models,len(ns))
-    if sample_models and m != len(ns):
-      model_nums = np.random.choice(ns,(m,),replace=False)
-    else:
-      model_nums = ns[:m]
-    return model_nums   
-
-  def run(self, num_recycles=None, num_models=None, sample_models=None, models=None,
-          backprop=True, callback=None, model_nums=None, return_aux=False):
-    '''run model to get outputs, losses and gradients'''
-    
-    # pre-design callbacks
-    for fn in self._callbacks["design"]["pre"]: fn(self)
-
-    # decide which model params to use
-    if model_nums is None:
-      model_nums = self._get_model_nums(num_models, sample_models, models)
-    assert len(model_nums) > 0, "ERROR: no model params defined"
-
-    # loop through model params
-    auxs = []
-    for n in model_nums:
-      p = self._model_params[n]
-      auxs.append(self._recycle(p, num_recycles=num_recycles, backprop=backprop))
-    auxs = jax.tree_map(lambda *x: np.stack(x), *auxs)
-
-    # update aux (average outputs)
-    def avg_or_first(x):
-      if np.issubdtype(x.dtype, np.integer): return x[0]
-      else: return x.mean(0)
-
-    self.aux = jax.tree_map(avg_or_first, auxs)
-    self.aux["atom_positions"] = auxs["atom_positions"][0]
-    self.aux["all"] = auxs
-    
-    # post-design callbacks
-    for fn in (self._callbacks["design"]["post"] + to_list(callback)): fn(self)
-
-    # update log
-    self.aux["log"] = {**self.aux["losses"]}
-    self.aux["log"]["plddt"] = 1 - self.aux["log"]["plddt"]
-    for k in ["loss","i_ptm","ptm"]: self.aux["log"][k] = self.aux[k]
-    for k in ["hard","soft","temp"]: self.aux["log"][k] = self.opt[k]
-
-    # compute sequence recovery
-    if self.protocol in ["fixbb","partial"] or (self.protocol == "binder" and self._args["redesign"]):
-      if self.protocol == "partial":
-        aatype = self.aux["aatype"][...,self.opt["pos"]]
-      else:
-        aatype = self.aux["seq"]["pseudo"].argmax(-1)
-
-      mask = self._wt_aatype != -1
-      true = self._wt_aatype[mask]
-      pred = aatype[...,mask]
-      self.aux["log"]["seqid"] = (true == pred).mean()
-
-    self.aux["log"] = to_float(self.aux["log"])
-    self.aux["log"].update({"recycles":int(self.aux["num_recycles"]),
-                            "models":model_nums})
-    
-    if return_aux: return self.aux
-
-  def _single(self, model_params, backprop=True):
-    '''single pass through the model'''
-    self._inputs["opt"] = self.opt
-    flags  = [self._params, model_params, self._inputs, self.key()]
-    if backprop:
-      (loss, aux), grad = self._model["grad_fn"](*flags)
-    else:
-      loss, aux = self._model["fn"](*flags)
-      grad = jax.tree_map(np.zeros_like, self._params)
-    aux.update({"loss":loss,"grad":grad})
-    return aux
-
-  def _recycle(self, model_params, num_recycles=None, backprop=True):   
-    '''multiple passes through the model (aka recycle)'''
-    a = self._args
-    mode = a["recycle_mode"]
-    if num_recycles is None:
-      num_recycles = self.opt["num_recycles"]
-
-    if mode in ["backprop","add_prev"]:
-      # recycles compiled into model, only need single-pass
-      aux = self._single(model_params, backprop)
-    
-    else:
-      L = self._inputs["residue_index"].shape[0]
-      
-      # intialize previous
-      if "prev" not in self._inputs or a["clear_prev"]:
-        prev = {'prev_msa_first_row': np.zeros([L,256]),
-                'prev_pair': np.zeros([L,L,128])}
-
-        if a["use_initial_guess"] and "batch" in self._inputs:
-          prev["prev_pos"] = self._inputs["batch"]["all_atom_positions"] 
-        else:
-          prev["prev_pos"] = np.zeros([L,37,3])
-
-        if a["use_dgram"]:
-          # TODO: add support for initial_guess + use_dgram
-          prev["prev_dgram"] = np.zeros([L,L,64])
-
-        if a["use_initial_atom_pos"]:
-          if "batch" in self._inputs:
-            self._inputs["initial_atom_pos"] = self._inputs["batch"]["all_atom_positions"] 
-          else:
-            self._inputs["initial_atom_pos"] = np.zeros([L,37,3])              
-      
-      self._inputs["prev"] = prev
-      # decide which layers to compute gradients for
-      cycles = (num_recycles + 1)
-      mask = [0] * cycles
-
-      if mode == "sample":  mask[np.random.randint(0,cycles)] = 1
-      if mode == "average": mask = [1/cycles] * cycles
-      if mode == "last":    mask[-1] = 1
-      if mode == "first":   mask[0] = 1
-      
-      # gather gradients across recycles 
-      grad = []
-      for m in mask:        
-        if m == 0:
-          aux = self._single(model_params, backprop=False)
-        else:
-          aux = self._single(model_params, backprop)
-          grad.append(jax.tree_map(lambda x:x*m, aux["grad"]))
-        self._inputs["prev"] = aux["prev"]
-        if a["use_initial_atom_pos"]:
-          self._inputs["initial_atom_pos"] = aux["prev"]["prev_pos"]                
-
-      aux["grad"] = jax.tree_map(lambda *x: np.stack(x).sum(0), *grad)
-    
-    aux["num_recycles"] = num_recycles
-    return aux
-
-  def step(self, lr_scale=1.0, num_recycles=None,
-           num_models=None, sample_models=None, models=None, backprop=True,
-           callback=None, save_best=False, verbose=1):
-    '''do one step of gradient descent'''
-    
-    # run
-    self.run(num_recycles=num_recycles, num_models=num_models, sample_models=sample_models,
-             models=models, backprop=backprop, callback=callback)
-
-    # modify gradients    
-    if self.opt["norm_seq_grad"]: self._norm_seq_grad()
-    self._state, self.aux["grad"] = self._optimizer(self._state, self.aux["grad"], self._params)
-  
-    # apply gradients
-    lr = self.opt["learning_rate"] * lr_scale
-    self._params = jax.tree_map(lambda x,g:x-lr*g, self._params, self.aux["grad"])
-
-    # save results
-    self._save_results(save_best=save_best, verbose=verbose)
-
-    # increment
-    self._k += 1
-
-  def _print_log(self, print_str=None, aux=None):
-    if aux is None: aux = self.aux
-    keys = ["models","recycles","hard","soft","temp","seqid","loss",
-            "seq_ent","mlm","helix","pae","i_pae","exp_res","con","i_con",
-            "sc_fape","sc_rmsd","dgram_cce","fape","plddt","ptm"]
-    
-    if "i_ptm" in aux["log"]:
-      if len(self._lengths) > 1:
-        keys.append("i_ptm")
-      else:
-        aux["log"].pop("i_ptm")
-
-    print(dict_to_str(aux["log"], filt=self.opt["weights"],
-                      print_str=print_str, keys=keys+["rmsd"], ok=["plddt","rmsd"]))
-
-  def _save_results(self, aux=None, save_best=False,
-                    best_metric=None, metric_higher_better=False,
-                    verbose=True):
-    if aux is None: aux = self.aux    
-    self._tmp["log"].append(aux["log"])    
-    if (self._k % self._args["traj_iter"]) == 0:
-      # update traj
-      traj = {"seq":   aux["seq"]["pseudo"],
-              "xyz":   aux["atom_positions"][:,1,:],
-              "plddt": aux["plddt"],
-              "pae":   aux["pae"]}
-      for k,v in traj.items():
-        if len(self._tmp["traj"][k]) == self._args["traj_max"]:
-          self._tmp["traj"][k].pop(0)
-        self._tmp["traj"][k].append(v)
-
-    # save best
-    if save_best:
-      if best_metric is None:
-        best_metric = self._args["best_metric"]
-      metric = float(aux["log"][best_metric])
-      if self._args["best_metric"] in ["plddt","ptm","i_ptm","seqid","composite"] or metric_higher_better:
-        metric = -metric
-      if "metric" not in self._tmp["best"] or metric < self._tmp["best"]["metric"]:
-        self._tmp["best"]["aux"] = copy_dict(aux)
-        self._tmp["best"]["metric"] = metric
-
-    if verbose and ((self._k+1) % verbose) == 0:
-      self._print_log(f"{self._k+1}", aux=aux)
-
-  def predict(self, seq=None, bias=None,
-              num_models=None, num_recycles=None, models=None, sample_models=False,
-              dropout=False, hard=True, soft=False, temp=1,
-              return_aux=False, verbose=True,  seed=None, **kwargs):
-    '''predict structure for input sequence (if provided)'''
-
-    def load_settings():    
-      if "save" in self._tmp:
-        [self.opt, self._args, self._params, self._inputs] = self._tmp.pop("save")
-
-    def save_settings():
-      load_settings()
-      self._tmp["save"] = [copy_dict(x) for x in [self.opt, self._args, self._params, self._inputs]]
-
-    save_settings()
-
-    # set seed if defined
-    if seed is not None: self.set_seed(seed)
-
-    # set [seq]uence/[opt]ions
-    if seq is not None: self.set_seq(seq=seq, bias=bias)    
-    self.set_opt(hard=hard, soft=soft, temp=temp, dropout=dropout, pssm_hard=True)
-    self.set_args(shuffle_first=False)
-    
-    # run
-    self.run(num_recycles=num_recycles, num_models=num_models,
-             sample_models=sample_models, models=models, backprop=False, **kwargs)
-    if verbose: self._print_log("predict")
-
-    load_settings()
-
-    # return (or save) results
-    if return_aux: return self.aux
-
-  # ---------------------------------------------------------------------------------
-  # example design functions
-  # ---------------------------------------------------------------------------------
-  def design(self, iters=100,
-             soft=0.0, e_soft=None,
-             temp=1.0, e_temp=None,
-             hard=0.0, e_hard=None,
-             step=1.0, e_step=None,
-             dropout=True, opt=None, weights=None, 
-             num_recycles=None, ramp_recycles=False, 
-             num_models=None, sample_models=None, models=None,
-             backprop=True, callback=None, save_best=False, verbose=1):
-
-    # update options/settings (if defined)
-    self.set_opt(opt, dropout=dropout)
-    self.set_weights(weights)    
-    m = {"soft":[soft,e_soft],"temp":[temp,e_temp],
-         "hard":[hard,e_hard],"step":[step,e_step]}
-    m = {k:[s,(s if e is None else e)] for k,(s,e) in m.items()}
-
-    if ramp_recycles:
-      if num_recycles is None:
-        num_recycles = self.opt["num_recycles"]
-      m["num_recycles"] = [0,num_recycles]
-
-    for i in range(iters):
-      for k,(s,e) in m.items():
-        if k == "temp":
-          self.set_opt({k:(e+(s-e)*(1-(i+1)/iters)**2)})
-        else:
-          v = (s+(e-s)*((i+1)/iters))
-          if k == "step": step = v
-          elif k == "num_recycles": num_recycles = round(v)
-          else: self.set_opt({k:v})
-      
-      # decay learning rate based on temperature
-      lr_scale = step * ((1 - self.opt["soft"]) + (self.opt["soft"] * self.opt["temp"]))
-      
-      self.step(lr_scale=lr_scale, num_recycles=num_recycles,
-                num_models=num_models, sample_models=sample_models, models=models,
-                backprop=backprop, callback=callback, save_best=save_best, verbose=verbose)
-
-  def design_logits(self, iters=100, **kwargs):
-    ''' optimize logits '''
-    self.design(iters, **kwargs)
-
-  def design_soft(self, iters=100, temp=1, **kwargs):
-    ''' optimize softmax(logits/temp)'''
-    self.design(iters, soft=1, temp=temp, **kwargs)
-  
-  def design_hard(self, iters=100, **kwargs):
-    ''' optimize argmax(logits) '''
-    self.design(iters, soft=1, hard=1, **kwargs)
-
-  # ---------------------------------------------------------------------------------
-  # experimental
-  # ---------------------------------------------------------------------------------
-  def design_3stage(self, soft_iters=300, temp_iters=100, hard_iters=10,
-                    ramp_recycles=True, **kwargs):
-    '''three stage design (logits→soft→hard)'''
-
-    verbose = kwargs.get("verbose",1)
-
-    # stage 1: logits -> softmax(logits/1.0)
-    if soft_iters > 0:
-      if verbose: print("Stage 1: running (logits → soft)")
-      self.design_logits(soft_iters, e_soft=1,
-        ramp_recycles=ramp_recycles, **kwargs)
-      self._tmp["seq_logits"] = self.aux["seq"]["logits"]
-      
-    # stage 2: softmax(logits/1.0) -> softmax(logits/0.01)
-    if temp_iters > 0:
-      if verbose: print("Stage 2: running (soft → hard)")
-      self.design_soft(temp_iters, e_temp=1e-2, **kwargs)
-    
-    # stage 3:
-    if hard_iters > 0:
-      if verbose: print("Stage 3: running (hard)")
-      kwargs["dropout"] = False
-      kwargs["save_best"] = True
-      kwargs["num_models"] = len(self._model_names)
-      self.design_hard(hard_iters, temp=1e-2, **kwargs)
-
-  def _mutate(self, seq, plddt=None, logits=None, mutation_rate=1):
-    '''mutate random position'''
-    seq = np.array(seq)
-    N,L = seq.shape
-
-    # fix some positions
-    i_prob = np.ones(L) if plddt is None else np.maximum(1-plddt,0)
-    i_prob[np.isnan(i_prob)] = 0
-    if "fix_pos" in self.opt:
-      if "pos" in self.opt:
-        p = self.opt["pos"][self.opt["fix_pos"]]
-        seq[...,p] = self._wt_aatype_sub
-      else:
-        p = self.opt["fix_pos"]
-        seq[...,p] = self._wt_aatype[...,p]
-      i_prob[p] = 0
-    
-    for m in range(mutation_rate):
-      # sample position
-      # https://www.biorxiv.org/content/10.1101/2021.08.24.457549v1
-      i = np.random.choice(np.arange(L),p=i_prob/i_prob.sum())
-
-      # sample amino acid
-      logits = np.array(0 if logits is None else logits)
-      if logits.ndim == 3: logits = logits[:,i]
-      elif logits.ndim == 2: logits = logits[i]
-      a_logits = logits - np.eye(self._args["alphabet_size"])[seq[:,i]] * 1e8
-      a = categorical(softmax(a_logits))
-
-      # return mutant
-      seq[:,i] = a
-    
-    return seq
-
-  def design_semigreedy(self, iters=100, tries=10, dropout=False,
-                        save_best=True, seq_logits=None, e_tries=None, **kwargs):
-
-    '''semigreedy search'''    
-    if e_tries is None: e_tries = tries
-
-    # get starting sequence
-    if hasattr(self,"aux"):
-      seq = self.aux["seq"]["logits"].argmax(-1)
-    else:
-      seq = (self._params["seq"] + self._inputs["bias"]).argmax(-1)
-
-    # bias sampling towards the defined bias
-    if seq_logits is None: seq_logits = 0
-    
-    model_flags = {k:kwargs.pop(k,None) for k in ["num_models","sample_models","models"]}
-    verbose = kwargs.pop("verbose",1)
-
-    # get current plddt
-    aux = self.predict(seq, return_aux=True, verbose=False, **model_flags, **kwargs)
-    plddt = self.aux["plddt"]
-    plddt = plddt[self._target_len:] if self.protocol == "binder" else plddt[:self._len]
-
-    # optimize!
-    if verbose:
-      print("Running semigreedy optimization...")
-    
-    for i in range(iters):
-      buff = []
-      model_nums = self._get_model_nums(**model_flags)
-      num_tries = (tries+(e_tries-tries)*((i+1)/iters))
-      for t in range(int(num_tries)):
-        mut_seq = self._mutate(seq=seq, plddt=plddt,
-                               logits=seq_logits + self._inputs["bias"])
-        aux = self.predict(seq=mut_seq, return_aux=True, model_nums=model_nums, verbose=False, **kwargs)
-        buff.append({"aux":aux, "seq":np.array(mut_seq)})
-
-      # accept best
-      losses = [x["aux"]["loss"] for x in buff]
-      best = buff[np.argmin(losses)]
-      self.aux, seq = best["aux"], jnp.array(best["seq"])
-      self.set_seq(seq=seq, bias=self._inputs["bias"])
-      self._save_results(save_best=save_best, verbose=verbose)
-
-      # update plddt
-      plddt = best["aux"]["plddt"]
-      plddt = plddt[self._target_len:] if self.protocol == "binder" else plddt[:self._len]
-      self._k += 1
-
-  def design_pssm_semigreedy(self, soft_iters=300, hard_iters=32, tries=10, e_tries=None,
-                             ramp_recycles=True, ramp_models=True, **kwargs):
-
-    verbose = kwargs.get("verbose",1)
-
-    # stage 1: logits -> softmax(logits)
-    if soft_iters > 0:
-      self.design_3stage(soft_iters, 0, 0, ramp_recycles=ramp_recycles, **kwargs)
-      self._tmp["seq_logits"] = kwargs["seq_logits"] = self.aux["seq"]["logits"]
-
-    # stage 2: semi_greedy
-    if hard_iters > 0:
-      kwargs["dropout"] = False
-      if ramp_models:
-        num_models = len(kwargs.get("models",self._model_names))
-        iters = hard_iters
-        for m in range(num_models):
-          if verbose and m > 0: print(f'Increasing number of models to {m+1}.')
-
-          kwargs["num_models"] = m + 1
-          kwargs["save_best"] = (m + 1) == num_models
-          self.design_semigreedy(iters, tries=tries, e_tries=e_tries, **kwargs)
-          if m < 2: iters = iters // 2
-      else:
-        self.design_semigreedy(hard_iters, tries=tries, e_tries=e_tries, **kwargs)
-
-  # ---------------------------------------------------------------------------------
-  # experimental optimizers (not extensively evaluated)
-  # ---------------------------------------------------------------------------------
-
-  def _design_mcmc(self, steps=1000, half_life=200, T_init=0.01, mutation_rate=1,
-                   seq_logits=None, save_best=True, **kwargs):
-    '''
-    MCMC with simulated annealing
-    ----------------------------------------
-    steps = number for steps for the MCMC trajectory
-    half_life = half-life for the temperature decay during simulated annealing
-    T_init = starting temperature for simulated annealing. Temperature is decayed exponentially
-    mutation_rate = number of mutations at each MCMC step
-    '''
-
-    # code borrowed from: github.com/bwicky/oligomer_hallucination
-
-    # gather settings
-    verbose = kwargs.pop("verbose",1)
-    model_flags = {k:kwargs.pop(k,None) for k in ["num_models","sample_models","models"]}
-
-    # initialize
-    plddt, best_loss, current_loss = None, np.inf, np.inf 
-    current_seq = (self._params["seq"] + self._inputs["bias"]).argmax(-1)
-    if seq_logits is None: seq_logits = 0
-
-    # run!
-    if verbose: print("Running MCMC with simulated annealing...")
-    for i in range(steps):
-
-      # update temperature
-      T = T_init * (np.exp(np.log(0.5) / half_life) ** i) 
-
-      # mutate sequence
-      if i == 0:
-        mut_seq = current_seq
-      else:
-        mut_seq = self._mutate(seq=current_seq, plddt=plddt,
-                               logits=seq_logits + self._inputs["bias"],
-                               mutation_rate=mutation_rate)
-
-      # get loss
-      model_nums = self._get_model_nums(**model_flags)
-      aux = self.predict(seq=mut_seq, return_aux=True, verbose=False, model_nums=model_nums, **kwargs)
-      loss = aux["log"]["loss"]
-  
-      # decide
-      delta = loss - current_loss
-      if i == 0 or delta < 0 or np.random.uniform() < np.exp( -delta / T):
-
-        # accept
-        (current_seq,current_loss) = (mut_seq,loss)
-        
-        plddt = aux["all"]["plddt"].mean(0)
-        plddt = plddt[self._target_len:] if self.protocol == "binder" else plddt[:self._len]
-        
-        if loss < best_loss:
-          (best_loss, self._k) = (loss, i)
-          self.set_seq(seq=current_seq, bias=self._inputs["bias"])
-          self._save_results(save_best=save_best, verbose=verbose)
diff --git a/colabdesign/af/inputs.py b/colabdesign/af/inputs.py
deleted file mode 100644
index fe390a6..0000000
--- a/colabdesign/af/inputs.py
+++ /dev/null
@@ -1,155 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-
-from colabdesign.shared.utils import copy_dict
-from colabdesign.shared.model import soft_seq
-from colabdesign.af.alphafold.common import residue_constants
-from colabdesign.af.alphafold.model import model, config
-
-############################################################################
-# AF_INPUTS - functions for modifying inputs before passing to alphafold
-############################################################################
-class _af_inputs:
-
-  def _get_seq(self, inputs, aux, key=None):
-    params, opt = inputs["params"], inputs["opt"]
-    '''get sequence features'''
-    seq = soft_seq(params["seq"], inputs["bias"], opt, key, num_seq=self._num,
-                   shuffle_first=self._args["shuffle_first"])
-    seq = self._fix_pos(seq)
-    aux.update({"seq":seq, "seq_pseudo":seq["pseudo"]})
-    
-    # protocol specific modifications to seq features
-    if self.protocol == "binder":
-      # concatenate target and binder sequence
-      seq_target = jax.nn.one_hot(inputs["batch"]["aatype"][:self._target_len],self._args["alphabet_size"])
-      seq_target = jnp.broadcast_to(seq_target,(self._num, *seq_target.shape))
-      seq = jax.tree_map(lambda x:jnp.concatenate([seq_target,x],1), seq)
-      
-    if self.protocol in ["fixbb","hallucination","partial"] and self._args["copies"] > 1:
-      seq = jax.tree_map(lambda x:expand_copies(x, self._args["copies"], self._args["block_diag"]), seq)
-
-    return seq
-
-  def _fix_pos(self, seq, return_p=False):
-    if "fix_pos" in self.opt:
-      if "pos" in self.opt:
-        seq_ref = jax.nn.one_hot(self._wt_aatype_sub,self._args["alphabet_size"])
-        p = self.opt["pos"][self.opt["fix_pos"]]
-        fix_seq = lambda x: x.at[...,p,:].set(seq_ref)
-      else:
-        seq_ref = jax.nn.one_hot(self._wt_aatype,self._args["alphabet_size"])
-        p = self.opt["fix_pos"]
-        fix_seq = lambda x: x.at[...,p,:].set(seq_ref[...,p,:])
-      seq = jax.tree_map(fix_seq, seq)
-      if return_p: return seq, p
-    return seq
-
-  def _update_template(self, inputs, key):
-    ''''dynamically update template features''' 
-    if "batch" in inputs:
-      batch, opt = inputs["batch"], inputs["opt"]
-
-      # enable templates
-      inputs["template_mask"] = inputs["template_mask"].at[0].set(1)
-      L = batch["aatype"].shape[0]
-      
-      # decide which position to remove sequence and/or sidechains
-      rm     = jnp.broadcast_to(inputs.get("rm_template",False),L)
-      rm_seq = jnp.where(rm,True,jnp.broadcast_to(inputs.get("rm_template_seq",True),L))
-      rm_sc  = jnp.where(rm_seq,True,jnp.broadcast_to(inputs.get("rm_template_sc",True),L))
-                          
-      # define template features
-      template_feats = {"template_aatype":jnp.where(rm_seq,21,batch["aatype"])}
-
-      if "dgram" in batch:
-        # use dgram from batch if provided
-        template_feats.update({"template_dgram":batch["dgram"]})
-        nT,nL = inputs["template_aatype"].shape
-        inputs["template_dgram"] = jnp.zeros((nT,nL,nL,39))
-        
-      if "all_atom_positions" in batch:
-        # get pseudo-carbon-beta coordinates (carbon-alpha for glycine)
-        # aatype = is used to define template's CB coordinates (CA in case of glycine)
-        cb, cb_mask = model.modules.pseudo_beta_fn(
-          jnp.where(rm_seq,0,batch["aatype"]),
-          batch["all_atom_positions"],
-          batch["all_atom_mask"])
-        template_feats.update({"template_pseudo_beta":        cb,
-                               "template_pseudo_beta_mask":   cb_mask,
-                               "template_all_atom_positions": batch["all_atom_positions"],
-                               "template_all_atom_mask":      batch["all_atom_mask"]})
-
-      # inject template features
-      if self.protocol == "partial":
-        pos = opt["pos"]
-        if self._args["repeat"] or self._args["homooligomer"]:
-          C,L = self._args["copies"], self._len
-          pos = (jnp.repeat(pos,C).reshape(-1,C) + jnp.arange(C) * L).T.flatten()
-
-      for k,v in template_feats.items():
-        if self.protocol == "partial":
-          if k in ["template_dgram"]:
-            inputs[k] = inputs[k].at[0,pos[:,None],pos[None,:]].set(v)
-          else:
-            inputs[k] = inputs[k].at[0,pos].set(v)
-        else:
-          inputs[k] = inputs[k].at[0].set(v)
-        
-        # remove sidechains (mask anything beyond CB)
-        if k in ["template_all_atom_mask"]:
-          if self.protocol == "partial":
-            inputs[k] = inputs[k].at[:,pos,5:].set(jnp.where(rm_sc[:,None],0,inputs[k][:,pos,5:]))
-            inputs[k] = inputs[k].at[:,pos].set(jnp.where(rm[:,None],0,inputs[k][:,pos]))
-          else:
-            inputs[k] = inputs[k].at[...,5:].set(jnp.where(rm_sc[:,None],0,inputs[k][...,5:]))
-            inputs[k] = jnp.where(rm[:,None],0,inputs[k])
-
-def update_seq(seq, inputs, seq_1hot=None, seq_pssm=None, mlm=None):
-  '''update the sequence features'''
-  
-  if seq_1hot is None: seq_1hot = seq 
-  if seq_pssm is None: seq_pssm = seq
-  target_feat = seq_1hot[0,:,:20]
-
-  seq_1hot = jnp.pad(seq_1hot,[[0,0],[0,0],[0,22-seq_1hot.shape[-1]]])
-  seq_pssm = jnp.pad(seq_pssm,[[0,0],[0,0],[0,22-seq_pssm.shape[-1]]])
-  msa_feat = jnp.zeros_like(inputs["msa_feat"]).at[...,0:22].set(seq_1hot).at[...,25:47].set(seq_pssm)
-
-  # masked language modeling (randomly mask positions)
-  if mlm is not None:    
-    X = jax.nn.one_hot(22,23)
-    X = jnp.zeros(msa_feat.shape[-1]).at[...,:23].set(X).at[...,25:48].set(X)
-    msa_feat = jnp.where(mlm[...,None],X,msa_feat)
-    
-  inputs.update({"msa_feat":msa_feat, "target_feat":target_feat})
-
-def update_aatype(aatype, inputs):
-  r = residue_constants
-  a = {"atom14_atom_exists":r.restype_atom14_mask,
-       "atom37_atom_exists":r.restype_atom37_mask,
-       "residx_atom14_to_atom37":r.restype_atom14_to_atom37,
-       "residx_atom37_to_atom14":r.restype_atom37_to_atom14}
-  mask = inputs["seq_mask"][:,None]
-  inputs.update(jax.tree_map(lambda x:jnp.where(mask,jnp.asarray(x)[aatype],0),a))
-  inputs["aatype"] = aatype
-
-def expand_copies(x, copies, block_diag=True):
-  '''
-  given msa (N,L,20) expand to (1+N*copies,L*copies,22) if block_diag else (N,L*copies,22)
-  '''
-  if x.shape[-1] < 22:
-    x = jnp.pad(x,[[0,0],[0,0],[0,22-x.shape[-1]]])
-  x = jnp.tile(x,[1,copies,1])
-  if copies > 1 and block_diag:
-    L = x.shape[1]
-    sub_L = L // copies
-    y = x.reshape((-1,1,copies,sub_L,22))
-    block_diag_mask = jnp.expand_dims(jnp.eye(copies),(0,3,4))
-    seq = block_diag_mask * y
-    gap_seq = (1-block_diag_mask) * jax.nn.one_hot(jnp.repeat(21,sub_L),22)  
-    y = (seq + gap_seq).swapaxes(0,1).reshape(-1,L,22)
-    return jnp.concatenate([x[:1],y],0)
-  else:
-    return x
\ No newline at end of file
diff --git a/colabdesign/af/loss.py b/colabdesign/af/loss.py
deleted file mode 100644
index d998215..0000000
--- a/colabdesign/af/loss.py
+++ /dev/null
@@ -1,548 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-
-from colabdesign.shared.utils import Key, copy_dict
-from colabdesign.shared.protein import jnp_rmsd_w, _np_kabsch, _np_rmsd, _np_get_6D_loss
-from colabdesign.af.alphafold.model import model, folding, all_atom
-from colabdesign.af.alphafold.common import confidence, residue_constants
-
-####################################################
-# AF_LOSS - setup loss function
-####################################################
-
-class _af_loss:
-  # protocol specific loss functions
-  def _loss_fixbb(self, inputs, outputs, aux):
-    opt = inputs["opt"]
-    '''get losses'''
-    copies = self._args["copies"] if self._args["homooligomer"] else 1    
-    # rmsd loss
-    aln = get_rmsd_loss(inputs, outputs, copies=copies)
-    if self._args["realign"]:
-      aux["atom_positions"] = aln["align"](aux["atom_positions"]) * aux["atom_mask"][...,None]
-    
-    # supervised losses
-    aux["losses"].update({
-      "fape":      get_fape_loss(inputs, outputs, copies=copies, clamp=opt["fape_cutoff"]),
-      "dgram_cce": get_dgram_loss(inputs, outputs, copies=copies, aatype=inputs["aatype"]),
-      "rmsd":      aln["rmsd"],
-    })
-    
-    # unsupervised losses
-    self._loss_unsupervised(inputs, outputs, aux)
-
-  def _loss_binder(self, inputs, outputs, aux):
-    '''get losses'''
-    opt = inputs["opt"]
-    mask = inputs["seq_mask"]
-    zeros = jnp.zeros_like(mask)
-    tL,bL = self._target_len, self._binder_len
-    binder_id = zeros.at[-bL:].set(mask[-bL:])
-    if "hotspot" in opt:
-      target_id = zeros.at[opt["hotspot"]].set(mask[opt["hotspot"]])
-      i_con_loss = get_con_loss(inputs, outputs, opt["i_con"], mask_1d=target_id, mask_1b=binder_id)
-    else:
-      target_id = zeros.at[:tL].set(mask[:tL])
-      i_con_loss = get_con_loss(inputs, outputs, opt["i_con"], mask_1d=binder_id, mask_1b=target_id)
-
-    # unsupervised losses
-    aux["losses"].update({
-      "plddt":   get_plddt_loss(outputs, mask_1d=binder_id), # plddt over binder
-      "exp_res": get_exp_res_loss(outputs, mask_1d=binder_id),
-      "pae":     get_pae_loss(outputs, mask_1d=binder_id), # pae over binder + interface
-      "con":     get_con_loss(inputs, outputs, opt["con"], mask_1d=binder_id, mask_1b=binder_id),
-      # interface
-      "i_con":   i_con_loss,
-      "i_pae":   get_pae_loss(outputs, mask_1d=binder_id, mask_1b=target_id),
-    })
-
-    # supervised losses
-    if self._args["redesign"]:      
-  
-      aln = get_rmsd_loss(inputs, outputs, L=tL, include_L=False)
-      align_fn = aln["align"]
-      
-      # compute cce of binder + interface
-      aatype = inputs["aatype"]
-      cce = get_dgram_loss(inputs, outputs, aatype=aatype, return_mtx=True)
-
-      # compute fape
-      fape = get_fape_loss(inputs, outputs, clamp=opt["fape_cutoff"], return_mtx=True)
-
-      aux["losses"].update({
-        "rmsd":      aln["rmsd"],
-        "dgram_cce": cce[-bL:].sum()  / (mask[-bL:].sum() + 1e-8),
-        "fape":      fape[-bL:].sum() / (mask[-bL:].sum() + 1e-8)
-      })
-
-    else:
-      align_fn = get_rmsd_loss(inputs, outputs, L=tL)["align"]
-
-    if self._args["realign"]:
-      aux["atom_positions"] = align_fn(aux["atom_positions"]) * aux["atom_mask"][...,None]
-
-  def _loss_partial(self, inputs, outputs, aux):
-    '''get losses'''    
-    opt = inputs["opt"]
-    pos = opt["pos"]
-    if self._args["repeat"] or self._args["homooligomer"]:
-      C,L = self._args["copies"], self._len
-      pos = (jnp.repeat(pos,C).reshape(-1,C) + jnp.arange(C) * L).T.flatten()
-      
-    def sub(x, axis=0):
-      return jax.tree_map(lambda y:jnp.take(y,pos,axis),x)
-    
-    copies = self._args["copies"] if self._args["homooligomer"] else 1
-    aatype = sub(inputs["aatype"])
-    dgram = {"logits":sub(sub(outputs["distogram"]["logits"]),1),
-             "bin_edges":outputs["distogram"]["bin_edges"]}
-    atoms = sub(outputs["structure_module"]["final_atom_positions"])
-    
-    I = {"aatype": aatype, "batch": inputs["batch"], "seq_mask":sub(inputs["seq_mask"])}
-    O = {"distogram": dgram, "structure_module": {"final_atom_positions": atoms}}
-    aln = get_rmsd_loss(I, O, copies=copies)
-
-    # supervised losses
-    aux["losses"].update({
-      "dgram_cce": get_dgram_loss(I, O, copies=copies, aatype=I["aatype"]),
-      "fape":      get_fape_loss(I, O, copies=copies, clamp=opt["fape_cutoff"]),
-      "rmsd":      aln["rmsd"],
-    })
-    
-    # unsupervised losses
-    self._loss_unsupervised(inputs, outputs, aux)
-
-    # sidechain specific losses
-    if self._args["use_sidechains"] and copies == 1:
-    
-      struct = outputs["structure_module"]
-      pred_pos = sub(struct["final_atom14_positions"])
-      true_pos = all_atom.atom37_to_atom14(inputs["batch"]["all_atom_positions"], self._sc["batch"])
-
-      # sc_rmsd
-      aln = _get_sc_rmsd_loss(true_pos, pred_pos, self._sc["pos"])
-      aux["losses"]["sc_rmsd"] = aln["rmsd"]
-      
-      # sc_fape
-      if not self._args["use_multimer"]:
-        sc_struct = {**folding.compute_renamed_ground_truth(self._sc["batch"], pred_pos),
-                     "sidechains":{k: sub(struct["sidechains"][k],1) for k in ["frames","atom_pos"]}}
-        batch =     {**inputs["batch"],
-                     **all_atom.atom37_to_frames(**inputs["batch"])}
-        aux["losses"]["sc_fape"] = folding.sidechain_loss(batch, sc_struct,
-          self._cfg.model.heads.structure_module)["loss"]
-
-      else:  
-        # TODO
-        print("ERROR: 'sc_fape' not currently supported for 'multimer' mode")
-        aux["losses"]["sc_fape"] = 0.0
-
-    # align final atoms
-    if self._args["realign"]:
-      aux["atom_positions"] = aln["align"](aux["atom_positions"]) * aux["atom_mask"][...,None]
-
-  def _loss_hallucination(self, inputs, outputs, aux):
-    # unsupervised losses
-    self._loss_unsupervised(inputs, outputs, aux)
-
-  def _loss_unsupervised(self, inputs, outputs, aux):
-
-    # define masks
-    opt = inputs["opt"]
-    if "pos" in opt:
-      C,L = self._args["copies"], self._len
-      pos = opt["pos"]
-      if C > 1: pos = (jnp.repeat(pos,C).reshape(-1,C) + jnp.arange(C) * L).T.flatten()
-      mask_1d = inputs["seq_mask"].at[pos].set(0)
-    else:
-      mask_1d = inputs["seq_mask"]
-    
-    seq_mask_2d = inputs["seq_mask"][:,None] * inputs["seq_mask"][None,:]
-    mask_2d = inputs["asym_id"][:,None] == inputs["asym_id"][None,:]
-    masks = {"mask_1d":mask_1d,
-             "mask_2d":jnp.where(seq_mask_2d,mask_2d,0)}
-
-    # define losses
-    losses = {
-      "exp_res": get_exp_res_loss(outputs, mask_1d=mask_1d),
-      "plddt":   get_plddt_loss(outputs, mask_1d=mask_1d),
-      "pae":     get_pae_loss(outputs, **masks),
-      "con":     get_con_loss(inputs, outputs, opt["con"], **masks),
-      "helix":   get_helix_loss(inputs, outputs)
-    }
-
-    # define losses at interface
-    if self._args["copies"] > 1 and not self._args["repeat"]:
-      masks = {"mask_1d": mask_1d if self._args["homooligomer"] else inputs["seq_mask"],
-               "mask_2d": jnp.where(seq_mask_2d,mask_2d == False,0)}
-      losses.update({
-        "i_pae": get_pae_loss(outputs, **masks),
-        "i_con": get_con_loss(inputs, outputs, opt["i_con"], **masks),
-      })
-
-    aux["losses"].update(losses)
-
-#####################################################################################
-
-def get_plddt(outputs):
-  logits = outputs["predicted_lddt"]["logits"]
-  num_bins = logits.shape[-1]
-  bin_width = 1.0 / num_bins
-  bin_centers = jnp.arange(start=0.5 * bin_width, stop=1.0, step=bin_width)
-  probs = jax.nn.softmax(logits, axis=-1)
-  return jnp.sum(probs * bin_centers[None, :], axis=-1)
-
-def get_pae(outputs):
-  prob = jax.nn.softmax(outputs["predicted_aligned_error"]["logits"],-1)
-  breaks = outputs["predicted_aligned_error"]["breaks"]
-  step = breaks[1]-breaks[0]
-  bin_centers = breaks + step/2
-  bin_centers = jnp.append(bin_centers,bin_centers[-1]+step)
-  return (prob*bin_centers).sum(-1)
-
-def get_ptm(inputs, outputs, interface=False):
-  pae = {"residue_weights":inputs["seq_mask"],
-         **outputs["predicted_aligned_error"]}
-  if interface:
-    if "asym_id" not in pae:
-      pae["asym_id"] = inputs["asym_id"]
-  else:
-    if "asym_id" in pae:
-      pae.pop("asym_id")
-  return confidence.predicted_tm_score(**pae, use_jnp=True)
-  
-def get_dgram_bins(outputs):
-  dgram = outputs["distogram"]["logits"]
-  if dgram.shape[-1] == 64:
-    dgram_bins = jnp.append(0,jnp.linspace(2.3125,21.6875,63))
-  if dgram.shape[-1] == 39:
-    dgram_bins = jnp.linspace(3.25,50.75,39) + 1.25
-  return dgram_bins
-
-def get_contact_map(outputs, dist=8.0):
-  '''get contact map from distogram'''
-  dist_logits = outputs["distogram"]["logits"]
-  dist_bins = get_dgram_bins(outputs)
-  return (jax.nn.softmax(dist_logits) * (dist_bins < dist)).sum(-1)
-
-####################
-# confidence metrics
-####################
-def mask_loss(x, mask=None, mask_grad=False):
-  if mask is None:
-    return x.mean()
-  else:
-    x_masked = (x * mask).sum() / (1e-8 + mask.sum())
-    if mask_grad:
-      return jax.lax.stop_gradient(x.mean() - x_masked) + x_masked
-    else:
-      return x_masked
-
-def get_exp_res_loss(outputs, mask_1d=None):
-  p = jax.nn.sigmoid(outputs["experimentally_resolved"]["logits"])
-  p = 1 - p[...,residue_constants.atom_order["CA"]]
-  return mask_loss(p, mask_1d)
-
-def get_plddt_loss(outputs, mask_1d=None):
-  p = 1 - get_plddt(outputs)
-  return mask_loss(p, mask_1d)
-
-def get_pae_loss(outputs, mask_1d=None, mask_1b=None, mask_2d=None):
-  p = get_pae(outputs) / 31.0
-  p = (p + p.T) / 2
-  L = p.shape[0]
-  if mask_1d is None: mask_1d = jnp.ones(L)
-  if mask_1b is None: mask_1b = jnp.ones(L)
-  if mask_2d is None: mask_2d = jnp.ones((L,L))
-  mask_2d = mask_2d * mask_1d[:,None] * mask_1b[None,:]
-  return mask_loss(p, mask_2d)
-
-def get_con_loss(inputs, outputs, con_opt,
-                 mask_1d=None, mask_1b=None, mask_2d=None):
-
-  # get top k
-  def min_k(x, k=1, mask=None):
-    y = jnp.sort(x if mask is None else jnp.where(mask,x,jnp.nan))
-    k_mask = jnp.logical_and(jnp.arange(y.shape[-1]) < k, jnp.isnan(y) == False)
-    return jnp.where(k_mask,y,0).sum(-1) / (k_mask.sum(-1) + 1e-8)
-  
-  # decide on what offset to use
-  if "offset" in inputs:
-    offset = inputs["offset"]
-  else:
-    idx = inputs["residue_index"].flatten()
-    offset = idx[:,None] - idx[None,:]
-
-  # define distogram
-  dgram = outputs["distogram"]["logits"]
-  dgram_bins = get_dgram_bins(outputs)
-
-  p = _get_con_loss(dgram, dgram_bins, cutoff=con_opt["cutoff"], binary=con_opt["binary"])
-  if "seqsep" in con_opt:
-    m = jnp.abs(offset) >= con_opt["seqsep"]
-  else:
-    m = jnp.ones_like(offset)
-
-  # mask results
-  if mask_1d is None: mask_1d = jnp.ones(m.shape[0])
-  if mask_1b is None: mask_1b = jnp.ones(m.shape[0])
-  
-  if mask_2d is None:
-    m = jnp.logical_and(m, mask_1b)
-  else:
-    m = jnp.logical_and(m, mask_2d)  
-
-  p = min_k(p, con_opt["num"], m)
-  return min_k(p, con_opt["num_pos"], mask_1d)
-
-def _get_con_loss(dgram, dgram_bins, cutoff=None, binary=True):
-  '''dgram to contacts'''
-  if cutoff is None: cutoff = dgram_bins[-1]
-  bins = dgram_bins < cutoff  
-  px = jax.nn.softmax(dgram)
-  px_ = jax.nn.softmax(dgram - 1e7 * (1-bins))        
-  # binary/cateogorical cross-entropy
-  con_loss_cat_ent = -(px_ * jax.nn.log_softmax(dgram)).sum(-1)
-  con_loss_bin_ent = -jnp.log((bins * px + 1e-8).sum(-1))
-  return jnp.where(binary, con_loss_bin_ent, con_loss_cat_ent)
-
-def get_helix_loss(inputs, outputs):  
-  # decide on what offset to use
-  if "offset" in inputs:
-    offset = inputs["offset"]
-  else:
-    idx = inputs["residue_index"].flatten()
-    offset = idx[:,None] - idx[None,:]
-
-  # define distogram
-  dgram = outputs["distogram"]["logits"]
-  dgram_bins = get_dgram_bins(outputs)
-
-  mask_2d = inputs["seq_mask"][:,None] * inputs["seq_mask"][None,:]
-  return _get_helix_loss(dgram, dgram_bins, offset, mask_2d=mask_2d)
-
-def _get_helix_loss(dgram, dgram_bins, offset=None, mask_2d=None, **kwargs):
-  '''helix bias loss'''
-  x = _get_con_loss(dgram, dgram_bins, cutoff=6.0, binary=True)
-  if offset is None:
-    if mask_2d is None:
-      return jnp.diagonal(x,3).mean()
-    else:
-      return jnp.diagonal(x * mask_2d,3).sum() + (jnp.diagonal(mask_2d,3).sum() + 1e-8)
-  else:
-    mask = offset == 3
-    if mask_2d is not None:
-      mask = jnp.where(mask_2d,mask,0)
-    return jnp.where(mask,x,0.0).sum() / (mask.sum() + 1e-8)
-
-####################
-# loss functions
-####################
-def get_dgram_loss(inputs, outputs, copies=1, aatype=None, return_mtx=False):
-
-  batch = inputs["batch"]
-  # gather features
-  if aatype is None: aatype = batch["aatype"]
-  pred = outputs["distogram"]["logits"]
-
-  # get true features
-  x, weights = model.modules.pseudo_beta_fn(aatype=aatype,
-                                            all_atom_positions=batch["all_atom_positions"],
-                                            all_atom_mask=batch["all_atom_mask"])
-
-  dm = jnp.square(x[:,None]-x[None,:]).sum(-1,keepdims=True)
-  bin_edges = jnp.linspace(2.3125, 21.6875, pred.shape[-1] - 1)
-  true = jax.nn.one_hot((dm > jnp.square(bin_edges)).sum(-1), pred.shape[-1])
-
-  def loss_fn(t,p,m):
-    cce = -(t*jax.nn.log_softmax(p)).sum(-1)
-    return cce, (cce*m).sum((-1,-2))/(m.sum((-1,-2))+1e-8)
-  
-  weights = jnp.where(inputs["seq_mask"],weights,0)
-  return _get_pw_loss(true, pred, loss_fn, weights=weights, copies=copies, return_mtx=return_mtx)
-
-def get_fape_loss(inputs, outputs, copies=1, clamp=10.0, return_mtx=False):
-
-  def robust_norm(x, axis=-1, keepdims=False, eps=1e-8):
-    return jnp.sqrt(jnp.square(x).sum(axis=axis, keepdims=keepdims) + eps)
-
-  def get_R(N, CA, C):
-    (v1,v2) = (C-CA, N-CA)
-    e1 = v1 / robust_norm(v1, axis=-1, keepdims=True)
-    c = jnp.einsum('li, li -> l', e1, v2)[:,None]
-    e2 = v2 - c * e1
-    e2 = e2 / robust_norm(e2, axis=-1, keepdims=True)
-    e3 = jnp.cross(e1, e2, axis=-1)
-    return jnp.concatenate([e1[:,:,None], e2[:,:,None], e3[:,:,None]], axis=-1)
-
-  def get_ij(R,T):
-    return jnp.einsum('rji,rsj->rsi',R,T[None,:]-T[:,None])
-
-  def loss_fn(t,p,m):
-    fape = robust_norm(t-p)
-    fape = jnp.clip(fape, 0, clamp) / 10.0
-    return fape, (fape*m).sum((-1,-2))/(m.sum((-1,-2)) + 1e-8)
-
-  true = inputs["batch"]["all_atom_positions"]
-  pred = outputs["structure_module"]["final_atom_positions"]
-
-  N,CA,C = (residue_constants.atom_order[k] for k in ["N","CA","C"])
-
-  true_mask = jnp.where(inputs["seq_mask"][:,None],inputs["batch"]["all_atom_mask"],0)
-  weights = true_mask[:,N] * true_mask[:,CA] * true_mask[:,C]
-
-  true = get_ij(get_R(true[:,N],true[:,CA],true[:,C]),true[:,CA])
-  pred = get_ij(get_R(pred[:,N],pred[:,CA],pred[:,C]),pred[:,CA])
-
-  return _get_pw_loss(true, pred, loss_fn, weights=weights, copies=copies, return_mtx=return_mtx)
-
-def _get_pw_loss(true, pred, loss_fn, weights=None, copies=1, return_mtx=False):
-  length = true.shape[0]
-  
-  if weights is None:
-    weights = jnp.ones(length)
-  
-  F = {"t":true, "p":pred, "m":weights[:,None] * weights[None,:]}  
-
-  if copies > 1:
-    (L,C) = (length//copies, copies-1)
-
-    # intra (L,L,F)
-    intra = jax.tree_map(lambda x:x[:L,:L], F)
-    mtx, loss = loss_fn(**intra)
-
-    # inter (C*L,L,F)
-    inter = jax.tree_map(lambda x:x[L:,:L], F)
-    if C == 0:
-      i_mtx, i_loss = loss_fn(**inter)
-
-    else:
-      # (C,L,L,F)
-      inter = jax.tree_map(lambda x:x.reshape(C,L,L,-1), inter)
-      inter = {"t":inter["t"][:,None],        # (C,1,L,L,F)
-               "p":inter["p"][None,:],        # (1,C,L,L,F)
-               "m":inter["m"][:,None,:,:,0]}  # (C,1,L,L)             
-      
-      # (C,C,L,L,F) → (C,C,L,L) → (C,C) → (C) → ()
-      i_mtx, i_loss = loss_fn(**inter)
-      i_loss = sum([i_loss.min(i).sum() for i in [0,1]]) / 2
-
-    total_loss = (loss + i_loss) / copies
-    return (mtx, i_mtx) if return_mtx else total_loss
-
-  else:
-    mtx, loss = loss_fn(**F)
-    return mtx if return_mtx else loss
-  
-def get_rmsd_loss(inputs, outputs, L=None, include_L=True, copies=1):
-  batch = inputs["batch"]
-  true = batch["all_atom_positions"][:,1]
-  pred = outputs["structure_module"]["final_atom_positions"][:,1]
-  weights = jnp.where(inputs["seq_mask"],batch["all_atom_mask"][:,1],0)
-  return _get_rmsd_loss(true, pred, weights=weights, L=L, include_L=include_L, copies=copies)
-
-def _get_rmsd_loss(true, pred, weights=None, L=None, include_L=True, copies=1):
-  '''
-  get rmsd + alignment function
-  align based on the first L positions, computed weighted rmsd using all 
-  positions (if include_L=True) or remaining positions (if include_L=False).
-  '''
-  # normalize weights
-  length = true.shape[-2]
-  if weights is None:
-    weights = (jnp.ones(length)/length)[...,None]
-  else:
-    weights = (weights/(weights.sum(-1,keepdims=True) + 1e-8))[...,None]
-
-  # determine alignment [L]ength and remaining [l]ength
-  if copies > 1:
-    if L is None:
-      L = iL = length // copies; C = copies-1
-    else:
-      (iL,C) = ((length-L) // copies, copies)
-  else:
-    (L,iL,C) = (length,0,0) if L is None else (L,length-L,1)
-
-  # slice inputs
-  if iL == 0:
-    (T,P,W) = (true,pred,weights)
-  else:
-    (T,P,W) = (x[...,:L,:] for x in (true,pred,weights))
-    (iT,iP,iW) = (x[...,L:,:] for x in (true,pred,weights))
-
-  # get alignment and rmsd functions
-  (T_mu,P_mu) = ((x*W).sum(-2,keepdims=True)/W.sum((-1,-2)) for x in (T,P))
-  aln = _np_kabsch((P-P_mu)*W, T-T_mu)   
-  align_fn = lambda x: (x - P_mu) @ aln + T_mu
-  msd_fn = lambda t,p,w: (w*jnp.square(align_fn(p)-t)).sum((-1,-2))
-  
-  # compute rmsd
-  if iL == 0:
-    msd = msd_fn(true,pred,weights)
-  elif C > 1:
-    # all vs all alignment of remaining, get min RMSD
-    iT = iT.reshape(-1,C,1,iL,3).swapaxes(0,-3)
-    iP = iP.reshape(-1,1,C,iL,3).swapaxes(0,-3)
-    imsd = msd_fn(iT, iP, iW.reshape(-1,C,1,iL,1).swapaxes(0,-3))
-    imsd = (imsd.min(0).sum(0) + imsd.min(1).sum(0)) / 2 
-    imsd = imsd.reshape(jnp.broadcast_shapes(true.shape[:-2],pred.shape[:-2]))
-    msd = (imsd + msd_fn(T,P,W)) if include_L else (imsd/iW.sum((-1,-2)))
-  else:
-    msd = msd_fn(true,pred,weights) if include_L else (msd_fn(iT,iP,iW)/iW.sum((-1,-2)))
-  rmsd = jnp.sqrt(msd + 1e-8)
-
-  return {"rmsd":rmsd, "align":align_fn}
-
-def _get_sc_rmsd_loss(true, pred, sc):
-  '''get sidechain rmsd + alignment function'''
-
-  # select atoms
-  (T, P) = (true.reshape(-1,3), pred.reshape(-1,3))
-  (T, T_alt, P) = (T[sc["pos"]], T[sc["pos_alt"]], P[sc["pos"]])
-
-  # select non-ambigious atoms
-  (T_na, P_na) = (T[sc["non_amb"]], P[sc["non_amb"]])
-
-  # get alignment of non-ambigious atoms
-  if "weight_non_amb" in sc:
-    T_mu_na = (T_na * sc["weight_non_amb"]).sum(0)
-    P_mu_na = (P_na * sc["weight_non_amb"]).sum(0)
-    aln = _np_kabsch((P_na-P_mu_na) * sc["weight_non_amb"], T_na-T_mu_na)
-  else:
-    T_mu_na, P_mu_na = T_na.mean(0), P_na.mean(0)
-    aln = _np_kabsch(P_na-P_mu_na, T_na-T_mu_na)
-
-  # apply alignment to all atoms
-  align_fn = lambda x: (x - P_mu_na) @ aln + T_mu_na
-  P = align_fn(P)
-
-  # compute rmsd
-  sd = jnp.minimum(jnp.square(P-T).sum(-1), jnp.square(P-T_alt).sum(-1))
-  if "weight" in sc:
-    msd = (sd*sc["weight"]).sum()
-  else:
-    msd = sd.mean()
-  rmsd = jnp.sqrt(msd + 1e-8)
-  return {"rmsd":rmsd, "align":align_fn}
-
-def get_seq_ent_loss(inputs):
-  opt = inputs["opt"]
-  x = inputs["seq"]["logits"] / opt["temp"]
-  ent = -(jax.nn.softmax(x) * jax.nn.log_softmax(x)).sum(-1)
-  mask = inputs["seq_mask"][-x.shape[1]:]
-  if "fix_pos" in opt:
-    if "pos" in opt:
-      p = opt["pos"][opt["fix_pos"]]
-    else:
-      p = opt["fix_pos"]
-    mask = mask.at[p].set(0)
-  ent = (ent * mask).sum() / (mask.sum() + 1e-8)
-  return {"seq_ent":ent.mean()}
-
-def get_mlm_loss(outputs, mask, truth=None):
-  x = outputs["masked_msa"]["logits"][...,:20]
-  if truth is None: truth = jax.nn.softmax(x)
-  ent = -(truth[...,:20] * jax.nn.log_softmax(x)).sum(-1)
-  ent = (ent * mask).sum(-1) / (mask.sum() + 1e-8)
-  return {"mlm":ent.mean()}
\ No newline at end of file
diff --git a/colabdesign/af/model.py b/colabdesign/af/model.py
index 37053bd..7c289fd 100644
--- a/colabdesign/af/model.py
+++ b/colabdesign/af/model.py
@@ -1,33 +1,1121 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+##################################################
+#####  alphafold-prep
+#################################################
 import os
 import jax
 import jax.numpy as jnp
 import numpy as np
-from inspect import signature
+import re
+import torch
+from colabdesign.af.alphafold.model import data, config, model, all_atom  # directly import from ooaf2
+
+from colabdesign.af.alphafold.data import prep_inputs         # pipeline,
+from colabdesign.af.alphafold.common import protein, residue_constants  # directly import from ooaf2
+from colabdesign.af.alphafold.model.tf import shape_placeholders
+from colabdesign.af.alphafold.model import config
+
+from colabdesign.shared.model import Key, copy_dict, design_model, _np_get_cb, pdb_to_string, prep_pos, copy_dict, order_aa
+from colabdesign.shared.model import update_dict, dict_to_str, to_float, softmax, categorical, to_list, copy_missing
+from colabdesign.shared.model import soft_seq, renum_pdb_str
+
+#from colabdesign.af.loss import _af_loss #, get_plddt, get_pae, get_ptm
+#from colabdesign.af.loss import get_contact_map, get_seq_ent_loss, get_mlm_loss
+
+resname_to_idx = residue_constants.resname_to_idx
+idx_to_resname = dict((v,k) for k,v in resname_to_idx.items())
+
+#################################################
+# AF_PREP - input prep functions
+#################################################
+class _af_prep:
+
+  def _prep_model(self, **kwargs):
+    '''prep model'''
+    if not hasattr(self,"_model") or self._cfg != self._model["runner"].config:
+      self._cfg.model.global_config.subbatch_size = None
+      self._model = self._get_model(self._cfg)
+      if sum(self._lengths) > 384:
+        self._cfg.model.global_config.subbatch_size = 4
+        self._model["fn"] = self._get_model(self._cfg)["fn"]
+
+    self._opt = copy_dict(self.opt)
+    self.restart(**kwargs)
+
+  def _prep_features(self, num_res, num_seq=None, num_templates=1):
+    '''process features'''
+    if num_seq is None: num_seq = self._num
+    return prep_input_features(L=num_res, N=num_seq, T=num_templates)
+
+  def _prep_fixbb(self, pdb_filename, chain=None,
+                  copies=1, repeat=False, homooligomer=False,
+                  rm_template=False,
+                  rm_template_seq=True,
+                  rm_template_sc=True,
+                  rm_template_ic=False,
+                  fix_pos=None, ignore_missing=True, **kwargs):
+    '''
+    prep inputs for fixed backbone design
+    ---------------------------------------------------
+    if copies > 1:
+      -homooligomer=True - input pdb chains are parsed as homo-oligomeric units
+      -repeat=True       - tie the repeating sequence within single chain
+    -rm_template_seq     - if template is defined, remove information about template sequence
+    -fix_pos="1,2-10"    - specify which positions to keep fixed in the sequence
+                           note: supervised loss is applied to all positions, use "partial"
+                           protocol to apply supervised loss to only subset of positions
+    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
+    ---------------------------------------------------
+    '''
+    # prep features
+    self._pdb = prep_pdb(pdb_filename, chain=chain, ignore_missing=ignore_missing,
+                         offsets=kwargs.pop("pdb_offsets",None),
+                         lengths=kwargs.pop("pdb_lengths",None))
+
+    self._len = self._pdb["residue_index"].shape[0]
+    self._lengths = [self._len]
+
+    # feat dims
+    num_seq = self._num
+    res_idx = self._pdb["residue_index"]
+
+    # get [pos]itions of interests
+    if fix_pos is not None and fix_pos != "":
+      self._pos_info = prep_pos(fix_pos, **self._pdb["idx"])
+      self.opt["fix_pos"] = self._pos_info["pos"]
+
+    if homooligomer and chain is not None and copies == 1:
+      copies = len(chain.split(","))
+
+    # repeat/homo-oligomeric support
+    if copies > 1:
+
+      if repeat or homooligomer:
+        self._len = self._len // copies
+        if "fix_pos" in self.opt:
+          self.opt["fix_pos"] = self.opt["fix_pos"][self.opt["fix_pos"] < self._len]
+
+      if repeat:
+        self._lengths = [self._len * copies]
+        block_diag = False
+
+      else:
+        self._lengths = [self._len] * copies
+        block_diag = not self._args["use_multimer"]
+
+        res_idx = repeat_idx(res_idx[:self._len], copies)
+        num_seq = (self._num * copies + 1) if block_diag else self._num
+        self.opt["weights"].update({"i_pae":0.0, "i_con":0.0})
+
+      self._args.update({"copies":copies, "repeat":repeat, "homooligomer":homooligomer, "block_diag":block_diag})
+      homooligomer = not repeat
+    else:
+      self._lengths = self._pdb["lengths"]
+
+    # configure input features
+    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
+    self._inputs["residue_index"] = res_idx
+    self._inputs["batch"] = make_fixed_size(self._pdb["batch"], num_res=sum(self._lengths))
+    self._inputs.update(get_multi_id(self._lengths, homooligomer=homooligomer))
+    self._inputs['seq_length'] = np.array([num_seq] * num_seq, dtype=np.int32)
+
+
+    # configure options/weights
+    self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0, "con":0.0})
+    self._wt_aatype = self._inputs["batch"]["aatype"][:self._len]
+
+    # configure template [opt]ions
+    rm,L = {},sum(self._lengths)
+    for n,x in {"rm_template":    rm_template,
+                "rm_template_seq":rm_template_seq,
+                "rm_template_sc": rm_template_sc}.items():
+      rm[n] = np.full(L,False)
+      if isinstance(x,str):
+        rm[n][prep_pos(x,**self._pdb["idx"])["pos"]] = True
+      else:
+        rm[n][:] = x
+    self.opt["template"]["rm_ic"] = rm_template_ic
+    self._inputs.update(rm)
+
+    self._prep_model(**kwargs)
+
+  def _prep_hallucination(self, length=100, copies=1, repeat=False, **kwargs):
+    '''
+    prep inputs for hallucination
+    ---------------------------------------------------
+    if copies > 1:
+      -repeat=True - tie the repeating sequence within single chain
+    ---------------------------------------------------
+    '''
+
+    # define num copies (for repeats/ homo-oligomers)
+    if not repeat and copies > 1 and not self._args["use_multimer"]:
+      (num_seq, block_diag) = (self._num * copies + 1, True)
+    else:
+      (num_seq, block_diag) = (self._num, False)
+
+    self._args.update({"repeat":repeat,"block_diag":block_diag,"copies":copies})
+
+    # prep features
+    self._len = length
+
+    # set weights
+    self.opt["weights"].update({"con":1.0})
+    if copies > 1:
+      if repeat:
+        offset = 1
+        self._lengths = [self._len * copies]
+        self._args["repeat"] = True
+      else:
+        offset = 50
+        self._lengths = [self._len] * copies
+        self.opt["weights"].update({"i_pae":0.0, "i_con":1.0})
+        self._args["homooligomer"] = True
+      res_idx = repeat_idx(np.arange(length), copies, offset=offset)
+    else:
+      self._lengths = [self._len]
+      res_idx = np.arange(length)
+
+    # configure input features
+    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
+    self._inputs["residue_index"] = res_idx
+    self._inputs.update(get_multi_id(self._lengths, homooligomer=True))
+    self._inputs['seq_length'] = np.array([num_seq] * num_seq, dtype=np.int32)
+
+    self._prep_model(**kwargs)
+
+  def _prep_binder(self, pdb_filename,
+                   target_chain="A", binder_len=50,
+                   rm_target = False,
+                   rm_target_seq = False,
+                   rm_target_sc = False,
+
+                   # if binder_chain is defined
+                   binder_chain=None,
+                   rm_binder=True,
+                   rm_binder_seq=True,
+                   rm_binder_sc=True,
+                   rm_template_ic=False,
+
+                   hotspot=None, ignore_missing=True, **kwargs):
+    '''
+    prep inputs for binder design
+    ---------------------------------------------------
+    -binder_len = length of binder to hallucinate (option ignored if binder_chain is defined)
+    -binder_chain = chain of binder to redesign
+    -use_binder_template = use binder coordinates as template input
+    -rm_template_ic = use target and binder coordinates as seperate template inputs
+    -hotspot = define position/hotspots on target
+    -rm_[binder/target]_seq = remove sequence info from template
+    -rm_[binder/target]_sc  = remove sidechain info from template
+    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
+    ---------------------------------------------------
+    '''
+    redesign = binder_chain is not None
+    rm_binder = not kwargs.pop("use_binder_template", not rm_binder)
+
+    self._args.update({"redesign":redesign})
+
+    # get pdb info
+    target_chain = kwargs.pop("chain",target_chain) # backward comp
+    chains = f"{target_chain},{binder_chain}" if redesign else target_chain
+    im = [True] * len(target_chain.split(","))
+    if redesign: im += [ignore_missing] * len(binder_chain.split(","))
+
+    self._pdb = prep_pdb(pdb_filename, chain=chains, ignore_missing=im)
+    res_idx = self._pdb["residue_index"]
+
+    if redesign:
+      self._target_len = sum([(self._pdb["idx"]["chain"] == c).sum() for c in target_chain.split(",")])
+      self._binder_len = sum([(self._pdb["idx"]["chain"] == c).sum() for c in binder_chain.split(",")])
+    else:
+      self._target_len = self._pdb["residue_index"].shape[0]
+      self._binder_len = binder_len
+      res_idx = np.append(res_idx, res_idx[-1] + np.arange(binder_len) + 50)
+
+    self._len = self._binder_len
+    self._lengths = [self._target_len, self._binder_len]
+
+    # gather hotspot info
+    if hotspot is not None:
+      self.opt["hotspot"] = prep_pos(hotspot, **self._pdb["idx"])["pos"]
+
+    if redesign:
+      # binder redesign
+      self._wt_aatype = self._pdb["batch"]["aatype"][self._target_len:]
+      self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0,
+                                  "con":0.0, "i_con":0.0, "i_pae":0.0})
+    else:
+      # binder hallucination
+      self._pdb["batch"] = make_fixed_size(self._pdb["batch"], num_res=sum(self._lengths))
+      self.opt["weights"].update({"plddt":0.1, "con":0.0, "i_con":1.0, "i_pae":0.0})
+    num_seq = self._num
+    # configure input features
+    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=1)
+    self._inputs["residue_index"] = res_idx
+    self._inputs["batch"] = self._pdb["batch"]
+    self._inputs.update(get_multi_id(self._lengths))
+    self._inputs['seq_length'] = np.array([num_seq] * num_seq, dtype=np.int32)
+
+    # configure template rm masks
+    (T,L,rm) = (self._lengths[0],sum(self._lengths),{})
+    rm_opt = {
+              "rm_template":    {"target":rm_target,    "binder":rm_binder},
+              "rm_template_seq":{"target":rm_target_seq,"binder":rm_binder_seq},
+              "rm_template_sc": {"target":rm_target_sc, "binder":rm_binder_sc}
+             }
+    for n,x in rm_opt.items():
+      rm[n] = np.full(L,False)
+      for m,y in x.items():
+        if isinstance(y,str):
+          rm[n][prep_pos(y,**self._pdb["idx"])["pos"]] = True
+        else:
+          if m == "target": rm[n][:T] = y
+          if m == "binder": rm[n][T:] = y
+
+    # set template [opt]ions
+    self.opt["template"]["rm_ic"] = rm_template_ic
+    self._inputs.update(rm)
+
+    self._prep_model(**kwargs)
+
+  def _prep_partial(self, pdb_filename, chain=None, length=None,
+                    copies=1, repeat=False, homooligomer=False,
+                    pos=None, fix_pos=None, use_sidechains=False, atoms_to_exclude=None,
+                    rm_template=False,
+                    rm_template_seq=False,
+                    rm_template_sc=False,
+                    rm_template_ic=False,
+                    ignore_missing=True, **kwargs):
+    '''
+    prep input for partial hallucination
+    ---------------------------------------------------
+    -length=100 - total length of protein (if different from input PDB)
+    -pos="1,2-10" - specify which positions to apply supervised loss to
+    -use_sidechains=True - add a sidechain supervised loss to the specified positions
+      -atoms_to_exclude=["N","C","O"] (for sc_rmsd loss, specify which atoms to exclude)
+    -rm_template_seq - if template is defined, remove information about template sequence
+    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
+    ---------------------------------------------------
+    '''
+    # prep features
+    self._pdb = prep_pdb(pdb_filename, chain=chain, ignore_missing=ignore_missing,
+                   offsets=kwargs.pop("pdb_offsets",None),
+                   lengths=kwargs.pop("pdb_lengths",None))
+
+    self._pdb["len"] = sum(self._pdb["lengths"])
+
+    self._len = self._pdb["len"] if length is None else length
+    self._lengths = [self._len]
+
+    # feat dims
+    num_seq = self._num
+    res_idx = np.arange(self._len)
+
+    # get [pos]itions of interests
+    if pos is None:
+      self.opt["pos"] = self._pdb["pos"] = np.arange(self._pdb["len"])
+      self._pos_info = {"length":np.array([self._pdb["len"]]), "pos":self._pdb["pos"]}
+    else:
+      self._pos_info = prep_pos(pos, **self._pdb["idx"])
+      self.opt["pos"] = self._pdb["pos"] = self._pos_info["pos"]
+
+    if homooligomer and chain is not None and copies == 1:
+      copies = len(chain.split(","))
+
+    # repeat/homo-oligomeric support
+    if copies > 1:
+
+      if repeat or homooligomer:
+        self._len = self._len // copies
+        self._pdb["len"] = self._pdb["len"] // copies
+        self.opt["pos"] = self._pdb["pos"][self._pdb["pos"] < self._pdb["len"]]
+
+        # repeat positions across copies
+        self._pdb["pos"] = repeat_pos(self.opt["pos"], copies, self._pdb["len"])
+
+      if repeat:
+        self._lengths = [self._len * copies]
+        block_diag = False
+
+      else:
+        self._lengths = [self._len] * copies
+        block_diag = not self._args["use_multimer"]
+
+        num_seq = (self._num * copies + 1) if block_diag else self._num
+        res_idx = repeat_idx(np.arange(self._len), copies)
+
+        self.opt["weights"].update({"i_pae":0.0, "i_con":1.0})
+
+      self._args.update({"copies":copies, "repeat":repeat, "homooligomer":homooligomer, "block_diag":block_diag})
+      homooligomer = not repeat
+
+    # configure input features
+    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
+    self._inputs["residue_index"] = res_idx
+    self._inputs["batch"] = jax.tree_map(lambda x:x[self._pdb["pos"]], self._pdb["batch"])
+    self._inputs.update(get_multi_id(self._lengths, homooligomer=homooligomer))
+    self._inputs['seq_length'] = np.array([num_seq] * num_seq, dtype=np.int32)
+
+    # configure options/weights
+    self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0, "con":1.0})
+    self._wt_aatype = self._pdb["batch"]["aatype"][self.opt["pos"]]
+
+    # configure sidechains
+    self._args["use_sidechains"] = use_sidechains
+    if use_sidechains:
+      self._sc = {"batch":prep_inputs.make_atom14_positions(self._inputs["batch"]),
+                  "pos":get_sc_pos(self._wt_aatype, atoms_to_exclude)}
+      self.opt["weights"].update({"sc_rmsd":0.1, "sc_fape":0.1})
+      self.opt["fix_pos"] = np.arange(self.opt["pos"].shape[0])
+      self._wt_aatype_sub = self._wt_aatype
+
+    elif fix_pos is not None and fix_pos != "":
+      sub_fix_pos = []
+      sub_i = []
+      pos = self.opt["pos"].tolist()
+      for i in prep_pos(fix_pos, **self._pdb["idx"])["pos"]:
+        if i in pos:
+          sub_i.append(i)
+          sub_fix_pos.append(pos.index(i))
+      self.opt["fix_pos"] = np.array(sub_fix_pos)
+      self._wt_aatype_sub = self._pdb["batch"]["aatype"][sub_i]
+
+    elif kwargs.pop("fix_seq",False):
+      self.opt["fix_pos"] = np.arange(self.opt["pos"].shape[0])
+      self._wt_aatype_sub = self._wt_aatype
+
+    self.opt["template"].update({"rm_ic":rm_template_ic})
+    self._inputs.update({"rm_template":     rm_template,
+                         "rm_template_seq": rm_template_seq,
+                         "rm_template_sc":  rm_template_sc})
+
+    self._prep_model(**kwargs)
+
+#######################
+# utils
+#######################
+def repeat_idx(idx, copies=1, offset=50):
+  idx_offset = np.repeat(np.cumsum([0]+[idx[-1]+offset]*(copies-1)),len(idx))
+  return np.tile(idx,copies) + idx_offset
+
+def repeat_pos(pos, copies, length):
+  return (np.repeat(pos,copies).reshape(-1,copies) + np.arange(copies) * length).T.flatten()
+
+def prep_pdb(pdb_filename, chain=None,
+             offsets=None, lengths=None,
+             ignore_missing=False):
+  '''extract features from pdb'''
+
+  def add_cb(batch):
+    '''add missing CB atoms based on N,CA,C'''
+    p,m = batch["all_atom_positions"], batch["all_atom_mask"]
+    atom_idx = residue_constants.atom_order
+    atoms = {k:p[...,atom_idx[k],:] for k in ["N","CA","C"]}
+    cb = atom_idx["CB"]
+    cb_atoms = _np_get_cb(**atoms, use_jax=False)
+    cb_mask = np.prod([m[...,atom_idx[k]] for k in ["N","CA","C"]],0)
+    batch["all_atom_positions"][...,cb,:] = np.where(m[:,cb,None], p[:,cb,:], cb_atoms)
+    batch["all_atom_mask"][...,cb] = (m[:,cb] + cb_mask) > 0
+    return {"atoms":batch["all_atom_positions"][:,cb],"mask":cb_mask}
+
+  if isinstance(chain,str) and "," in chain:
+    chains = chain.split(",")
+  elif not isinstance(chain,list):
+    chains = [chain]
+
+  o,last = [],0
+  residue_idx, chain_idx = [],[]
+  full_lengths = []
+
+  # go through each defined chain
+  for n,chain in enumerate(chains):
+    pdb_str = pdb_to_string(pdb_filename, chains=chain, models=[1])
+    protein_obj = protein.from_pdb_string(pdb_str, chain_id=chain)
+    batch = {'aatype': protein_obj.aatype,
+             'all_atom_positions': protein_obj.atom_positions,
+             'all_atom_mask': protein_obj.atom_mask,
+             'residue_index': protein_obj.residue_index}
+
+    cb_feat = add_cb(batch) # add in missing cb (in the case of glycine)
+
+    im = ignore_missing[n] if isinstance(ignore_missing,list) else ignore_missing
+    if im:
+      r = batch["all_atom_mask"][:,0] == 1
+      batch = jax.tree_map(lambda x:x[r], batch)
+      residue_index = batch["residue_index"] + last
+
+    else:
+      # pad values
+      offset = 0 if offsets is None else (offsets[n] if isinstance(offsets,list) else offsets)
+      r = offset + (protein_obj.residue_index - protein_obj.residue_index.min())
+      length = (r.max()+1) if lengths is None else (lengths[n] if isinstance(lengths,list) else lengths)
+      def scatter(x, value=0):
+        shape = (length,) + x.shape[1:]
+        y = np.full(shape, value, dtype=x.dtype)
+        y[r] = x
+        return y
+
+      batch = {"aatype":scatter(batch["aatype"],-1),
+               "all_atom_positions":scatter(batch["all_atom_positions"]),
+               "all_atom_mask":scatter(batch["all_atom_mask"]),
+               "residue_index":scatter(batch["residue_index"],-1)}
+
+      residue_index = np.arange(length) + last
+
+    last = residue_index[-1] + 50
+    o.append({"batch":batch,
+              "residue_index": residue_index,
+              "cb_feat":cb_feat})
+
+    residue_idx.append(batch.pop("residue_index"))
+    chain_idx.append([chain] * len(residue_idx[-1]))
+    full_lengths.append(len(residue_index))
+
+  # concatenate chains
+  o = jax.tree_util.tree_map(lambda *x:np.concatenate(x,0),*o)
+
+  # save original residue and chain index
+  o["idx"] = {"residue":np.concatenate(residue_idx), "chain":np.concatenate(chain_idx)}
+  o["lengths"] = full_lengths
+  return o
+
+def make_fixed_size(feat, num_res, num_seq=1, num_templates=1):
+  '''pad input features'''
+  shape_schema = {k:v for k,v in config.CONFIG.data.eval.feat.items()}
+
+  pad_size_map = {
+      shape_placeholders.NUM_RES: num_res,
+      shape_placeholders.NUM_MSA_SEQ: num_seq,
+      shape_placeholders.NUM_EXTRA_SEQ: 1,
+      shape_placeholders.NUM_TEMPLATES: num_templates
+  }
+  for k,v in feat.items():
+    if k == "batch":
+      feat[k] = make_fixed_size(v, num_res)
+    else:
+      shape = list(v.shape)
+      schema = shape_schema[k]
+      assert len(shape) == len(schema), (
+          f'Rank mismatch between shape and shape schema for {k}: '
+          f'{shape} vs {schema}')
+      pad_size = [pad_size_map.get(s2, None) or s1 for (s1, s2) in zip(shape, schema)]
+      padding = [(0, p - v.shape[i]) for i, p in enumerate(pad_size)]
+      feat[k] = np.pad(v, padding)
+  return feat
+
+def get_sc_pos(aa_ident, atoms_to_exclude=None):
+  '''get sidechain indices/weights for all_atom14_positions'''
+
+  # decide what atoms to exclude for each residue type
+  a2e = {}
+  for r in resname_to_idx:
+    if isinstance(atoms_to_exclude,dict):
+      a2e[r] = atoms_to_exclude.get(r,atoms_to_exclude.get("ALL",["N","C","O"]))
+    else:
+      a2e[r] = ["N","C","O"] if atoms_to_exclude is None else atoms_to_exclude
+
+  # collect atom indices
+  pos,pos_alt = [],[]
+  N,N_non_amb = [],[]
+  for n,a in enumerate(aa_ident):
+    aa = idx_to_resname[a]
+    atoms = set(residue_constants.residue_atoms[aa])
+    atoms14 = residue_constants.restype_name_to_atom14_names[aa]
+    swaps = residue_constants.residue_atom_renaming_swaps.get(aa,{})
+    swaps.update({v:k for k,v in swaps.items()})
+    for atom in atoms.difference(a2e[aa]):
+      pos.append(n * 14 + atoms14.index(atom))
+      if atom in swaps:
+        pos_alt.append(n * 14 + atoms14.index(swaps[atom]))
+      else:
+        pos_alt.append(pos[-1])
+        N_non_amb.append(n)
+      N.append(n)
+
+  pos, pos_alt = np.asarray(pos), np.asarray(pos_alt)
+  non_amb = pos == pos_alt
+  N, N_non_amb = np.asarray(N), np.asarray(N_non_amb)
+  w = np.array([1/(n == N).sum() for n in N])
+  w_na = np.array([1/(n == N_non_amb).sum() for n in N_non_amb])
+  w, w_na = w/w.sum(), w_na/w_na.sum()
+  return {"pos":pos, "pos_alt":pos_alt, "non_amb":non_amb,
+          "weight":w, "weight_non_amb":w_na[:,None]}
+
+def prep_input_features(L, N=1, T=1, eN=1):
+  '''
+  given [L]ength, [N]umber of sequences and number of [T]emplates
+  return dictionary of blank features
+  '''
+  inputs = {'aatype': np.zeros(L,int),
+            'target_feat': np.zeros((L,20)),
+            'msa_feat': np.zeros((N,L,49)),
+            # 23 = one_hot -> (20, UNK, GAP, MASK)
+            # 1  = has deletion
+            # 1  = deletion_value
+            # 23 = profile
+            # 1  = deletion_mean_value
+
+            'seq_mask': np.ones(L),
+            'msa_mask': np.ones((N,L)),
+            'msa_row_mask': np.ones(N),
+            'atom14_atom_exists': np.zeros((L,14)),
+            'atom37_atom_exists': np.zeros((L,37)),
+            'residx_atom14_to_atom37': np.zeros((L,14),int),
+            'residx_atom37_to_atom14': np.zeros((L,37),int),
+            'residue_index': np.arange(L),
+            'extra_deletion_value': np.zeros((eN,L)),
+            'extra_has_deletion': np.zeros((eN,L)),
+            'extra_msa': np.zeros((eN,L),int),
+            'extra_msa_mask': np.zeros((eN,L)),
+            'extra_msa_row_mask': np.zeros(eN),
+
+            # for template inputs
+            'template_aatype': np.zeros((T,L),int),
+            'template_all_atom_mask': np.zeros((T,L,37)),
+            'template_all_atom_positions': np.zeros((T,L,37,3)),
+            'template_mask': np.zeros(T),
+            'template_pseudo_beta': np.zeros((T,L,3)),
+            'template_pseudo_beta_mask': np.zeros((T,L)),
+
+            # for alphafold-multimer
+            'asym_id': np.zeros(L),
+            'sym_id': np.zeros(L),
+            'entity_id': np.zeros(L),
+            'all_atom_positions': np.zeros((N,37,3))}
+  return inputs
+
+def get_multi_id(lengths, homooligomer=False):
+  '''set info for alphafold-multimer'''
+  i = np.concatenate([[n]*l for n,l in enumerate(lengths)])
+  if homooligomer:
+    return {"asym_id":i, "sym_id":i, "entity_id":np.zeros_like(i)}
+  else:
+    return {"asym_id":i, "sym_id":i, "entity_id":i}
+
+#################################################################
+######## alphafold- design
+#################################################################
+class _af_design:
+
+  def restart(self, seed=None, opt=None, weights=None,
+              seq=None, mode=None, keep_history=False, reset_opt=True, **kwargs):
+    '''
+    restart the optimization
+    ------------
+    note: model.restart() resets the [opt]ions and weights to their defaults
+    use model.set_opt(..., set_defaults=True) and model.set_weights(..., set_defaults=True)
+    or model.restart(reset_opt=False) to avoid this
+    ------------
+    seed=0 - set seed for reproducibility
+    reset_opt=False - do NOT reset [opt]ions/weights to defaults
+    keep_history=True - do NOT clear the trajectory/[opt]ions/weights
+    '''
+    # reset [opt]ions
+    if reset_opt and not keep_history:
+      copy_missing(self.opt, self._opt)
+      self.opt = copy_dict(self._opt)
+      if hasattr(self,"aux"): del self.aux
+
+    if not keep_history:
+      # initialize trajectory
+      self._tmp = {"traj":{"seq":[],"xyz":[],"plddt":[],"pae":[]},
+                   "log":[],"best":{}}
+
+    # update options/settings (if defined)
+    self.set_opt(opt)
+    self.set_weights(weights)
+
+    # initialize sequence
+    self.set_seed(seed)
+    self.set_seq(seq=seq, mode=mode, **kwargs)
 
-from colabdesign.af.alphafold.model import data, config, model, all_atom
+    # reset optimizer
+    self._k = 0
+    self.set_optimizer()
 
-from colabdesign.shared.model import design_model
-from colabdesign.shared.utils import Key
+  def _get_model_nums(self, num_models=None, sample_models=None, models=None):
+    '''decide which model params to use'''
+    if num_models is None: num_models = self.opt["num_models"]
+    if sample_models is None: sample_models = self.opt["sample_models"]
 
-from colabdesign.af.prep   import _af_prep
-from colabdesign.af.loss   import _af_loss, get_plddt, get_pae, get_ptm
-from colabdesign.af.loss   import get_contact_map, get_seq_ent_loss, get_mlm_loss
-from colabdesign.af.utils  import _af_utils
-from colabdesign.af.design import _af_design
-from colabdesign.af.inputs import _af_inputs, update_seq, update_aatype
+    ns_name = self._model_names
+    ns = list(range(len(ns_name)))
+    if models is not None:
+      models = models if isinstance(models,list) else [models]
+      ns = [ns[n if isinstance(n,int) else ns_name.index(n)] for n in models]
 
-################################################################
-# MK_DESIGN_MODEL - initialize model, and put it all together
-################################################################
+    m = min(num_models,len(ns))
+    if sample_models and m != len(ns):
+      model_nums = np.random.choice(ns,(m,),replace=False)
+    else:
+      model_nums = ns[:m]
+    return model_nums
+
+  def run(self, num_recycles=None, num_models=None, sample_models=None, models=None,
+          backprop=True, callback=None, model_nums=None, return_aux=False):
+    '''run model to get outputs, losses and gradients'''
+
+    # pre-design callbacks
+    for fn in self._callbacks["design"]["pre"]: fn(self)
+
+    # decide which model params to use
+    if model_nums is None:
+      model_nums = self._get_model_nums(num_models, sample_models, models)
+    assert len(model_nums) > 0, "ERROR: no model params defined"
+
+    # loop through model params
+    auxs = []
+    for n in model_nums:
+      p = self._model_params[n]
+      auxs.append(self._recycle(p, num_recycles=num_recycles, backprop=backprop))
+    auxs = jax.tree_map(lambda *x: np.stack(x), *auxs)
+
+    # update aux (average outputs)
+    """def avg_or_first(x):
+      if np.issubdtype(x.dtype, np.integer): return x[0]
+      else: return x.mean(0)
+
+    self.aux = jax.tree_map(avg_or_first, auxs)
+    self.aux["atom_positions"] = auxs["atom_positions"][0]
+    self.aux["all"] = auxs
+
+    # post-design callbacks
+    for fn in (self._callbacks["design"]["post"] + to_list(callback)): fn(self)
+
+    # update log
+    self.aux["log"] = {**self.aux["losses"]}
+    self.aux["log"]["plddt"] = 1 - self.aux["log"]["plddt"]
+    for k in ["loss","i_ptm","ptm"]: self.aux["log"][k] = self.aux[k]
+    for k in ["hard","soft","temp"]: self.aux["log"][k] = self.opt[k]
+
+    # compute sequence recovery
+    if self.protocol in ["fixbb","partial"] or (self.protocol == "binder" and self._args["redesign"]):
+      if self.protocol == "partial":
+        aatype = self.aux["aatype"][...,self.opt["pos"]]
+      else:
+        aatype = self.aux["seq"]["pseudo"].argmax(-1)
+
+      mask = self._wt_aatype != -1
+      true = self._wt_aatype[mask]
+      pred = aatype[...,mask]
+      self.aux["log"]["seqid"] = (true == pred).mean()
+
+    self.aux["log"] = to_float(self.aux["log"])
+    self.aux["log"].update({"recycles":int(self.aux["num_recycles"]),
+                            "models":model_nums})
+
+    if return_aux: return self.aux"""
+
+  def _single(self, model_params, backprop=True):
+    '''single pass through the model'''
+    self._inputs["opt"] = self.opt
+    flags  = [self._params, model_params, self._inputs, self.key()]
+    self._model["fn"](*flags)
+
+  """def _single_org(self, model_params, backprop=True):
+    '''single pass through the model'''
+    self._inputs["opt"] = self.opt
+    flags  = [self._params, model_params, self._inputs, self.key()]
+    if backprop:
+      (loss, aux), grad = self._model["grad_fn"](*flags)
+    else:
+      loss, aux = self._model["fn"](*flags)
+      grad = jax.tree_map(np.zeros_like, self._params)
+    aux.update({"loss":loss,"grad":grad})
+    return aux"""
+
+  def _recycle(self, model_params, num_recycles=None, backprop=True):
+    '''multiple passes through the model (aka recycle)'''
+    a = self._args
+    mode = a["recycle_mode"]
+    if num_recycles is None:
+      num_recycles = self.opt["num_recycles"]
+
+    if mode in ["backprop","add_prev"]:
+      # recycles compiled into model, only need single-pass
+      aux = self._single(model_params, backprop)
+
+    else:
+      L = self._inputs["residue_index"].shape[0]
+
+      # intialize previous
+      """if "prev" not in self._inputs or a["clear_prev"]:
+        prev = {'prev_msa_first_row': np.zeros([L,256]),
+                'prev_pair': np.zeros([L,L,128])}
+
+        if a["use_initial_guess"] and "batch" in self._inputs:
+          prev["prev_pos"] = self._inputs["batch"]["all_atom_positions"]
+        else:
+          prev["prev_pos"] = np.zeros([L,37,3])
+
+        if a["use_dgram"]:
+          # TODO: add support for initial_guess + use_dgram
+          prev["prev_dgram"] = np.zeros([L,L,64])
+
+        if a["use_initial_atom_pos"]:
+          if "batch" in self._inputs:
+            self._inputs["initial_atom_pos"] = self._inputs["batch"]["all_atom_positions"]
+          else:
+            self._inputs["initial_atom_pos"] = np.zeros([L,37,3])"""
+
+      #self._inputs["prev"] = prev
+      # decide which layers to compute gradients for
+      cycles = (num_recycles + 1)
+      mask = [0] * cycles
+
+      if mode == "sample":  mask[np.random.randint(0,cycles)] = 1
+      if mode == "average": mask = [1/cycles] * cycles
+      if mode == "last":    mask[-1] = 1
+      if mode == "first":   mask[0] = 1
+
+      # gather gradients across recycles
+      grad = []
+      for m in mask:
+        if m == 0:
+          aux = self._single(model_params, backprop=False)
+        """else:
+          aux = self._single(model_params, backprop)
+          grad.append(jax.tree_map(lambda x:x*m, aux["grad"]))
+        self._inputs["prev"] = aux["prev"]
+        if a["use_initial_atom_pos"]:
+          self._inputs["initial_atom_pos"] = aux["prev"]["prev_pos"]
+
+      aux["grad"] = jax.tree_map(lambda *x: np.stack(x).sum(0), *grad)
+
+    aux["num_recycles"] = num_recycles"""
+    return aux
+
+  """def step(self, lr_scale=1.0, num_recycles=None,
+           num_models=None, sample_models=None, models=None, backprop=True,
+           callback=None, save_best=False, verbose=1):
+    '''do one step of gradient descent'''
+
+    # run
+    self.run(num_recycles=num_recycles, num_models=num_models, sample_models=sample_models,
+             models=models, backprop=backprop, callback=callback)
+
+    # modify gradients
+    if self.opt["norm_seq_grad"]: self._norm_seq_grad()
+    self._state, self.aux["grad"] = self._optimizer(self._state, self.aux["grad"], self._params)
+
+    # apply gradients
+    lr = self.opt["learning_rate"] * lr_scale
+    self._params = jax.tree_map(lambda x,g:x-lr*g, self._params, self.aux["grad"])
+
+    # save results
+    self._save_results(save_best=save_best, verbose=verbose)
+
+    # increment
+    self._k += 1
+
+  def _print_log(self, print_str=None, aux=None):
+    if aux is None: aux = self.aux
+    keys = ["models","recycles","hard","soft","temp","seqid","loss",
+            "seq_ent","mlm","helix","pae","i_pae","exp_res","con","i_con",
+            "sc_fape","sc_rmsd","dgram_cce","fape","plddt","ptm"]
+
+    if "i_ptm" in aux["log"]:
+      if len(self._lengths) > 1:
+        keys.append("i_ptm")
+      else:
+        aux["log"].pop("i_ptm")
+
+    print(dict_to_str(aux["log"], filt=self.opt["weights"],
+                      print_str=print_str, keys=keys+["rmsd"], ok=["plddt","rmsd"]))
+
+  def _save_results(self, aux=None, save_best=False,
+                    best_metric=None, metric_higher_better=False,
+                    verbose=True):
+    if aux is None: aux = self.aux
+    self._tmp["log"].append(aux["log"])
+    if (self._k % self._args["traj_iter"]) == 0:
+      # update traj
+      traj = {"seq":   aux["seq"]["pseudo"],
+              "xyz":   aux["atom_positions"][:,1,:],
+              "plddt": aux["plddt"],
+              "pae":   aux["pae"]}
+      for k,v in traj.items():
+        if len(self._tmp["traj"][k]) == self._args["traj_max"]:
+          self._tmp["traj"][k].pop(0)
+        self._tmp["traj"][k].append(v)
+
+    # save best
+    if save_best:
+      if best_metric is None:
+        best_metric = self._args["best_metric"]
+      metric = float(aux["log"][best_metric])
+      if self._args["best_metric"] in ["plddt","ptm","i_ptm","seqid","composite"] or metric_higher_better:
+        metric = -metric
+      if "metric" not in self._tmp["best"] or metric < self._tmp["best"]["metric"]:
+        self._tmp["best"]["aux"] = copy_dict(aux)
+        self._tmp["best"]["metric"] = metric
+
+    if verbose and ((self._k+1) % verbose) == 0:
+      self._print_log(f"{self._k+1}", aux=aux)"""
+
+  def predict(self, seq=None, bias=None,
+              num_models=None, num_recycles=None, models=None, sample_models=False,
+              dropout=False, hard=True, soft=False, temp=1,
+              return_aux=False, verbose=True,  seed=None, **kwargs):
+    '''predict structure for input sequence (if provided)'''
+
+    """def load_settings():
+      if "save" in self._tmp:
+        [self.opt, self._args, self._params, self._inputs] = self._tmp.pop("save")
+
+    def save_settings():
+      load_settings()
+      self._tmp["save"] = [copy_dict(x) for x in [self.opt, self._args, self._params, self._inputs]]
+
+    save_settings()"""
+
+    # set seed if defined
+    if seed is not None: self.set_seed(seed)
+
+    # set [seq]uence/[opt]ions
+    if seq is not None: self.set_seq(seq=seq, bias=bias)
+    self.set_opt(hard=hard, soft=soft, temp=temp, dropout=dropout, pssm_hard=True)
+    self.set_args(shuffle_first=False)
+
+    # run
+    self.run(num_recycles=num_recycles, num_models=num_models,
+             sample_models=sample_models, models=models, backprop=False, **kwargs)
+    if verbose: self._print_log("predict")
+
+    #load_settings()
+
+    # return (or save) results
+    #if return_aux: return self.aux
+######################################################################################
+############ alphafold- inputs
+######################################################################################
+class _af_inputs:
+
+  def _get_seq(self, inputs, aux, key=None):
+    params, opt = inputs["params"], inputs["opt"]
+    '''get sequence features'''
+    seq = soft_seq(params["seq"], inputs["bias"], opt, key, num_seq=self._num,
+                   shuffle_first=self._args["shuffle_first"])
+    seq = self._fix_pos(seq)
+    aux.update({"seq":seq, "seq_pseudo":seq["pseudo"]})
+
+    # protocol specific modifications to seq features
+    if self.protocol == "binder":
+      # concatenate target and binder sequence
+      seq_target = jax.nn.one_hot(inputs["batch"]["aatype"][:self._target_len],self._args["alphabet_size"])
+      seq_target = jnp.broadcast_to(seq_target,(self._num, *seq_target.shape))
+      seq = jax.tree_map(lambda x:jnp.concatenate([seq_target,x],1), seq)
 
-class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_utils):
+    if self.protocol in ["fixbb","hallucination","partial"] and self._args["copies"] > 1:
+      seq = jax.tree_map(lambda x:expand_copies(x, self._args["copies"], self._args["block_diag"]), seq)
+
+    return seq
+
+  def _fix_pos(self, seq, return_p=False):
+    if "fix_pos" in self.opt:
+      if "pos" in self.opt:
+        seq_ref = jax.nn.one_hot(self._wt_aatype_sub,self._args["alphabet_size"])
+        p = self.opt["pos"][self.opt["fix_pos"]]
+        fix_seq = lambda x: x.at[...,p,:].set(seq_ref)
+      else:
+        seq_ref = jax.nn.one_hot(self._wt_aatype,self._args["alphabet_size"])
+        p = self.opt["fix_pos"]
+        fix_seq = lambda x: x.at[...,p,:].set(seq_ref[...,p,:])
+      seq = jax.tree_map(fix_seq, seq)
+      if return_p: return seq, p
+    return seq
+
+  def _update_template(self, inputs, key):
+    ''''dynamically update template features'''
+    if "batch" in inputs:
+      batch, opt = inputs["batch"], inputs["opt"]
+
+      # enable templates
+      inputs["template_mask"] = jnp.asarray(inputs["template_mask"]).at[0].set(1)
+      #inputs["template_mask"] = inputs["template_mask"].at[0].set(1)
+      L = batch["aatype"].shape[0]
+
+      # decide which position to remove sequence and/or sidechains
+      rm     = jnp.broadcast_to(inputs.get("rm_template",False),L)
+      rm_seq = jnp.where(rm,True,jnp.broadcast_to(inputs.get("rm_template_seq",True),L))
+      rm_sc  = jnp.where(rm_seq,True,jnp.broadcast_to(inputs.get("rm_template_sc",True),L))
+
+      # define template features
+      template_feats = {"template_aatype":jnp.where(rm_seq,21,batch["aatype"])}
+
+      if "dgram" in batch:
+        # use dgram from batch if provided
+        template_feats.update({"template_dgram":batch["dgram"]})
+        nT,nL = inputs["template_aatype"].shape
+        inputs["template_dgram"] = jnp.zeros((nT,nL,nL,39))
+
+      if "all_atom_positions" in batch:
+        # get pseudo-carbon-beta coordinates (carbon-alpha for glycine)
+        # aatype = is used to define template's CB coordinates (CA in case of glycine)
+        cb, cb_mask = model.modules.pseudo_beta_fn(
+          jnp.where(rm_seq,0,batch["aatype"]),
+          batch["all_atom_positions"],
+          batch["all_atom_mask"])
+        template_feats.update({"template_pseudo_beta":        cb,
+                               "template_pseudo_beta_mask":   cb_mask,
+                               "template_all_atom_positions": batch["all_atom_positions"],
+                               "template_all_atom_mask":      batch["all_atom_mask"]})
+
+      # inject template features
+      if self.protocol == "partial":
+        pos = opt["pos"]
+        if self._args["repeat"] or self._args["homooligomer"]:
+          C,L = self._args["copies"], self._len
+          pos = (jnp.repeat(pos,C).reshape(-1,C) + jnp.arange(C) * L).T.flatten()
+
+      for k,v in template_feats.items():
+        if self.protocol == "partial":
+          if k in ["template_dgram"]:
+            inputs[k] = inputs[k].at[0,pos[:,None],pos[None,:]].set(v)
+          else:
+            inputs[k] = inputs[k].at[0,pos].set(v)
+        else:
+          inputs[k] = jnp.asarray(inputs[k]).at[0].set(v)
+          #inputs[k] = inputs[k].at[0].set(v)
+
+        # remove sidechains (mask anything beyond CB)
+        if k in ["template_all_atom_mask"]:
+          if self.protocol == "partial":
+            inputs[k] = inputs[k].at[:,pos,5:].set(jnp.where(rm_sc[:,None],0,inputs[k][:,pos,5:]))
+            inputs[k] = inputs[k].at[:,pos].set(jnp.where(rm[:,None],0,inputs[k][:,pos]))
+          else:
+            inputs[k] = inputs[k].at[...,5:].set(jnp.where(rm_sc[:,None],0,inputs[k][...,5:]))
+            inputs[k] = jnp.where(rm[:,None],0,inputs[k])
+
+def update_seq(seq, inputs, seq_1hot=None, seq_pssm=None, mlm=None):
+  '''update the sequence features'''
+
+  if seq_1hot is None: seq_1hot = seq
+  if seq_pssm is None: seq_pssm = seq
+  target_feat = seq_1hot[0,:,:20]
+
+  seq_1hot = jnp.pad(seq_1hot,[[0,0],[0,0],[0,22-seq_1hot.shape[-1]]])
+  seq_pssm = jnp.pad(seq_pssm,[[0,0],[0,0],[0,22-seq_pssm.shape[-1]]])
+  msa_feat = jnp.zeros_like(inputs["msa_feat"]).at[...,0:22].set(seq_1hot).at[...,25:47].set(seq_pssm)
+
+  # masked language modeling (randomly mask positions)
+  if mlm is not None:
+    X = jax.nn.one_hot(22,23)
+    X = jnp.zeros(msa_feat.shape[-1]).at[...,:23].set(X).at[...,25:48].set(X)
+    msa_feat = jnp.where(mlm[...,None],X,msa_feat)
+
+  inputs.update({"msa_feat":msa_feat, "target_feat":target_feat})
+
+def update_aatype(aatype, inputs):
+  r = residue_constants
+  a = {"atom14_atom_exists":r.restype_atom14_mask,
+       "atom37_atom_exists":r.restype_atom37_mask,
+       "residx_atom14_to_atom37":r.restype_atom14_to_atom37,
+       "residx_atom37_to_atom14":r.restype_atom37_to_atom14}
+  mask = inputs["seq_mask"][:,None]
+  inputs.update(jax.tree_map(lambda x:jnp.where(mask,jnp.asarray(x)[aatype],0),a))
+  inputs["aatype"] = aatype
+################################################################################
+######### alphafold-utils
+################################################################################
+class _af_utils:
+
+  def set_opt(self, *args, **kwargs):
+    '''
+    set [opt]ions
+    -------------------
+    note: model.restart() resets the [opt]ions to their defaults
+    use model.set_opt(..., set_defaults=True)
+    or model.restart(..., reset_opt=False) to avoid this
+    -------------------
+    model.set_opt(num_models=1, num_recycles=0)
+    model.set_opt(con=dict(num=1)) or set_opt({"con":{"num":1}}) or set_opt("con",num=1)
+    model.set_opt(lr=1, set_defaults=True)
+    '''
+    ks = list(kwargs.keys())
+    self.set_args(**{k:kwargs.pop(k) for k in ks if k in self._args})
+
+    if kwargs.pop("set_defaults", False):
+      update_dict(self._opt, *args, **kwargs)
+
+    update_dict(self.opt, *args, **kwargs)
+
+  def set_args(self, **kwargs):
+    '''
+    set [arg]uments
+    '''
+    for k in ["best_metric", "traj_iter", "shuffle_first"]:
+      if k in kwargs: self._args[k] = kwargs.pop(k)
+
+    if "recycle_mode" in kwargs:
+      ok_recycle_mode_swap = ["average","sample","first","last"]
+      if kwargs["recycle_mode"] in ok_recycle_mode_swap and self._args["recycle_mode"] in ok_recycle_mode_swap:
+        self._args["recycle_mode"] = kwargs.pop("recycle_mode")
+      else:
+        print(f"ERROR: use {self.__class__.__name__}(recycle_mode=...) to set the recycle_mode")
+
+    if "optimizer" in kwargs:
+      self.set_optimizer(kwargs.pop("optimizer"),
+        learning_rate=kwargs.pop("learning_rate", None))
+
+    ks = list(kwargs.keys())
+    if len(ks) > 0:
+      print(f"ERROR: the following args were not set: {ks}")
+
+  """def get_loss(self, x="loss"):
+    '''output the loss (for entire trajectory)'''
+    return np.array([loss[x] for loss in self._tmp["log"]])
+
+  def save_pdb(self, filename=None, get_best=True, renum_pdb=True, aux=None):
+    '''
+    save pdb coordinates (if filename provided, otherwise return as string)
+    - set get_best=False, to get the last sampled sequence
+    '''
+    if aux is None:
+      aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
+    aux = aux["all"]
+
+    p = {k:aux[k] for k in ["aatype","residue_index","atom_positions","atom_mask"]}
+    p["b_factors"] = 100 * p["atom_mask"] * aux["plddt"][...,None]
+
+    def to_pdb_str(x, n=None):
+      p_str = protein.to_pdb(protein.Protein(**x))
+      p_str = "\n".join(p_str.splitlines()[1:-2])
+      if renum_pdb: p_str = renum_pdb_str(p_str, self._lengths)
+      if n is not None:
+        p_str = f"MODEL{n:8}\n{p_str}\nENDMDL\n"
+      return p_str
+
+    p_str = ""
+    for n in range(p["atom_positions"].shape[0]):
+      p_str += to_pdb_str(jax.tree_map(lambda x:x[n],p), n+1)
+    p_str += "END\n"
+
+    if filename is None:
+      return p_str
+    else:
+      with open(filename, 'w') as f:
+        f.write(p_str)
+  def save_current_pdb(self, filename=None):
+    '''save pdb coordinates (if filename provided, otherwise return as string)'''
+    #print("af-utils: save_current_pdb")
+    self.save_pdb(filename=filename, get_best=False)"""
+
+####################################################
+##### alphafold-model
+####################################################
+#class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_utils):
+class mk_af_model(design_model, _af_inputs, _af_prep, _af_design, _af_utils):
   def __init__(self,
-               protocol="fixbb", 
+               protocol="fixbb",
                use_multimer=False,
                use_templates=False,
                debug=False,
-               data_dir=".", 
-               **kwargs):  
+               data_dir=".",
+               **kwargs):
     assert protocol in ["fixbb","hallucination","binder","partial"]
 
     self.protocol = protocol
@@ -35,21 +1123,21 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
     self._args = {"use_templates":use_templates, "use_multimer":use_multimer, "use_bfloat16":True,
                   "recycle_mode":"last", "use_mlm": False, "realign": True,
                   "debug":debug, "repeat":False, "homooligomer":False, "copies":1,
-                  "optimizer":"sgd", "best_metric":"loss", 
+                  "optimizer":"sgd", "best_metric":"loss",
                   "traj_iter":1, "traj_max":10000,
                   "clear_prev": True, "use_dgram":False,
                   "shuffle_first":True, "use_remat":True,
-                  "alphabet_size":20, 
+                  "alphabet_size":20,
                   "use_initial_guess":False, "use_initial_atom_pos":False}
 
     if self.protocol == "binder": self._args["use_templates"] = True
 
     self.opt = {"dropout":True, "pssm_hard":False, "learning_rate":0.1, "norm_seq_grad":True,
-                "num_recycles":0, "num_models":1, "sample_models":True,                
+                "num_recycles":0, "num_models":1, "sample_models":True,
                 "temp":1.0, "soft":0.0, "hard":0.0, "alpha":2.0,
                 "con":      {"num":2, "cutoff":14.0, "binary":False, "seqsep":9, "num_pos":float("inf")},
                 "i_con":    {"num":1, "cutoff":21.6875, "binary":False, "num_pos":float("inf")},
-                "template": {"rm_ic":False},                
+                "template": {"rm_ic":False},
                 "weights":  {"seq_ent":0.0, "plddt":0.0, "pae":0.0, "exp_res":0.0, "helix":0.0},
                 "fape_cutoff":10.0}
 
@@ -72,7 +1160,7 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
                                  "loss":kwargs.pop("loss_callback",None)},
                        "design":{"pre": kwargs.pop("pre_design_callback",None),
                                  "post":kwargs.pop("post_design_callback",None)}}
-    
+
     for m,n in self._callbacks.items():
       for k,v in n.items():
         if v is None: v = []
@@ -94,7 +1182,7 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
       self.opt["pssm_hard"] = True
     else:
       self._cfg = config.model_config("model_1_ptm" if self._args["use_templates"] else "model_3_ptm")
-    
+
     if self._args["recycle_mode"] in ["average","first","last","sample"]:
       num_recycles = 0
     else:
@@ -131,7 +1219,7 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
     #####################################
     idx = ["fixbb","hallucination","binder","partial"].index(self.protocol)
     self.prep_inputs = [self._prep_fixbb, self._prep_hallucination, self._prep_binder, self._prep_partial][idx]
-    self._get_loss   = [self._loss_fixbb, self._loss_hallucination, self._loss_binder, self._loss_partial][idx]
+    #self._get_loss   = [self._loss_fixbb, self._loss_hallucination, self._loss_binder, self._loss_partial][idx]
 
   def _get_model(self, cfg, callback=None):
 
@@ -153,8 +1241,8 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
       #######################################################################
       # get sequence
       seq = self._get_seq(inputs, aux, key())
-            
-      # update sequence features      
+
+      # update sequence features
       pssm = jnp.where(opt["pssm_hard"], seq["hard"], seq["pseudo"])
       if a["use_mlm"]:
         shape = seq["pseudo"].shape[:2]
@@ -162,9 +1250,9 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
         update_seq(seq["pseudo"], inputs, seq_pssm=pssm, mlm=mlm)
       else:
         update_seq(seq["pseudo"], inputs, seq_pssm=pssm)
-      
+
       # update amino acid sidechain identity
-      update_aatype(seq["pseudo"][0].argmax(-1), inputs) 
+      update_aatype(seq["pseudo"][0].argmax(-1), inputs)
 
       # define masks
       inputs["msa_mask"] = jnp.where(inputs["seq_mask"],inputs["msa_mask"],0)
@@ -175,7 +1263,7 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
       inputs["mask_template_interchain"] = opt["template"]["rm_ic"]
       if a["use_templates"]:
         self._update_template(inputs, key())
-      
+
       # set dropout
       inputs["use_dropout"] = opt["dropout"]
 
@@ -188,21 +1276,90 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
                    "seq":seq, "key":key(), "params":params}
         sub_args = {k:fn_args.get(k,None) for k in signature(fn).parameters}
         fn(**sub_args)
-      
+
       #######################################################################
       # OUTPUTS
       #######################################################################
-      outputs = runner.apply(model_params, key(), inputs)
+      #cleaned_inputs = {}
+      #for k, v in inputs.items():
+      #  if isinstance(v, np.ndarray):  # Keep only NumPy arrays
+      #    cleaned_inputs[k] = v
+      #  elif hasattr(v, "numpy"):  # Convert PyTorch tensors
+      #    cleaned_inputs[k] = v.numpy()
+      #  elif hasattr(v, "block_until_ready"):  # Convert JAX tensors
+      #    cleaned_inputs[k] = np.array(v)
+
+      cleaned_inputs = {k: (v.numpy() if isinstance(v, torch.Tensor) else np.array(v)) for k, v in inputs.items()}
+      #print(cleaned_inputs['aatype'])
+      keys_to_remove = {'prev', 'opt', 'params', 'seq', 'batch','asym_id','sym_id','entity_id', 'all_atom_positions'}
+      inputs_dict = dict(inputs)
+      cleaned_inputs = {k: v for k, v in inputs_dict.items() if k not in keys_to_remove}
+      cleaned_inputs['template_all_atom_masks'] = cleaned_inputs['template_all_atom_mask']
+      del cleaned_inputs['use_dropout']
+      del cleaned_inputs['mask_template_interchain']
+
+      features_to_skip = {
+    	"seq_length",
+    	"batch", "rm_template", "rm_template_seq", "rm_template_sc",
+    	"bias", "prev", "opt", "params", "seq", "mask_template_interchain",
+    	"use_dropout", "resample_msa_in_recycling"
+	  }
+
+      for k in cleaned_inputs:
+        if k not in features_to_skip:
+          cleaned_inputs[k] = np.expand_dims(cleaned_inputs[k], axis=0)
+
+      #for k, v in cleaned_inputs.items():
+      #  print(f"{k}: shape={v.shape}, dtype={v.dtype}")
+
+      cleaned_inputs['aatype'] = np.repeat(cleaned_inputs['aatype'], 4, axis=0)
+
+      cleaned_inputs['target_feat'] = torch.tensor(np.array(cleaned_inputs['target_feat']))  # Ensure conversion
+      b_start = torch.zeros([cleaned_inputs['target_feat'].shape[0], cleaned_inputs['target_feat'].shape[1], 1], dtype=cleaned_inputs['target_feat'].dtype)
+      b_end = torch.zeros([cleaned_inputs['target_feat'].shape[0], cleaned_inputs['target_feat'].shape[1], 1], dtype=cleaned_inputs['target_feat'].dtype)
+      cleaned_inputs['target_feat'] = torch.cat((b_start, cleaned_inputs['target_feat'], b_end), dim=2)
+
+      #cleaned_inputs.update({"seq_length": torch.tensor([1], dtype=torch.int32)})
+
+      #for k, v in cleaned_inputs.items():
+      #  print(f"{k}: shape={v.shape}, dtype={v.dtype}")
+
+      #output_dir = "/home/hgx/omics/shaikr2x/af_colab/ColabDesign/af_output/design/intermediate"
+      output_dir = "./af_output/design/intermediates"
+      os.makedirs(output_dir, exist_ok=True)
+      output_file = os.path.join(output_dir, "processed_features.npz")
+      np.savez(output_file, **cleaned_inputs)
+      print(f"Saved processed inputs to {output_file}")
+      if "outdir" not in self.opt:
+        self.opt["outdir"] = "./af_output/design"
+      intermediates_dir = os.path.join(self.opt["outdir"], "intermediates")
+      os.makedirs(intermediates_dir, exist_ok=True)
+      lengths_path = os.path.join(intermediates_dir, "lengths.npz")
+      np.savez(lengths_path, lengths=self._lengths)
+      print(f"[INFO] Saved lengths to {lengths_path}")
+      #output_dir_base = os.environ.get("output_dir")
+      #if not output_dir_base:
+      #  raise ValueError("Environment variable 'output_dir' is not set. Please export it before running the script.")
+
+      #output_dir = os.path.join(output_dir_base, fasta_name)
+      #os.makedirs(output_dir, exist_ok=True)
+      #tmp_output_dir = os.path.join(output_dir, 'intermediates')
+      #os.makedirs(tmp_output_dir, exist_ok=True)
+
+      #print(f"Processed features saved at: {npz_file_path}")
+
+      #print("Colab AF2 Inputs:", {k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in cleaned_inputs.items()})
+      """outputs = runner.apply(model_params, key(), inputs)
 
       # add aux outputs
       aux.update({"atom_positions": outputs["structure_module"]["final_atom_positions"],
-                  "atom_mask":      outputs["structure_module"]["final_atom_mask"],                  
+                  "atom_mask":      outputs["structure_module"]["final_atom_mask"],
                   "residue_index":  inputs["residue_index"],
                   "aatype":         inputs["aatype"],
                   "plddt":          get_plddt(outputs),
-                  "pae":            get_pae(outputs), 
+                  "pae":            get_pae(outputs),
                   "ptm":            get_ptm(inputs, outputs),
-                  "i_ptm":          get_ptm(inputs, outputs, interface=True), 
+                  "i_ptm":          get_ptm(inputs, outputs, interface=True),
                   "cmap":           get_contact_map(outputs, opt["con"]["cutoff"]),
                   "i_cmap":         get_contact_map(outputs, opt["i_con"]["cutoff"]),
                   "prev":           outputs["prev"]})
@@ -217,7 +1374,7 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
 
       # sequence entropy loss
       aux["losses"].update(get_seq_ent_loss(inputs))
-      
+
       # experimental masked-language-modeling
       if a["use_mlm"]:
         aux["mlm"] = outputs["masked_msa"]["logits"]
@@ -235,11 +1392,13 @@ class mk_af_model(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_
 
       # save for debugging
       if a["debug"]: aux["debug"] = {"inputs":inputs,"outputs":outputs}
-  
+
       # weighted loss
       w = opt["weights"]
       loss = sum([v * w[k] if k in w else v for k,v in aux["losses"].items()])
-      return loss, aux
-    
+      return loss, aux"""
+
     return {"grad_fn":jax.jit(jax.value_and_grad(_model, has_aux=True, argnums=0)),
-            "fn":jax.jit(_model), "runner":runner}
+			"fn":_model, "runner":runner}
+    #return {"grad_fn":jax.jit(jax.value_and_grad(_model, has_aux=True, argnums=0)),
+    #        "fn":jax.jit(_model), "runner":runner}"""
diff --git a/colabdesign/af/prep.py b/colabdesign/af/prep.py
deleted file mode 100644
index a8691d9..0000000
--- a/colabdesign/af/prep.py
+++ /dev/null
@@ -1,581 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-import re
-
-from colabdesign.af.alphafold.data import pipeline, prep_inputs
-from colabdesign.af.alphafold.common import protein, residue_constants
-from colabdesign.af.alphafold.model.tf import shape_placeholders
-from colabdesign.af.alphafold.model import config
-
-
-from colabdesign.shared.protein import _np_get_cb, pdb_to_string
-from colabdesign.shared.prep import prep_pos
-from colabdesign.shared.utils import copy_dict
-from colabdesign.shared.model import order_aa
-
-resname_to_idx = residue_constants.resname_to_idx
-idx_to_resname = dict((v,k) for k,v in resname_to_idx.items())
-
-#################################################
-# AF_PREP - input prep functions
-#################################################
-class _af_prep:
-
-  def _prep_model(self, **kwargs):
-    '''prep model'''
-    if not hasattr(self,"_model") or self._cfg != self._model["runner"].config:
-      self._cfg.model.global_config.subbatch_size = None
-      self._model = self._get_model(self._cfg)
-      if sum(self._lengths) > 384:
-        self._cfg.model.global_config.subbatch_size = 4
-        self._model["fn"] = self._get_model(self._cfg)["fn"]
-
-    self._opt = copy_dict(self.opt)  
-    self.restart(**kwargs)
-
-  def _prep_features(self, num_res, num_seq=None, num_templates=1):
-    '''process features'''
-    if num_seq is None: num_seq = self._num
-    return prep_input_features(L=num_res, N=num_seq, T=num_templates)
-
-  def _prep_fixbb(self, pdb_filename, chain=None,
-                  copies=1, repeat=False, homooligomer=False,
-                  rm_template=False,
-                  rm_template_seq=True,
-                  rm_template_sc=True,
-                  rm_template_ic=False,
-                  fix_pos=None, ignore_missing=True, **kwargs):
-    '''
-    prep inputs for fixed backbone design
-    ---------------------------------------------------
-    if copies > 1:
-      -homooligomer=True - input pdb chains are parsed as homo-oligomeric units
-      -repeat=True       - tie the repeating sequence within single chain
-    -rm_template_seq     - if template is defined, remove information about template sequence
-    -fix_pos="1,2-10"    - specify which positions to keep fixed in the sequence
-                           note: supervised loss is applied to all positions, use "partial" 
-                           protocol to apply supervised loss to only subset of positions
-    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
-    ---------------------------------------------------
-    '''    
-    # prep features
-    self._pdb = prep_pdb(pdb_filename, chain=chain, ignore_missing=ignore_missing,
-                         offsets=kwargs.pop("pdb_offsets",None),
-                         lengths=kwargs.pop("pdb_lengths",None))
-
-    self._len = self._pdb["residue_index"].shape[0]
-    self._lengths = [self._len]
-
-    # feat dims
-    num_seq = self._num
-    res_idx = self._pdb["residue_index"]
-    
-    # get [pos]itions of interests    
-    if fix_pos is not None and fix_pos != "":
-      self._pos_info = prep_pos(fix_pos, **self._pdb["idx"])
-      self.opt["fix_pos"] = self._pos_info["pos"]
-
-    if homooligomer and chain is not None and copies == 1:
-      copies = len(chain.split(","))
-      
-    # repeat/homo-oligomeric support
-    if copies > 1:
-
-      if repeat or homooligomer:
-        self._len = self._len // copies
-        if "fix_pos" in self.opt:
-          self.opt["fix_pos"] = self.opt["fix_pos"][self.opt["fix_pos"] < self._len]
-
-      if repeat:
-        self._lengths = [self._len * copies]
-        block_diag = False
-
-      else:
-        self._lengths = [self._len] * copies
-        block_diag = not self._args["use_multimer"]
-
-        res_idx = repeat_idx(res_idx[:self._len], copies)
-        num_seq = (self._num * copies + 1) if block_diag else self._num
-        self.opt["weights"].update({"i_pae":0.0, "i_con":0.0})
-
-      self._args.update({"copies":copies, "repeat":repeat, "homooligomer":homooligomer, "block_diag":block_diag})
-      homooligomer = not repeat
-    else:
-      self._lengths = self._pdb["lengths"]
-
-    # configure input features
-    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
-    self._inputs["residue_index"] = res_idx
-    self._inputs["batch"] = make_fixed_size(self._pdb["batch"], num_res=sum(self._lengths))
-    self._inputs.update(get_multi_id(self._lengths, homooligomer=homooligomer))
-
-    # configure options/weights
-    self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0, "con":0.0})
-    self._wt_aatype = self._inputs["batch"]["aatype"][:self._len]
-
-    # configure template [opt]ions
-    rm,L = {},sum(self._lengths)
-    for n,x in {"rm_template":    rm_template,
-                "rm_template_seq":rm_template_seq,
-                "rm_template_sc": rm_template_sc}.items():
-      rm[n] = np.full(L,False)
-      if isinstance(x,str):
-        rm[n][prep_pos(x,**self._pdb["idx"])["pos"]] = True
-      else:
-        rm[n][:] = x
-    self.opt["template"]["rm_ic"] = rm_template_ic
-    self._inputs.update(rm)
-  
-    self._prep_model(**kwargs)
-    
-  def _prep_hallucination(self, length=100, copies=1, repeat=False, **kwargs):
-    '''
-    prep inputs for hallucination
-    ---------------------------------------------------
-    if copies > 1:
-      -repeat=True - tie the repeating sequence within single chain
-    ---------------------------------------------------
-    '''
-    
-    # define num copies (for repeats/ homo-oligomers)
-    if not repeat and copies > 1 and not self._args["use_multimer"]:
-      (num_seq, block_diag) = (self._num * copies + 1, True)
-    else:
-      (num_seq, block_diag) = (self._num, False)
-    
-    self._args.update({"repeat":repeat,"block_diag":block_diag,"copies":copies})
-      
-    # prep features
-    self._len = length
-    
-    # set weights
-    self.opt["weights"].update({"con":1.0})
-    if copies > 1:
-      if repeat:
-        offset = 1
-        self._lengths = [self._len * copies]
-        self._args["repeat"] = True
-      else:
-        offset = 50
-        self._lengths = [self._len] * copies
-        self.opt["weights"].update({"i_pae":0.0, "i_con":1.0})
-        self._args["homooligomer"] = True
-      res_idx = repeat_idx(np.arange(length), copies, offset=offset)
-    else:
-      self._lengths = [self._len]
-      res_idx = np.arange(length)
-    
-    # configure input features
-    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
-    self._inputs["residue_index"] = res_idx
-    self._inputs.update(get_multi_id(self._lengths, homooligomer=True))
-
-    self._prep_model(**kwargs)
-
-  def _prep_binder(self, pdb_filename,
-                   target_chain="A", binder_len=50,                                         
-                   rm_target = False,
-                   rm_target_seq = False,
-                   rm_target_sc = False,
-                   
-                   # if binder_chain is defined
-                   binder_chain=None,
-                   rm_binder=True,
-                   rm_binder_seq=True,
-                   rm_binder_sc=True,
-                   rm_template_ic=False,
-                                      
-                   hotspot=None, ignore_missing=True, **kwargs):
-    '''
-    prep inputs for binder design
-    ---------------------------------------------------
-    -binder_len = length of binder to hallucinate (option ignored if binder_chain is defined)
-    -binder_chain = chain of binder to redesign
-    -use_binder_template = use binder coordinates as template input
-    -rm_template_ic = use target and binder coordinates as seperate template inputs
-    -hotspot = define position/hotspots on target
-    -rm_[binder/target]_seq = remove sequence info from template
-    -rm_[binder/target]_sc  = remove sidechain info from template
-    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
-    ---------------------------------------------------
-    '''
-    redesign = binder_chain is not None
-    rm_binder = not kwargs.pop("use_binder_template", not rm_binder)
-    
-    self._args.update({"redesign":redesign})
-
-    # get pdb info
-    target_chain = kwargs.pop("chain",target_chain) # backward comp
-    chains = f"{target_chain},{binder_chain}" if redesign else target_chain
-    im = [True] * len(target_chain.split(",")) 
-    if redesign: im += [ignore_missing] * len(binder_chain.split(","))
-
-    self._pdb = prep_pdb(pdb_filename, chain=chains, ignore_missing=im)
-    res_idx = self._pdb["residue_index"]
-
-    if redesign:
-      self._target_len = sum([(self._pdb["idx"]["chain"] == c).sum() for c in target_chain.split(",")])
-      self._binder_len = sum([(self._pdb["idx"]["chain"] == c).sum() for c in binder_chain.split(",")])
-    else:
-      self._target_len = self._pdb["residue_index"].shape[0]
-      self._binder_len = binder_len
-      res_idx = np.append(res_idx, res_idx[-1] + np.arange(binder_len) + 50)
-    
-    self._len = self._binder_len
-    self._lengths = [self._target_len, self._binder_len]
-
-    # gather hotspot info
-    if hotspot is not None:
-      self.opt["hotspot"] = prep_pos(hotspot, **self._pdb["idx"])["pos"]
-
-    if redesign:
-      # binder redesign
-      self._wt_aatype = self._pdb["batch"]["aatype"][self._target_len:]
-      self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0,
-                                  "con":0.0, "i_con":0.0, "i_pae":0.0})
-    else:
-      # binder hallucination
-      self._pdb["batch"] = make_fixed_size(self._pdb["batch"], num_res=sum(self._lengths))
-      self.opt["weights"].update({"plddt":0.1, "con":0.0, "i_con":1.0, "i_pae":0.0})
-
-    # configure input features
-    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=1)
-    self._inputs["residue_index"] = res_idx
-    self._inputs["batch"] = self._pdb["batch"]
-    self._inputs.update(get_multi_id(self._lengths))
-
-    # configure template rm masks
-    (T,L,rm) = (self._lengths[0],sum(self._lengths),{})
-    rm_opt = {
-              "rm_template":    {"target":rm_target,    "binder":rm_binder},
-              "rm_template_seq":{"target":rm_target_seq,"binder":rm_binder_seq},
-              "rm_template_sc": {"target":rm_target_sc, "binder":rm_binder_sc}
-             }
-    for n,x in rm_opt.items():
-      rm[n] = np.full(L,False)
-      for m,y in x.items():
-        if isinstance(y,str):
-          rm[n][prep_pos(y,**self._pdb["idx"])["pos"]] = True
-        else:
-          if m == "target": rm[n][:T] = y
-          if m == "binder": rm[n][T:] = y
-        
-    # set template [opt]ions
-    self.opt["template"]["rm_ic"] = rm_template_ic
-    self._inputs.update(rm)
-
-    self._prep_model(**kwargs)
-
-  def _prep_partial(self, pdb_filename, chain=None, length=None,
-                    copies=1, repeat=False, homooligomer=False,
-                    pos=None, fix_pos=None, use_sidechains=False, atoms_to_exclude=None,
-                    rm_template=False,
-                    rm_template_seq=False,
-                    rm_template_sc=False,
-                    rm_template_ic=False, 
-                    ignore_missing=True, **kwargs):
-    '''
-    prep input for partial hallucination
-    ---------------------------------------------------
-    -length=100 - total length of protein (if different from input PDB)
-    -pos="1,2-10" - specify which positions to apply supervised loss to
-    -use_sidechains=True - add a sidechain supervised loss to the specified positions
-      -atoms_to_exclude=["N","C","O"] (for sc_rmsd loss, specify which atoms to exclude)
-    -rm_template_seq - if template is defined, remove information about template sequence
-    -ignore_missing=True - skip positions that have missing density (no CA coordinate)
-    ---------------------------------------------------    
-    '''    
-    # prep features
-    self._pdb = prep_pdb(pdb_filename, chain=chain, ignore_missing=ignore_missing,
-                   offsets=kwargs.pop("pdb_offsets",None),
-                   lengths=kwargs.pop("pdb_lengths",None))
-
-    self._pdb["len"] = sum(self._pdb["lengths"])
-
-    self._len = self._pdb["len"] if length is None else length
-    self._lengths = [self._len]
-
-    # feat dims
-    num_seq = self._num
-    res_idx = np.arange(self._len)
-    
-    # get [pos]itions of interests
-    if pos is None:
-      self.opt["pos"] = self._pdb["pos"] = np.arange(self._pdb["len"])
-      self._pos_info = {"length":np.array([self._pdb["len"]]), "pos":self._pdb["pos"]}    
-    else:
-      self._pos_info = prep_pos(pos, **self._pdb["idx"])
-      self.opt["pos"] = self._pdb["pos"] = self._pos_info["pos"]
-
-    if homooligomer and chain is not None and copies == 1:
-      copies = len(chain.split(","))
-
-    # repeat/homo-oligomeric support
-    if copies > 1:
-      
-      if repeat or homooligomer:
-        self._len = self._len // copies
-        self._pdb["len"] = self._pdb["len"] // copies
-        self.opt["pos"] = self._pdb["pos"][self._pdb["pos"] < self._pdb["len"]]
-
-        # repeat positions across copies
-        self._pdb["pos"] = repeat_pos(self.opt["pos"], copies, self._pdb["len"])
-
-      if repeat:
-        self._lengths = [self._len * copies]
-        block_diag = False
-
-      else:
-        self._lengths = [self._len] * copies
-        block_diag = not self._args["use_multimer"]
-
-        num_seq = (self._num * copies + 1) if block_diag else self._num
-        res_idx = repeat_idx(np.arange(self._len), copies)
-
-        self.opt["weights"].update({"i_pae":0.0, "i_con":1.0})
-
-      self._args.update({"copies":copies, "repeat":repeat, "homooligomer":homooligomer, "block_diag":block_diag})
-      homooligomer = not repeat
-
-    # configure input features
-    self._inputs = self._prep_features(num_res=sum(self._lengths), num_seq=num_seq)
-    self._inputs["residue_index"] = res_idx
-    self._inputs["batch"] = jax.tree_map(lambda x:x[self._pdb["pos"]], self._pdb["batch"])     
-    self._inputs.update(get_multi_id(self._lengths, homooligomer=homooligomer))
-
-    # configure options/weights
-    self.opt["weights"].update({"dgram_cce":1.0, "rmsd":0.0, "fape":0.0, "con":1.0}) 
-    self._wt_aatype = self._pdb["batch"]["aatype"][self.opt["pos"]]
-
-    # configure sidechains
-    self._args["use_sidechains"] = use_sidechains
-    if use_sidechains:
-      self._sc = {"batch":prep_inputs.make_atom14_positions(self._inputs["batch"]),
-                  "pos":get_sc_pos(self._wt_aatype, atoms_to_exclude)}
-      self.opt["weights"].update({"sc_rmsd":0.1, "sc_fape":0.1})
-      self.opt["fix_pos"] = np.arange(self.opt["pos"].shape[0])      
-      self._wt_aatype_sub = self._wt_aatype
-      
-    elif fix_pos is not None and fix_pos != "":
-      sub_fix_pos = []
-      sub_i = []
-      pos = self.opt["pos"].tolist()
-      for i in prep_pos(fix_pos, **self._pdb["idx"])["pos"]:
-        if i in pos:
-          sub_i.append(i)
-          sub_fix_pos.append(pos.index(i))
-      self.opt["fix_pos"] = np.array(sub_fix_pos)
-      self._wt_aatype_sub = self._pdb["batch"]["aatype"][sub_i]
-      
-    elif kwargs.pop("fix_seq",False):
-      self.opt["fix_pos"] = np.arange(self.opt["pos"].shape[0])
-      self._wt_aatype_sub = self._wt_aatype
-
-    self.opt["template"].update({"rm_ic":rm_template_ic})
-    self._inputs.update({"rm_template":     rm_template,
-                         "rm_template_seq": rm_template_seq,
-                         "rm_template_sc":  rm_template_sc})
-  
-    self._prep_model(**kwargs)
-
-#######################
-# utils
-#######################
-def repeat_idx(idx, copies=1, offset=50):
-  idx_offset = np.repeat(np.cumsum([0]+[idx[-1]+offset]*(copies-1)),len(idx))
-  return np.tile(idx,copies) + idx_offset
-
-def repeat_pos(pos, copies, length):
-  return (np.repeat(pos,copies).reshape(-1,copies) + np.arange(copies) * length).T.flatten()
-
-def prep_pdb(pdb_filename, chain=None,
-             offsets=None, lengths=None,
-             ignore_missing=False):
-  '''extract features from pdb'''
-
-  def add_cb(batch):
-    '''add missing CB atoms based on N,CA,C'''
-    p,m = batch["all_atom_positions"], batch["all_atom_mask"]
-    atom_idx = residue_constants.atom_order
-    atoms = {k:p[...,atom_idx[k],:] for k in ["N","CA","C"]}
-    cb = atom_idx["CB"]
-    cb_atoms = _np_get_cb(**atoms, use_jax=False)
-    cb_mask = np.prod([m[...,atom_idx[k]] for k in ["N","CA","C"]],0)
-    batch["all_atom_positions"][...,cb,:] = np.where(m[:,cb,None], p[:,cb,:], cb_atoms)
-    batch["all_atom_mask"][...,cb] = (m[:,cb] + cb_mask) > 0
-    return {"atoms":batch["all_atom_positions"][:,cb],"mask":cb_mask}
-
-  if isinstance(chain,str) and "," in chain:
-    chains = chain.split(",")
-  elif not isinstance(chain,list):
-    chains = [chain]
-
-  o,last = [],0
-  residue_idx, chain_idx = [],[]
-  full_lengths = []
-
-  # go through each defined chain  
-  for n,chain in enumerate(chains):
-    pdb_str = pdb_to_string(pdb_filename, chains=chain, models=[1])
-    protein_obj = protein.from_pdb_string(pdb_str, chain_id=chain)
-    batch = {'aatype': protein_obj.aatype,
-             'all_atom_positions': protein_obj.atom_positions,
-             'all_atom_mask': protein_obj.atom_mask,
-             'residue_index': protein_obj.residue_index}
-
-    cb_feat = add_cb(batch) # add in missing cb (in the case of glycine)
-    
-    im = ignore_missing[n] if isinstance(ignore_missing,list) else ignore_missing
-    if im:
-      r = batch["all_atom_mask"][:,0] == 1
-      batch = jax.tree_map(lambda x:x[r], batch)
-      residue_index = batch["residue_index"] + last
-
-    else:
-      # pad values
-      offset = 0 if offsets is None else (offsets[n] if isinstance(offsets,list) else offsets)
-      r = offset + (protein_obj.residue_index - protein_obj.residue_index.min())
-      length = (r.max()+1) if lengths is None else (lengths[n] if isinstance(lengths,list) else lengths)    
-      def scatter(x, value=0):
-        shape = (length,) + x.shape[1:]
-        y = np.full(shape, value, dtype=x.dtype)
-        y[r] = x
-        return y
-
-      batch = {"aatype":scatter(batch["aatype"],-1),
-               "all_atom_positions":scatter(batch["all_atom_positions"]),
-               "all_atom_mask":scatter(batch["all_atom_mask"]),
-               "residue_index":scatter(batch["residue_index"],-1)}
-      
-      residue_index = np.arange(length) + last
-    
-    last = residue_index[-1] + 50
-    o.append({"batch":batch,
-              "residue_index": residue_index,
-              "cb_feat":cb_feat})
-    
-    residue_idx.append(batch.pop("residue_index"))
-    chain_idx.append([chain] * len(residue_idx[-1]))
-    full_lengths.append(len(residue_index))
-
-  # concatenate chains
-  o = jax.tree_util.tree_map(lambda *x:np.concatenate(x,0),*o)
-  
-  # save original residue and chain index
-  o["idx"] = {"residue":np.concatenate(residue_idx), "chain":np.concatenate(chain_idx)}
-  o["lengths"] = full_lengths
-  return o
-
-def make_fixed_size(feat, num_res, num_seq=1, num_templates=1):
-  '''pad input features'''
-  shape_schema = {k:v for k,v in config.CONFIG.data.eval.feat.items()}
-
-  pad_size_map = {
-      shape_placeholders.NUM_RES: num_res,
-      shape_placeholders.NUM_MSA_SEQ: num_seq,
-      shape_placeholders.NUM_EXTRA_SEQ: 1,
-      shape_placeholders.NUM_TEMPLATES: num_templates
-  }  
-  for k,v in feat.items():
-    if k == "batch":
-      feat[k] = make_fixed_size(v, num_res)
-    else:
-      shape = list(v.shape)
-      schema = shape_schema[k]
-      assert len(shape) == len(schema), (
-          f'Rank mismatch between shape and shape schema for {k}: '
-          f'{shape} vs {schema}')
-      pad_size = [pad_size_map.get(s2, None) or s1 for (s1, s2) in zip(shape, schema)]
-      padding = [(0, p - v.shape[i]) for i, p in enumerate(pad_size)]
-      feat[k] = np.pad(v, padding)
-  return feat
-
-def get_sc_pos(aa_ident, atoms_to_exclude=None):
-  '''get sidechain indices/weights for all_atom14_positions'''
-
-  # decide what atoms to exclude for each residue type
-  a2e = {}
-  for r in resname_to_idx:
-    if isinstance(atoms_to_exclude,dict):
-      a2e[r] = atoms_to_exclude.get(r,atoms_to_exclude.get("ALL",["N","C","O"]))
-    else:
-      a2e[r] = ["N","C","O"] if atoms_to_exclude is None else atoms_to_exclude
-
-  # collect atom indices
-  pos,pos_alt = [],[]
-  N,N_non_amb = [],[]
-  for n,a in enumerate(aa_ident):
-    aa = idx_to_resname[a]
-    atoms = set(residue_constants.residue_atoms[aa])
-    atoms14 = residue_constants.restype_name_to_atom14_names[aa]
-    swaps = residue_constants.residue_atom_renaming_swaps.get(aa,{})
-    swaps.update({v:k for k,v in swaps.items()})
-    for atom in atoms.difference(a2e[aa]):
-      pos.append(n * 14 + atoms14.index(atom))
-      if atom in swaps:
-        pos_alt.append(n * 14 + atoms14.index(swaps[atom]))
-      else:
-        pos_alt.append(pos[-1])
-        N_non_amb.append(n)
-      N.append(n)
-
-  pos, pos_alt = np.asarray(pos), np.asarray(pos_alt)
-  non_amb = pos == pos_alt
-  N, N_non_amb = np.asarray(N), np.asarray(N_non_amb)
-  w = np.array([1/(n == N).sum() for n in N])
-  w_na = np.array([1/(n == N_non_amb).sum() for n in N_non_amb])
-  w, w_na = w/w.sum(), w_na/w_na.sum()
-  return {"pos":pos, "pos_alt":pos_alt, "non_amb":non_amb,
-          "weight":w, "weight_non_amb":w_na[:,None]}
-
-def prep_input_features(L, N=1, T=1, eN=1):
-  '''
-  given [L]ength, [N]umber of sequences and number of [T]emplates
-  return dictionary of blank features
-  '''
-  inputs = {'aatype': np.zeros(L,int),
-            'target_feat': np.zeros((L,20)),
-            'msa_feat': np.zeros((N,L,49)),
-            # 23 = one_hot -> (20, UNK, GAP, MASK)
-            # 1  = has deletion
-            # 1  = deletion_value
-            # 23 = profile
-            # 1  = deletion_mean_value
-  
-            'seq_mask': np.ones(L),
-            'msa_mask': np.ones((N,L)),
-            'msa_row_mask': np.ones(N),
-            'atom14_atom_exists': np.zeros((L,14)),
-            'atom37_atom_exists': np.zeros((L,37)),
-            'residx_atom14_to_atom37': np.zeros((L,14),int),
-            'residx_atom37_to_atom14': np.zeros((L,37),int),            
-            'residue_index': np.arange(L),
-            'extra_deletion_value': np.zeros((eN,L)),
-            'extra_has_deletion': np.zeros((eN,L)),
-            'extra_msa': np.zeros((eN,L),int),
-            'extra_msa_mask': np.zeros((eN,L)),
-            'extra_msa_row_mask': np.zeros(eN),
-
-            # for template inputs
-            'template_aatype': np.zeros((T,L),int),
-            'template_all_atom_mask': np.zeros((T,L,37)),
-            'template_all_atom_positions': np.zeros((T,L,37,3)),
-            'template_mask': np.zeros(T),
-            'template_pseudo_beta': np.zeros((T,L,3)),
-            'template_pseudo_beta_mask': np.zeros((T,L)),
-
-            # for alphafold-multimer
-            'asym_id': np.zeros(L),
-            'sym_id': np.zeros(L),
-            'entity_id': np.zeros(L),
-            'all_atom_positions': np.zeros((N,37,3))}
-  return inputs
-
-def get_multi_id(lengths, homooligomer=False):
-  '''set info for alphafold-multimer'''
-  i = np.concatenate([[n]*l for n,l in enumerate(lengths)])
-  if homooligomer:
-    return {"asym_id":i, "sym_id":i, "entity_id":np.zeros_like(i)}
-  else:
-    return {"asym_id":i, "sym_id":i, "entity_id":i}
\ No newline at end of file
diff --git a/colabdesign/af/utils.py b/colabdesign/af/utils.py
deleted file mode 100644
index ca9dd72..0000000
--- a/colabdesign/af/utils.py
+++ /dev/null
@@ -1,189 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib.gridspec import GridSpec 
-
-from colabdesign.shared.protein import _np_kabsch
-from colabdesign.shared.utils import update_dict, Key
-from colabdesign.shared.plot import plot_pseudo_3D, make_animation, show_pdb
-from colabdesign.shared.protein import renum_pdb_str
-from colabdesign.af.alphafold.common import protein
-
-####################################################
-# AF_UTILS - various utils (save, plot, etc)
-####################################################
-class _af_utils:  
-
-  def set_opt(self, *args, **kwargs):
-    '''
-    set [opt]ions
-    -------------------
-    note: model.restart() resets the [opt]ions to their defaults
-    use model.set_opt(..., set_defaults=True) 
-    or model.restart(..., reset_opt=False) to avoid this
-    -------------------    
-    model.set_opt(num_models=1, num_recycles=0)
-    model.set_opt(con=dict(num=1)) or set_opt({"con":{"num":1}}) or set_opt("con",num=1)
-    model.set_opt(lr=1, set_defaults=True)
-    '''
-    ks = list(kwargs.keys())
-    self.set_args(**{k:kwargs.pop(k) for k in ks if k in self._args})
-        
-    if kwargs.pop("set_defaults", False):
-      update_dict(self._opt, *args, **kwargs)
-
-    update_dict(self.opt, *args, **kwargs)
-
-  def set_args(self, **kwargs):
-    '''
-    set [arg]uments
-    '''
-    for k in ["best_metric", "traj_iter", "shuffle_first"]:
-      if k in kwargs: self._args[k] = kwargs.pop(k)
-            
-    if "recycle_mode" in kwargs:
-      ok_recycle_mode_swap = ["average","sample","first","last"]
-      if kwargs["recycle_mode"] in ok_recycle_mode_swap and self._args["recycle_mode"] in ok_recycle_mode_swap:
-        self._args["recycle_mode"] = kwargs.pop("recycle_mode")
-      else:
-        print(f"ERROR: use {self.__class__.__name__}(recycle_mode=...) to set the recycle_mode")
-    
-    if "optimizer" in kwargs:
-      self.set_optimizer(kwargs.pop("optimizer"),
-        learning_rate=kwargs.pop("learning_rate", None))
-
-    ks = list(kwargs.keys())
-    if len(ks) > 0:
-      print(f"ERROR: the following args were not set: {ks}")
-
-  def get_loss(self, x="loss"):
-    '''output the loss (for entire trajectory)'''
-    return np.array([loss[x] for loss in self._tmp["log"]])
-
-  def save_pdb(self, filename=None, get_best=True, renum_pdb=True, aux=None):
-    '''
-    save pdb coordinates (if filename provided, otherwise return as string)
-    - set get_best=False, to get the last sampled sequence
-    '''
-    if aux is None:
-      aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
-    aux = aux["all"]
-    
-    p = {k:aux[k] for k in ["aatype","residue_index","atom_positions","atom_mask"]}
-    p["b_factors"] = 100 * p["atom_mask"] * aux["plddt"][...,None]
-
-    def to_pdb_str(x, n=None):
-      p_str = protein.to_pdb(protein.Protein(**x))
-      p_str = "\n".join(p_str.splitlines()[1:-2])
-      if renum_pdb: p_str = renum_pdb_str(p_str, self._lengths)
-      if n is not None:
-        p_str = f"MODEL{n:8}\n{p_str}\nENDMDL\n"
-      return p_str
-
-    p_str = ""
-    for n in range(p["atom_positions"].shape[0]):
-      p_str += to_pdb_str(jax.tree_map(lambda x:x[n],p), n+1)
-    p_str += "END\n"
-    
-    if filename is None:
-      return p_str
-    else: 
-      with open(filename, 'w') as f:
-        f.write(p_str)
-
-  #-------------------------------------
-  # plotting functions
-  #-------------------------------------
-  def animate(self, s=0, e=None, dpi=100, get_best=True, traj=None, aux=None, color_by="plddt"):
-    '''
-    animate the trajectory
-    - use [s]tart and [e]nd to define range to be animated
-    - use dpi to specify the resolution of animation
-    - color_by = ["plddt","chain","rainbow"]
-    '''
-    if aux is None:
-      aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
-    aux = aux["all"]    
-    if self.protocol in ["fixbb","binder"]:
-      pos_ref = self._inputs["batch"]["all_atom_positions"][:,1].copy()
-      pos_ref[(pos_ref == 0).any(-1)] = np.nan
-    else:
-      pos_ref = aux["atom_positions"][0,:,1,:]
-
-    if traj is None: traj = self._tmp["traj"]
-    sub_traj = {k:v[s:e] for k,v in traj.items()}
-
-    align_xyz = self.protocol == "hallucination"
-    return make_animation(**sub_traj, pos_ref=pos_ref, length=self._lengths,
-                          color_by=color_by, align_xyz=align_xyz, dpi=dpi) 
-
-  def plot_pdb(self, show_sidechains=False, show_mainchains=False,
-               color="pLDDT", color_HP=False, size=(800,480), animate=False,
-               get_best=True, aux=None, pdb_str=None):
-    '''
-    use py3Dmol to plot pdb coordinates
-    - color=["pLDDT","chain","rainbow"]
-    '''
-    if pdb_str is None:
-      pdb_str = self.save_pdb(get_best=get_best, aux=aux)
-    view = show_pdb(pdb_str,
-                    show_sidechains=show_sidechains,
-                    show_mainchains=show_mainchains,
-                    color=color,
-                    Ls=self._lengths,
-                    color_HP=color_HP,
-                    size=size,
-                    animate=animate)
-    view.show()
-  
-  def plot_traj(self, dpi=100):
-    fig = plt.figure(figsize=(5,5), dpi=dpi)
-    gs = GridSpec(4,1, figure=fig)
-    ax1 = fig.add_subplot(gs[:3,:])
-    ax2 = fig.add_subplot(gs[3:,:])
-    ax1_ = ax1.twinx()
-    
-    if self.protocol in ["fixbb","partial"] or (self.protocol == "binder" and self._args["redesign"]):
-      if self.protocol == "partial" and self._args["use_sidechains"]:
-        rmsd = self.get_loss("sc_rmsd")
-      else:
-        rmsd = self.get_loss("rmsd")
-      for k in [0.5,1,2,4,8,16,32]:
-        ax1.plot([0,len(rmsd)],[k,k],color="lightgrey")
-      ax1.plot(rmsd,color="black")
-      seqid = self.get_loss("seqid")
-      ax1_.plot(seqid,color="green",label="seqid")
-      # axes labels
-      ax1.set_yscale("log")
-      ticks = [0.25,0.5,1,2,4,8,16,32,64]
-      ax1.set(xticks=[])
-      ax1.set_yticks(ticks); ax1.set_yticklabels(ticks)
-      ax1.set_ylabel("RMSD",color="black");ax1_.set_ylabel("seqid",color="green")
-      ax1.set_ylim(0.25,64)
-      ax1_.set_ylim(0,0.8)
-      # extras
-      ax2.plot(self.get_loss("soft"),color="yellow",label="soft")
-      ax2.plot(self.get_loss("temp"),color="orange",label="temp")
-      ax2.plot(self.get_loss("hard"),color="red",label="hard")
-      ax2.set_ylim(-0.1,1.1)
-      ax2.set_xlabel("iterations")
-      ax2.legend(loc='center left')
-    else:
-      print("TODO")
-    plt.show()
-
-  def clear_best(self):
-    self._tmp["best"] = {}
-
-  def save_current_pdb(self, filename=None):
-    '''save pdb coordinates (if filename provided, otherwise return as string)'''
-    self.save_pdb(filename=filename, get_best=False)
-
-  def plot_current_pdb(self, show_sidechains=False, show_mainchains=False,
-    color="pLDDT", color_HP=False, size=(800,480), animate=False):
-    '''use py3Dmol to plot pdb coordinates
-    - color=["pLDDT","chain","rainbow"]
-    '''
-    self.plot_pdb(show_sidechains=show_sidechains, show_mainchains=show_mainchains, color=color,
-      color_HP=color_HP, size=size, animate=animate, get_best=False)
\ No newline at end of file
diff --git a/colabdesign/esm_msa/__init__.py b/colabdesign/esm_msa/__init__.py
deleted file mode 100644
index 940a2bf..0000000
--- a/colabdesign/esm_msa/__init__.py
+++ /dev/null
@@ -1,9 +0,0 @@
-# Copyright (c) Levinthal, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from .data import Alphabet, BatchConverter, FastaBatchedDataset  # noqa
-from .model import MSATransformer, RunModel  # noqa
-from . import pretrained  # noqa
-from . import config
diff --git a/colabdesign/esm_msa/axial_attention.py b/colabdesign/esm_msa/axial_attention.py
deleted file mode 100644
index 4f1a8f6..0000000
--- a/colabdesign/esm_msa/axial_attention.py
+++ /dev/null
@@ -1,200 +0,0 @@
-# Copyright (c) Levinthal, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import jax
-import math
-import jax.numpy as jnp
-import haiku as hk
-from jax.nn import softmax
-
-from colabdesign.shared.prng import SafeKey
-
-class RowSelfAttention(hk.Module):
-  """Compute self-attention over rows of a 2D input."""
-
-  def __init__(
-    self,
-    config,
-  ):
-    super().__init__()
-    self.head_num = config.RowAtt.head_num
-    self.embed_dim = config.RowAtt.embed_dim
-    self.dropout = config.dropout
-    self.max_tokens_per_msa = config.max_tokens_per_msa
-
-    self.head_dim = self.embed_dim // self.head_num
-    self.scaling = self.head_dim ** -0.5
-    self.attn_shape = "hij"
-
-    self.k_proj = hk.Linear(self.embed_dim, name="k_proj")
-    self.v_proj = hk.Linear(self.embed_dim, name="v_proj")
-    self.q_proj = hk.Linear(self.embed_dim, name="q_proj")
-    self.out_proj = hk.Linear(self.embed_dim, name="out_proj")
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-  def align_scaling(self, q):
-    num_rows = q.shape[0]
-    return self.scaling / math.sqrt(num_rows)
-
-  def _batched_forward(
-    self,
-    x,
-    self_attn_padding_mask,
-  ):
-    num_rows, num_cols, embed_dim = x.shape
-    max_rows = max(1, self.max_tokens_per_msa // num_cols)
-    attns = 0
-    scaling = self.align_scaling(x)
-    for start in range(0, num_rows, max_rows):
-      attn_weights = self.compute_attention_weights(
-        x[start: start + max_rows],
-        scaling,
-        self_attn_padding_mask=self_attn_padding_mask[:, start: start + max_rows]
-      )
-      attns += attn_weights
-    attn_probs = softmax(attns, -1)
-    self.safe_key, use_key = self.safe_key.split()
-    attn_probs = hk.dropout(use_key.get(), self.dropout, attn_probs)
-
-    outputs = []
-    for start in range(0, num_rows, max_rows):
-      output = self.compute_attention_update(x[start: start + max_rows], attn_probs)
-      outputs.append(output)
-
-    output = jnp.concatenate(outputs, 0)
-    return output, attn_probs
-
-  def compute_attention_weights(
-    self,
-    x,
-    scaling: float,
-    self_attn_padding_mask,
-  ):
-    num_rows, num_cols, embed_dim = x.shape
-    q = self.q_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-    k = self.k_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-    q *= scaling
-    # Zero out any padded aligned positions - this is important since
-    # we take a sum across the alignment axis.
-    q *= 1 - jnp.expand_dims(jnp.expand_dims(self_attn_padding_mask, 2), 3)
-
-    attn_weights = jnp.einsum(f"rihd,rjhd->{self.attn_shape}", q, k)
-    attn_weights *= 1 - jnp.expand_dims(jnp.expand_dims(self_attn_padding_mask[0], 0), 2)
-    attn_weights += jnp.expand_dims(jnp.expand_dims(self_attn_padding_mask[0], 0), 2) * -10000
-
-    return attn_weights
-
-  def compute_attention_update(
-    self,
-    x,
-    attn_probs,
-  ):
-    num_rows, num_cols, embed_dim = x.shape
-    v = self.v_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-    context = jnp.einsum(f"{self.attn_shape},rjhd->rihd", attn_probs, v)
-    context = context.reshape([num_rows, num_cols, embed_dim])
-    output = self.out_proj(context)
-    return output
-
-  def __call__(self, x,
-         self_attn_padding_mask,):
-    
-    num_rows, num_cols, embed_dim = x.shape
-    if num_rows * num_cols > self.max_tokens_per_msa:
-      return self._batched_forward(x, self_attn_padding_mask)
-    else:
-      scaling = self.align_scaling(x)
-      attn_weights = self.compute_attention_weights(
-        x, scaling, self_attn_padding_mask
-      )
-      attn_probs = softmax(attn_weights, -1)
-      self.safe_key, use_key = self.safe_key.split()
-      attn_probs = hk.dropout(use_key.get(), self.dropout, attn_probs)
-      output = self.compute_attention_update(x, attn_probs)
-      return output, attn_probs
-
-
-class ColumnSelfAttention(hk.Module):
-  """Compute self-attention over columns of a 2D input."""
-
-  def __init__(self, config):
-    super().__init__()
-    self.head_num = config.ColAtt.head_num
-    self.embed_dim = config.RowAtt.embed_dim
-    self.dropout = config.dropout
-    self.max_tokens_per_msa = config.max_tokens_per_msa
-
-    self.head_dim = self.embed_dim // self.head_num
-    self.scaling = self.head_dim ** -0.5
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-    self.k_proj = hk.Linear(self.embed_dim, name='k_proj')
-    self.v_proj = hk.Linear(self.embed_dim, name='v_proj')
-    self.q_proj = hk.Linear(self.embed_dim, name='q_proj')
-    self.out_proj = hk.Linear(self.embed_dim, name='out_proj')
-
-  def _batched_forward(
-    self,
-    x,
-    self_attn_padding_mask,
-  ):
-    num_rows, num_cols, embed_dim = x.shape
-    max_cols = max(1, self.max_tokens_per_msa // num_rows)
-    outputs = []
-    attns = []
-    for start in range(0, num_cols, max_cols):
-      output, attn = self.compute_attention_update(
-        x[:, start: start + max_cols],
-        self_attn_padding_mask=self_attn_padding_mask[:, :, start: start + max_cols]
-      )
-      outputs.append(output)
-      attns.append(attn)
-    output = jnp.concatenate(outputs, 1)
-    attns = jnp.concatenate(attns, 1)
-    return output, attns
-
-  def compute_attention_update(
-    self,
-    x,
-    self_attn_padding_mask,
-  ):
-    num_rows, num_cols, embed_dim = x.shape
-    if num_rows == 1:
-      attn_probs = jnp.ones(
-        [self.head_num, num_cols, num_rows, num_rows],
-        dtype=x.dtype,
-      )
-      output = self.out_proj(self.v_proj(x))
-      return output, attn_probs
-    else:
-      q = self.q_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-      k = self.k_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-      v = self.v_proj(x).reshape([num_rows, num_cols, self.head_num, self.head_dim])
-      q *= self.scaling
-
-      attn_weights = jnp.einsum("ichd,jchd->hcij", q, k)
-      attn_weights *= 1 - jnp.expand_dims(jnp.expand_dims(self_attn_padding_mask.transpose(), 0), 3)
-      attn_weights += jnp.expand_dims(jnp.expand_dims(self_attn_padding_mask.transpose(), 0), 3) * -10000
-      attn_probs = softmax(attn_weights, -1)
-
-      self.safe_key, use_key = self.safe_key.split()
-      attn_probs = hk.dropout(use_key.get(), self.dropout, attn_probs)
-      context = jnp.einsum("hcij,jchd->ichd", attn_probs, v)
-      context = context.reshape([num_rows, num_cols, embed_dim])
-      output = self.out_proj(context)
-      return output, attn_probs
-
-  def __call__(
-    self,
-    x,
-    self_attn_padding_mask,
-  ):
-    # if False and num_rows * num_cols > 2 ** 14 and not torch.is_grad_enabled():
-    num_rows, num_cols, embed_dim = x.shape
-    if num_rows * num_cols > self.max_tokens_per_msa:
-      return self._batched_forward(x, self_attn_padding_mask)
-    else:
-      return self.compute_attention_update(x, self_attn_padding_mask)
-
diff --git a/colabdesign/esm_msa/config.py b/colabdesign/esm_msa/config.py
deleted file mode 100644
index 99c19c8..0000000
--- a/colabdesign/esm_msa/config.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright 2021 Levinthal Limited
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""Model config."""
-
-import ml_collections
-
-CONFIG = ml_collections.ConfigDict({
-  'RowAtt': {
-    'embed_dim': 768,
-    'head_num': 12,
-
-  },
-  'ColAtt': {
-    'embed_dim': 768,
-    'head_num': 12,
-
-  },
-  'Ffn': {
-    'embed_dim': 3072,
-  },
-  'dropout': 0.0,
-  'max_tokens_per_msa': 2 ** 16,
-  'layer_num': 12,
-  'embed_dim': 768,
-  'max_position': 1024,
-})
diff --git a/colabdesign/esm_msa/constants.py b/colabdesign/esm_msa/constants.py
deleted file mode 100644
index c975e75..0000000
--- a/colabdesign/esm_msa/constants.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-# fmt: off
-proteinseq_toks = {
-  'toks': ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-']
-}
-# fmt: on
diff --git a/colabdesign/esm_msa/data.py b/colabdesign/esm_msa/data.py
deleted file mode 100644
index 263913c..0000000
--- a/colabdesign/esm_msa/data.py
+++ /dev/null
@@ -1,294 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from typing import Sequence, Tuple, Union
-import re
-import numpy as np
-import jax.numpy as jnp
-
-from .constants import proteinseq_toks
-
-RawMSA = Sequence[Tuple[str, str]]
-
-
-class FastaBatchedDataset(object):
-  def __init__(self, sequence_labels, sequence_strs):
-    self.sequence_labels = list(sequence_labels)
-    self.sequence_strs = list(sequence_strs)
-
-  @classmethod
-  def from_file(cls, fasta_file):
-    sequence_labels, sequence_strs = [], []
-    cur_seq_label = None
-    buf = []
-
-    def _flush_current_seq():
-      nonlocal cur_seq_label, buf
-      if cur_seq_label is None:
-        return
-      sequence_labels.append(cur_seq_label)
-      sequence_strs.append("".join(buf))
-      cur_seq_label = None
-      buf = []
-
-    with open(fasta_file, "r") as infile:
-      for line_idx, line in enumerate(infile):
-        if line.startswith(">"):  # label line
-          _flush_current_seq()
-          line = line[1:].strip()
-          if len(line) > 0:
-            cur_seq_label = line
-          else:
-            cur_seq_label = f"seqnum{line_idx:09d}"
-        else:  # sequence line
-          buf.append(line.strip())
-
-    _flush_current_seq()
-
-    assert len(set(sequence_labels)) == len(sequence_labels), "Found duplicate sequence labels"
-
-    return cls(sequence_labels, sequence_strs)
-
-  def __len__(self):
-    return len(self.sequence_labels)
-
-  def __getitem__(self, idx):
-    return self.sequence_labels[idx], self.sequence_strs[idx]
-
-  def get_batch_indices(self, toks_per_batch, extra_toks_per_seq=0):
-    sizes = [(len(s), i) for i, s in enumerate(self.sequence_strs)]
-    sizes.sort()
-    batches = []
-    buf = []
-    max_len = 0
-
-    def _flush_current_buf():
-      nonlocal max_len, buf
-      if len(buf) == 0:
-        return
-      batches.append(buf)
-      buf = []
-      max_len = 0
-
-    for sz, i in sizes:
-      sz += extra_toks_per_seq
-      if max(sz, max_len) * (len(buf) + 1) > toks_per_batch:
-        _flush_current_buf()
-      max_len = max(max_len, sz)
-      buf.append(i)
-
-    _flush_current_buf()
-    return batches
-
-
-class Alphabet(object):
-  def __init__(
-    self,
-    standard_toks: Sequence[str],
-    prepend_toks: Sequence[str] = ("<null_0>", "<pad>", "<eos>", "<unk>"),
-    append_toks: Sequence[str] = ("<cls>", "<mask>", "<sep>"),
-    prepend_bos: bool = True,
-    append_eos: bool = False,
-    use_msa: bool = False,
-  ):
-    self.standard_toks = list(standard_toks)
-    self.prepend_toks = list(prepend_toks)
-    self.append_toks = list(append_toks)
-    self.prepend_bos = prepend_bos
-    self.append_eos = append_eos
-    self.use_msa = use_msa
-
-    self.all_toks = list(self.prepend_toks)
-    self.all_toks.extend(self.standard_toks)
-    for i in range((8 - (len(self.all_toks) % 8)) % 8):
-      self.all_toks.append(f"<null_{i  + 1}>")
-    self.all_toks.extend(self.append_toks)
-
-    self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}
-
-    self.unk_idx = self.tok_to_idx["<unk>"]
-    self.padding_idx = self.get_idx("<pad>")
-    self.cls_idx = self.get_idx("<cls>")
-    self.mask_idx = self.get_idx("<mask>")
-    self.eos_idx = self.get_idx("<eos>")
-
-  def __len__(self):
-    return len(self.all_toks)
-
-  def get_idx(self, tok):
-    return self.tok_to_idx.get(tok, self.unk_idx)
-
-  def get_tok(self, ind):
-    return self.all_toks[ind]
-
-  def to_dict(self):
-    return {"toks": self.toks}
-
-  def get_batch_converter(self):
-    if self.use_msa:
-      return MSABatchConverter(self)
-    else:
-      return BatchConverter(self)
-
-  @classmethod
-  def from_dict(cls, d, **kwargs):
-    return cls(standard_toks=d["toks"], **kwargs)
-
-  @classmethod
-  def from_architecture(cls, name: str) -> "Alphabet":
-    if name in ("ESM-1", "protein_bert_base"):
-      standard_toks = proteinseq_toks["toks"]
-      prepend_toks: Tuple[str, ...] = ("<null_0>", "<pad>", "<eos>", "<unk>")
-      append_toks: Tuple[str, ...] = ("<cls>", "<mask>", "<sep>")
-      prepend_bos = True
-      append_eos = False
-      use_msa = False
-    elif name in ("ESM-1b", "roberta_large"):
-      standard_toks = proteinseq_toks["toks"]
-      prepend_toks = ("<cls>", "<pad>", "<eos>", "<unk>")
-      append_toks = ("<mask>",)
-      prepend_bos = True
-      append_eos = True
-      use_msa = False
-    elif name in ("MSA Transformer", "msa_transformer"):
-      standard_toks = proteinseq_toks["toks"]
-      prepend_toks = ("<cls>", "<pad>", "<eos>", "<unk>")
-      append_toks = ("<mask>",)
-      prepend_bos = True
-      append_eos = False
-      use_msa = True
-    else:
-      raise ValueError("Unknown architecture selected")
-    return cls(standard_toks, prepend_toks, append_toks, prepend_bos, append_eos, use_msa)
-
-
-class BatchConverter(object):
-  """Callable to convert an unprocessed (labels + strings) batch to a
-  processed (labels + tensor) batch.
-  """
-
-  def __init__(self, alphabet):
-    self.alphabet = alphabet
-
-  def __call__(self, raw_batch: Sequence[Tuple[str, str]], return_j=True):
-    # RoBERTa uses an eos token, while ESM-1 does not.
-    batch_size = len(raw_batch)
-    max_len = max(len(seq_str) for _, seq_str in raw_batch)
-    tokens_np = np.ones(
-      [
-        batch_size,
-        max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos)
-      ],
-      dtype=np.int64
-    ) * self.alphabet.padding_idx
-
-    labels = []
-    strs = []
-
-    for i, (label, seq_str) in enumerate(raw_batch):
-      labels.append(label)
-      strs.append(seq_str)
-      if self.alphabet.prepend_bos:
-        tokens_np[i, 0] = self.alphabet.cls_idx
-      seq = np.array([self.alphabet.get_idx(s) for s in seq_str], dtype=np.int64)
-      tokens_np[
-        i,
-        int(self.alphabet.prepend_bos): len(seq_str) + int(self.alphabet.prepend_bos),
-      ] = seq
-      if self.alphabet.append_eos:
-        tokens_np[i, len(seq_str) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx
-
-    if return_j:
-      tokens = jnp.array(tokens_np)
-    else:
-      tokens = tokens_np
-
-    return labels, strs, tokens
-
-
-class MSABatchConverter(BatchConverter):
-  def __call__(self, inputs: Union[Sequence[RawMSA], RawMSA], return_j=True):
-    if isinstance(inputs[0][0], str):
-      # Input is a single MSA
-      raw_batch: Sequence[RawMSA] = [inputs]  # type: ignore
-    else:
-      raw_batch = inputs  # type: ignore
-
-    batch_size = len(raw_batch)
-    max_alignments = max(len(msa) for msa in raw_batch)
-    max_seqlen = max(len(msa[0][1]) for msa in raw_batch)
-
-    tokens_np = np.ones(
-      [
-        batch_size,
-        max_alignments,
-        max_seqlen + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),
-      ],
-      dtype=np.int64,
-    ) * self.alphabet.padding_idx
-
-    labels = []
-    strs = []
-
-    for i, msa in enumerate(raw_batch):
-      msa_seqlens = set(len(seq) for _, seq in msa)
-      if not len(msa_seqlens) == 1:
-        raise RuntimeError(
-          "Received unaligned sequences for input to MSA, all sequence "
-          "lengths must be equal."
-        )
-      msa_labels, msa_strs, msa_tokens = super().__call__(msa, return_j=False)
-      labels.append(msa_labels)
-      strs.append(msa_strs)
-      tokens_np[i, :msa_tokens.shape[0], :msa_tokens.shape[1]] = msa_tokens
-
-    if return_j:
-      tokens = jnp.array(tokens_np)
-    else:
-      tokens = tokens_np
-
-    return labels, strs, tokens
-
-
-def read_fasta(
-  path,
-  keep_gaps=True,
-  keep_insertions=True,
-  to_upper=False,
-):
-  with open(path, "r") as f:
-    for result in read_alignment_lines(
-      f, keep_gaps=keep_gaps, keep_insertions=keep_insertions, to_upper=to_upper
-    ):
-      yield result
-
-
-def read_alignment_lines(
-  lines,
-  keep_gaps=True,
-  keep_insertions=True,
-  to_upper=False,
-):
-  seq = desc = None
-
-  def parse(s):
-    if not keep_gaps:
-      s = re.sub("-", "", s)
-    if not keep_insertions:
-      s = re.sub("[a-z]", "", s)
-    return s.upper() if to_upper else s
-
-  for line in lines:
-    # Line may be empty if seq % file_line_width == 0
-    if len(line) > 0 and line[0] == ">":
-      if seq is not None:
-        yield desc, parse(seq)
-      desc = line.strip()
-      seq = ""
-    else:
-      assert isinstance(seq, str)
-      seq += line.strip()
-  assert isinstance(seq, str) and isinstance(desc, str)
-  yield desc, parse(seq)
diff --git a/colabdesign/esm_msa/model.py b/colabdesign/esm_msa/model.py
deleted file mode 100644
index 90eff9d..0000000
--- a/colabdesign/esm_msa/model.py
+++ /dev/null
@@ -1,141 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import joblib
-import jax.numpy as jnp
-import numpy as np
-import haiku as hk
-import jax
-
-from .modules import (
-  AxialTransformerLayer,
-  EmbedPosition,
-  MSAPositionEmbedding,
-  ContactPredictionHead,
-  LmHead,
-)
-
-from colabdesign.shared.prng import SafeKey
-
-class MSATransformer(hk.Module):
-  def __init__(self, alphabet, config):
-    super().__init__()
-    self.alphabet_size = len(alphabet)
-    self.padding_idx = alphabet.padding_idx
-    self.mask_idx = alphabet.mask_idx
-    self.cls_idx = alphabet.cls_idx
-    self.eos_idx = alphabet.eos_idx
-    self.prepend_bos = alphabet.prepend_bos
-    self.append_eos = alphabet.append_eos
-    self.config = config
-    self.dropout = config.dropout
-
-    self.embed_tokens = hk.Embed(
-      vocab_size=self.alphabet_size,
-      embed_dim=self.config.embed_dim, 
-    )
-
-    self.msa_position_embedding = MSAPositionEmbedding(self.config.embed_dim)
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-    self.layers = [
-      AxialTransformerLayer(self.config)
-      for _ in range(self.config.layer_num)
-    ]
-
-    self.contact_head = ContactPredictionHead(
-      self.config.layer_num * self.config.RowAtt.head_num,
-      self.prepend_bos,
-      self.append_eos,
-      eos_idx=self.eos_idx,
-    )
-    self.embed_positions = EmbedPosition(
-      self.config,
-      self.padding_idx,
-    )
-
-    self.emb_layer_norm_before = hk.LayerNorm(-1, create_scale=True, create_offset=True)
-    self.emb_layer_norm_after = hk.LayerNorm(-1, create_scale=True, create_offset=True)
-
-    self.lm_head = LmHead(
-      config=self.config,
-      output_dim=self.alphabet_size,
-      weight=self.embed_tokens.embeddings.transpose(),
-    )
-
-  def __call__(self, tokens):
-    num_alignments, seqlen = tokens.shape
-    padding_mask = jnp.equal(tokens, self.padding_idx)  # R, C
-    x = self.embed_tokens(tokens)
-    x += self.embed_positions(tokens)
-    x += self.msa_position_embedding(tokens)
-    x = self.emb_layer_norm_before(x)
-
-    self.safe_key, use_key = self.safe_key.split()
-    x = hk.dropout(use_key.get(), self.dropout, x)
-    x = x * (1 - jnp.expand_dims(padding_mask, axis=-1))
-
-    row_attn_weights = []
-    col_attn_weights = []
-
-    for layer_idx, layer in enumerate(self.layers):
-      x = layer(
-        x,
-        self_attn_padding_mask=padding_mask,
-      )
-      x, col_attn, row_attn = x
-      col_attn_weights.append(col_attn)
-      row_attn_weights.append(row_attn)
-
-    x = self.emb_layer_norm_after(x)
-    x = self.lm_head(x)
-
-    result = {"logits": x}
-    # col_attentions: L x H x C x R x R
-    col_attentions = jnp.stack(col_attn_weights, 0)
-    # row_attentions: L x H x C x C
-    row_attentions = jnp.stack(row_attn_weights, 0)
-    result["col_attentions"] = col_attentions
-    result["row_attentions"] = row_attentions
-    contacts = self.contact_head(tokens, row_attentions)
-    result["contacts"] = contacts
-
-    return result
-
-
-class RunModel:
-  '''container for msa transformer'''
-
-  def __init__(self, alphabet, config):
-    self.padding_idx = alphabet.padding_idx
-
-    def _forward(tokens):
-      model = MSATransformer(alphabet, config)
-      return model(tokens)
-
-    _forward_t = hk.transform(_forward)
-    self.init = jax.jit(_forward_t.init)
-    self.apply = jax.jit(_forward_t.apply)
-    self.key = jax.random.PRNGKey(42)
-
-  def load_params(self, path):
-    self.params = joblib.load(path)
-
-  def __call__(self, tokens):
-    assert tokens.ndim == 2
-    num_alignments, seqlen = tokens.shape
-
-    if num_alignments > 1024:
-      raise RuntimeError(
-        "Using model with MSA position embedding trained on maximum MSA "
-        f"depth of 1024, but received {num_alignments} alignments."
-      )
-
-    self.key, use_key = jax.random.split(self.key)
-    result = self.apply(self.params, use_key, tokens)
-    result_new = {}
-    for ikey in result.keys():
-      result_new[ikey] = np.array(result[ikey])
-    return result_new
diff --git a/colabdesign/esm_msa/modules.py b/colabdesign/esm_msa/modules.py
deleted file mode 100644
index ca6ba98..0000000
--- a/colabdesign/esm_msa/modules.py
+++ /dev/null
@@ -1,229 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from typing import Optional
-
-import haiku as hk
-import jax
-import jax.numpy as jnp
-
-from .axial_attention import ColumnSelfAttention, RowSelfAttention
-from colabdesign.shared.prng import SafeKey
-
-
-
-def symmetrize(x):
-  "Make layer symmetric in final two dimensions, used for contact prediction."
-  return x + x.transpose([0, 2, 1])
-
-
-def apc(x):
-  "Perform average product correct, used for contact prediction."
-  a1 = x.sum(-1, keepdims=True)
-  a2 = x.sum(-2, keepdims=True)
-  a12 = x.sum((-1, -2), keepdims=True)
-
-  avg = a1 * a2
-  avg = avg / a12
-  normalized = x - avg
-  return normalized
-
-
-class AxialTransformerLayer(hk.Module):
-  """Implements an Axial MSA Transformer block."""
-
-  def __init__(
-    self,
-    config,
-  ) -> None:
-    super().__init__()
-    self.config = config
-
-    row_self_attention = RowSelfAttention(config)
-    column_self_attention = ColumnSelfAttention(config)
-    feed_forward_layer = FeedForwardNetwork(config)
-
-    self.row_self_attention = self.build_residual(row_self_attention, name='row_self_attention')
-    self.column_self_attention = self.build_residual(column_self_attention, name='column_self_attention')
-    self.feed_forward_layer = self.build_residual(feed_forward_layer, name='feed_forward_layer')
-
-  def build_residual(self, layer: hk.Module, name=None):
-    return NormalizedResidualBlock(
-      layer,
-      self.config,
-      name=name,
-    )
-
-  def __call__(
-    self,
-    x,
-    self_attn_padding_mask,
-  ):
-    """
-    LayerNorm is applied either before or after the self-attention/ffn
-    modules similar to the original Transformer implementation.
-    """
-    x, row_attn = self.row_self_attention(
-      x,
-      self_attn_padding_mask=self_attn_padding_mask,
-    )
-    x, column_attn = self.column_self_attention(
-      x,
-      self_attn_padding_mask=self_attn_padding_mask,
-    )
-    x = self.feed_forward_layer(x)
-    return x, column_attn, row_attn
-
-
-class LmHead(hk.Module):
-  def __init__(self, config, output_dim, weight):
-    super().__init__()
-    self.layer_norm = hk.LayerNorm(-1, create_scale=True, create_offset=True)
-    self.dense = hk.Linear(config.embed_dim, name='dense')
-    self.weight = weight
-    self.bias = hk.get_parameter(name='bias', shape=[output_dim], init=jnp.zeros)
-
-  def __call__(self, input):
-    x = self.dense(input)
-    x = jax.nn.gelu(x)
-    x = self.layer_norm(x)
-    x = jnp.dot(x, self.weight) + self.bias
-    return x
-
-
-class ContactPredictionHead(hk.Module):
-  """Performs symmetrization, apc, and computes a logistic regression on the output features"""
-
-  def __init__(
-    self,
-    in_features: int,
-    prepend_bos: bool,
-    append_eos: bool,
-    bias=True,
-    eos_idx: Optional[int] = None,
-  ):
-    super().__init__()
-    self.in_features = in_features
-    self.prepend_bos = prepend_bos
-    self.append_eos = append_eos
-    self.eos_idx = eos_idx
-    self.regression = hk.Linear(1, with_bias=bias)
-    self.activation = jax.nn.sigmoid
-
-  def __call__(self, tokens, attentions):
-    # remove eos token attentions
-    if self.append_eos:
-      eos_mask = jnp.not_equal(tokens, self.eos_idx)
-      eos_mask = jnp.expand_dims(eos_mask, axis=0) * jnp.expand_dims(eos_mask, axis=1)
-      attentions = attentions * eos_mask[None, None, :, :]
-      attentions = attentions[..., :-1, :-1]
-
-    # remove cls token attentions
-    if self.prepend_bos:
-      attentions = attentions[..., 1:, 1:]
-
-    layers, heads, seqlen, _ = attentions.shape
-    attentions = attentions.reshape([layers * heads, seqlen, seqlen])
-
-    # features: C x T x T
-    attentions = apc(symmetrize(attentions))
-    attentions = attentions.transpose([1, 2, 0])
-    return self.activation(self.regression(attentions).squeeze(2))
-
-
-class NormalizedResidualBlock(hk.Module):
-  def __init__(
-    self,
-    layer: hk.Module,
-    config,
-    name=None,
-  ):
-    super().__init__(name=name)
-    self.embed_dim = config.embed_dim
-    self.dropout = config.dropout
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-    self.layer = layer
-    self.layer_norm = hk.LayerNorm(-1, create_scale=True, create_offset=True)
-
-  def __call__(self, x, *args, **kwargs):
-    residual = x
-    x = self.layer_norm(x)
-    outputs = self.layer(x, *args, **kwargs)
-    if isinstance(outputs, tuple):
-      x, *out = outputs
-    else:
-      x = outputs
-      out = None
-
-    self.safe_key, use_key = self.safe_key.split()
-    x = hk.dropout(use_key.get(), self.dropout, x)
-    x = residual + x
-
-    if out is not None:
-      return (x,) + tuple(out)
-    else:
-      return x
-
-
-class FeedForwardNetwork(hk.Module):
-  def __init__(
-    self,
-    config,
-  ):
-    super().__init__()
-    self.embed_dim = config.embed_dim
-    self.ffn_embed_dim = config.Ffn.embed_dim
-    self.max_tokens_per_msa = config.max_tokens_per_msa
-    self.dropout = config.dropout
-
-    self.safe_key = SafeKey(hk.next_rng_key())
-    self.activation_fn = jax.nn.gelu
-
-    self.fc1 = hk.Linear(self.ffn_embed_dim, name='fc1')
-    self.fc2 = hk.Linear(self.embed_dim, name='fc2')
-
-  def __call__(self, x):
-    x = self.activation_fn(self.fc1(x))
-    self.safe_key, use_key = self.safe_key.split()
-    x = hk.dropout(use_key.get(), self.dropout, x)
-    x = self.fc2(x)
-    return x
-
-
-class MSAPositionEmbedding(hk.Module):
-  def __init__(self, embed_dim):
-    super().__init__()
-    self.embed_dim = embed_dim
-    self.weight = hk.get_parameter(name='data',
-                     shape=[1024, 1, embed_dim],
-                     init=jnp.zeros)
-
-  def __call__(self, x):
-    # num_alignments, seq_len = x.shape
-    num_rows, num_cols = x.shape
-    return self.weight[:num_rows]
-
-
-class EmbedPosition(hk.Module):
-  def __init__(self, config, padding_idx):
-    super().__init__()
-    self.max_position = config.max_position
-    self.embed_dim = config.embed_dim
-    self.padding_idx = padding_idx
-    self.max_position_ = self.max_position + self.padding_idx + 1
-    self.embed = hk.Embed(vocab_size=self.max_position_,
-                embed_dim=self.embed_dim)
-
-  def __call__(self, tokens):
-    mask = jnp.not_equal(tokens, self.padding_idx)
-    # tokens always begin with <cls>, do not consider. <cls> is before <pad> in alphabet.
-    positions = jnp.cumsum(mask, axis=-1, dtype='int32') * mask + self.padding_idx
-
-    # position_oh = jax.nn.one_hot(positions, self.max_position_)
-    # weight = hk.get_parameter('weight', shape=[self.max_position_, self.embed_dim], init=jnp.zeros)
-    # x = jnp.dot(position_one_hot, weight)
-    # return x
-    return self.embed(positions)
diff --git a/colabdesign/esm_msa/pretrained.py b/colabdesign/esm_msa/pretrained.py
deleted file mode 100644
index 89dd477..0000000
--- a/colabdesign/esm_msa/pretrained.py
+++ /dev/null
@@ -1,10 +0,0 @@
-from colabdesign import esm_msa
-import joblib
-
-
-def get_model():
-  alphabet = esm_msa.Alphabet.from_architecture('msa_transformer')
-  config = esm_msa.config.CONFIG
-  model = esm_msa.RunModel(alphabet, config)
-
-  return model, alphabet
diff --git a/colabdesign/mpnn/__init__.py b/colabdesign/mpnn/__init__.py
deleted file mode 100644
index 0b3b65a..0000000
--- a/colabdesign/mpnn/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-import os,jax
-# disable triton_gemm for jax versions > 0.3
-if int(jax.__version__.split(".")[1]) > 3:
-  os.environ["XLA_FLAGS"] = "--xla_gpu_enable_triton_gemm=false"
-
-import warnings
-warnings.simplefilter(action='ignore', category=FutureWarning)
-
-from colabdesign.shared.utils import clear_mem
-from .model import mk_mpnn_model
\ No newline at end of file
diff --git a/colabdesign/mpnn/legacy/example.ipynb b/colabdesign/mpnn/legacy/example.ipynb
deleted file mode 100644
index b7795c7..0000000
--- a/colabdesign/mpnn/legacy/example.ipynb
+++ /dev/null
@@ -1,311 +0,0 @@
-{
-  "cells": [
-    {
-      "cell_type": "markdown",
-      "metadata": {
-        "colab_type": "text",
-        "id": "view-in-github"
-      },
-      "source": [
-        "<a href=\"https://colab.research.google.com/github/sokrypton/ColabDesign/blob/v1.1.1/colabdesign/mpnn/legacy/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "id": "7nofzcgaP96j",
-      "metadata": {
-        "id": "7nofzcgaP96j"
-      },
-      "outputs": [],
-      "source": [
-        "#@title install\n",
-        "%%bash\n",
-        "pip -q install git+https://github.com/sokrypton/ColabDesign.git@v1.1.1\n",
-        "# for debugging\n",
-        "ln -s /usr/local/lib/python3.7/dist-packages/colabdesign colabdesign"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 1,
-      "id": "dcea2f7e",
-      "metadata": {
-        "id": "dcea2f7e"
-      },
-      "outputs": [
-        {
-          "name": "stdout",
-          "output_type": "stream",
-          "text": [
-            "py3Dmol not installed\n"
-          ]
-        }
-      ],
-      "source": [
-        "import numpy as np\n",
-        "import os, sys\n",
-        "import joblib\n",
-        "import jax\n",
-        "import jax.numpy as jnp\n",
-        "import re\n",
-        "import copy\n",
-        "import random\n",
-        "import haiku as hk\n",
-        "from tqdm import tqdm\n",
-        "from matplotlib import pyplot as plt\n",
-        "\n",
-        "from colabdesign.mpnn.legacy.wrapper import MPNN_wrapper"
-      ]
-    },
-    {
-      "cell_type": "markdown",
-      "id": "931140c7",
-      "metadata": {
-        "id": "931140c7"
-      },
-      "source": [
-        "# Initialize model"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 2,
-      "id": "23ef08a0",
-      "metadata": {
-        "colab": {
-          "base_uri": "https://localhost:8080/"
-        },
-        "id": "23ef08a0",
-        "outputId": "d5e1a8bb-bac9-4b0a-d99a-b94eb10eeef0"
-      },
-      "outputs": [
-        {
-          "name": "stdout",
-          "output_type": "stream",
-          "text": [
-            "Number of edges: 48\n",
-            "Training noise level: 0.02A\n"
-          ]
-        }
-      ],
-      "source": [
-        "params_path = '/content/colabdesign/mpnn/jax_weights'\n",
-        "model = MPNN_wrapper(params_path=params_path)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 3,
-      "id": "b3896466",
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "# set the provided pdb\n",
-        "pdb_path='1P3J.pdb'\n",
-        "input = model.prep_input(pdb_path=pdb_path,\n",
-        "                         target_chain='A')"
-      ]
-    },
-    {
-      "cell_type": "markdown",
-      "id": "1350bfbd",
-      "metadata": {
-        "id": "1350bfbd"
-      },
-      "source": [
-        "## Get the outputs from MPNN"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 4,
-      "id": "228701fa",
-      "metadata": {
-        "id": "228701fa"
-      },
-      "outputs": [],
-      "source": [
-        "L = len(input['dataset_valid'][0]['seq'])\n",
-        "seed = random.randint(0,2147483647)\n",
-        "order = jax.random.normal(jax.random.PRNGKey(seed), (L,))\n",
-        "logits, log_probs = model.score(input, order=order)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 5,
-      "id": "024d0ad9",
-      "metadata": {
-        "colab": {
-          "base_uri": "https://localhost:8080/"
-        },
-        "id": "024d0ad9",
-        "outputId": "4420ce1c-6f62-495c-8bec-30410838d5b7"
-      },
-      "outputs": [
-        {
-          "data": {
-            "text/plain": [
-              "((1, 212, 21), (1, 212, 21))"
-            ]
-          },
-          "execution_count": 5,
-          "metadata": {},
-          "output_type": "execute_result"
-        }
-      ],
-      "source": [
-        "logits.shape, log_probs.shape"
-      ]
-    },
-    {
-      "cell_type": "markdown",
-      "id": "3acf7c73",
-      "metadata": {},
-      "source": [
-        "## Generate sequences"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 6,
-      "id": "T_fMvaY2bANv",
-      "metadata": {
-        "id": "T_fMvaY2bANv"
-      },
-      "outputs": [
-        {
-          "name": "stderr",
-          "output_type": "stream",
-          "text": [
-            "100%|██████████| 1/1 [00:23<00:00, 23.37s/it]\n"
-          ]
-        }
-      ],
-      "source": [
-        "# generate sequences\n",
-        "seqs = model.sampling(input, 1, 1)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 7,
-      "id": "39616c44",
-      "metadata": {},
-      "outputs": [
-        {
-          "data": {
-            "text/plain": [
-              "['MNIVLLGLPGSGKSTIGELICKDLGVPLISIDDIYVKAIKEKTPYGKEAEKYILKGKLVPNELTNGIIEKELSKEECKNGFVLDGYPRTVEEAEALEKILEKRGRPIDLVIYLECEEEVLRERLLTRLVCSKCFRSYNLVYRPPKTPGVCDECGAKLVVPKWDKPEVVEVRLKEYKERVEPLLEYFKEKGKLVKVDANKNEEEVYEDVKKLL']"
-            ]
-          },
-          "execution_count": 7,
-          "metadata": {},
-          "output_type": "execute_result"
-        }
-      ],
-      "source": [
-        "seqs"
-      ]
-    },
-    {
-      "cell_type": "markdown",
-      "id": "791d40a0",
-      "metadata": {},
-      "source": [
-        "## Genrate sequences for homomer"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 8,
-      "id": "668ec046",
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "pdb_path='1O91.pdb'\n",
-        "input = model.prep_input(pdb_path=pdb_path,\n",
-        "                         target_chain='A B C',\n",
-        "                         ishomomer=True)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 9,
-      "id": "34518ad9",
-      "metadata": {},
-      "outputs": [
-        {
-          "name": "stderr",
-          "output_type": "stream",
-          "text": [
-            "100%|██████████| 1/1 [00:17<00:00, 17.29s/it]\n"
-          ]
-        }
-      ],
-      "source": [
-        "seqs = model.sampling(input, 1, 1)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": 10,
-      "id": "58db1d7a",
-      "metadata": {},
-      "outputs": [
-        {
-          "data": {
-            "text/plain": [
-              "['EKEAFTALLTTPYPPVGEPIKFDKLLYNGQNVYDPATGIFTCKTPGVYFFSWNLNVYGKDLHVQLYKNDEAIQSSYMEYIEGKLSLTSGSAVLKLEKGDKVYLECPTEEANGLYAGEDVHSSFSGFLLYET/EKEAFTALLTTPYPPVGEPIKFDKLLYNGQNVYDPATGIFTCKTPGVYFFSWNLNVYGKDLHVQLYKNDEAIQSSYMEYIEGKLSLTSGSAVLKLEKGDKVYLECPTEEANGLYAGEDVHSSFSGFLLYET/EKEAFTALLTTPYPPVGEPIKFDKLLYNGQNVYDPATGIFTCKTPGVYFFSWNLNVYGKDLHVQLYKNDEAIQSSYMEYIEGKLSLTSGSAVLKLEKGDKVYLECPTEEANGLYAGEDVHSSFSGFLLYET']"
-            ]
-          },
-          "execution_count": 10,
-          "metadata": {},
-          "output_type": "execute_result"
-        }
-      ],
-      "source": [
-        "seqs"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "id": "3f12f535",
-      "metadata": {},
-      "outputs": [],
-      "source": []
-    }
-  ],
-  "metadata": {
-    "colab": {
-      "include_colab_link": true,
-      "provenance": []
-    },
-    "kernelspec": {
-      "display_name": "Python 3.8.13 ('jax038')",
-      "language": "python",
-      "name": "python3"
-    },
-    "language_info": {
-      "codemirror_mode": {
-        "name": "ipython",
-        "version": 3
-      },
-      "file_extension": ".py",
-      "mimetype": "text/x-python",
-      "name": "python",
-      "nbconvert_exporter": "python",
-      "pygments_lexer": "ipython3",
-      "version": "3.8.13"
-    },
-    "vscode": {
-      "interpreter": {
-        "hash": "d39b7156cdbdfdeaeb5cea9c1b6bf180b493eeb3b22ef2423895aed018ecbde9"
-      }
-    }
-  },
-  "nbformat": 4,
-  "nbformat_minor": 5
-}
\ No newline at end of file
diff --git a/colabdesign/mpnn/legacy/modules.py b/colabdesign/mpnn/legacy/modules.py
deleted file mode 100644
index f96b526..0000000
--- a/colabdesign/mpnn/legacy/modules.py
+++ /dev/null
@@ -1,408 +0,0 @@
-import functools
-import haiku as hk
-import jax
-import jax.numpy as jnp
-import numpy as np
-import itertools
-import joblib
-
-from colabdesign.shared.prng import SafeKey
-from .utils import gather_edges, gather_nodes, cat_neighbors_nodes, scatter, get_ar_mask
-from .sample import mpnn_sample
-
-Gelu = functools.partial(jax.nn.gelu, approximate=False)
-
-class dropout_cust(hk.Module):
-  def __init__(self, rate) -> None:
-    super().__init__()
-    self.rate = rate
-    self.safe_key = SafeKey(hk.next_rng_key())
-  
-  def __call__(self, x):
-    self.safe_key, use_key = self.safe_key.split()
-    return hk.dropout(use_key.get(), self.rate, x)
-
-
-class EncLayer(hk.Module):
-  def __init__(self, num_hidden,
-         num_in, dropout=0.1,
-         num_heads=None, scale=30,
-         name=None):
-    super(EncLayer, self).__init__()
-    self.num_hidden = num_hidden
-    self.num_in = num_in
-    self.scale = scale
-
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-    self.dropout1 = dropout_cust(dropout)
-    self.dropout2 = dropout_cust(dropout)
-    self.dropout3 = dropout_cust(dropout)
-    self.norm1 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm1')
-    self.norm2 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm2')
-    self.norm3 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm3')
-
-    self.W1 = hk.Linear(num_hidden, with_bias=True, name=name + '_W1')
-    self.W2 = hk.Linear(num_hidden, with_bias=True, name=name + '_W2')
-    self.W3 = hk.Linear(num_hidden, with_bias=True, name=name + '_W3')
-    self.W11 = hk.Linear(num_hidden, with_bias=True, name=name + '_W11')
-    self.W12 = hk.Linear(num_hidden, with_bias=True, name=name + '_W12')
-    self.W13 = hk.Linear(num_hidden, with_bias=True, name=name + '_W13')
-    self.act = Gelu
-    self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4,
-                       name=name + '_dense')
-
-  def __call__(self, h_V, h_E, E_idx,
-        mask_V=None, mask_attend=None):
-    """ Parallel computation of full transformer layer """
-
-    h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, 1, h_EV.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_EV], -1)
-
-    h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))
-    if mask_attend is not None:
-      h_message = jnp.expand_dims(mask_attend, -1)* h_message
-    dh = jnp.sum(h_message, -2) / self.scale
-    h_V = self.norm1(h_V + self.dropout1(dh))
-
-    dh = self.dense(h_V)
-    h_V = self.norm2(h_V + self.dropout2(dh))
-    if mask_V is not None:
-      mask_V = jnp.expand_dims(mask_V, -1)
-      h_V = mask_V * h_V
-
-    h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, 1, h_EV.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_EV], -1)
-    h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))
-    h_E = self.norm3(h_E + self.dropout3(h_message))
-    return h_V, h_E
-
-class DecLayer(hk.Module):
-  def __init__(self, num_hidden, num_in,
-         dropout=0.1, num_heads=None,
-         scale=30, name=None):
-    super(DecLayer, self).__init__()
-    self.num_hidden = num_hidden
-    self.num_in = num_in
-    self.scale = scale
-    self.dropout1 = dropout_cust(dropout)
-    self.dropout2 = dropout_cust(dropout)
-    self.norm1 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm1')
-    self.norm2 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm2')
-
-    self.W1 = hk.Linear(num_hidden, with_bias=True, name=name + '_W1')
-    self.W2 = hk.Linear(num_hidden, with_bias=True, name=name + '_W2')
-    self.W3 = hk.Linear(num_hidden, with_bias=True, name=name + '_W3')
-    self.act = Gelu
-    self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4,
-                       name=name + '_dense')
-
-  def __call__(self, h_V, h_E,
-         mask_V=None, mask_attend=None):
-    """ Parallel computation of full transformer layer """
-
-    # Concatenate h_V_i to h_E_ij
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, 1, h_E.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_E], -1)
-
-    h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))
-    if mask_attend is not None:
-      h_message = jnp.expand_dims(mask_attend, -1) * h_message
-    dh = jnp.sum(h_message, -2) / self.scale
-
-    h_V = self.norm1(h_V + self.dropout1(dh))
-
-    # Position-wise feedforward
-    dh = self.dense(h_V)
-    h_V = self.norm2(h_V + self.dropout2(dh))
-
-    if mask_V is not None:
-      mask_V = jnp.expand_dims(mask_V, -1)
-      h_V = mask_V * h_V
-    return h_V 
-
-class PositionWiseFeedForward(hk.Module):
-  def __init__(self, num_hidden, num_ff, name=None):
-    super(PositionWiseFeedForward, self).__init__()
-    self.W_in = hk.Linear(num_ff, with_bias=True, name=name + '_W_in')
-    self.W_out = hk.Linear(num_hidden, with_bias=True, name=name + '_W_out')
-    self.act = Gelu
-  def __call__(self, h_V):
-    h = self.act(self.W_in(h_V), approximate=False)
-    h = self.W_out(h)
-    return h
-
-class PositionalEncodings(hk.Module):
-  def __init__(self, num_embeddings, max_relative_feature=32):
-    super(PositionalEncodings, self).__init__()
-    self.num_embeddings = num_embeddings
-    self.max_relative_feature = max_relative_feature
-    self.linear = hk.Linear(num_embeddings, name='embedding_linear')
-
-  def __call__(self, offset, mask):
-    d = jnp.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature) * mask + \
-      (1 - mask) * (2*self.max_relative_feature + 1)
-    d_onehot = jax.nn.one_hot(d, 2*self.max_relative_feature + 1 + 1)
-    E = self.linear(d_onehot)
-    return E
-
-class RunModel:
-  def __init__(self, config) -> None:
-    self.config = config
-
-    def _forward_score(inputs):
-      model = ProteinMPNN(**self.config)
-      return model(**inputs)
-    self.score = jax.jit(hk.transform(_forward_score).apply)
-    self.init_score = jax.jit(hk.transform(_forward_score).init)
-
-    def _forward_sample(inputs):
-      model = ProteinMPNN(**self.config)
-      return model.sample(**inputs)
-    self.sample = jax.jit(hk.transform(_forward_sample).apply)
-    self.init_sample = jax.jit(hk.transform(_forward_sample).init)
-
-    def _forward_tsample(inputs):
-      model = ProteinMPNN(**self.config)
-      return model.tied_sample(**inputs)
-    self.tied_sample = jax.jit(hk.transform(_forward_tsample).apply)
-    self.init_tsample = jax.jit(hk.transform(_forward_tsample).init)
-
-  def load_params(self, path):
-    self.params = joblib.load(path)
-
-class ProteinFeatures(hk.Module):
-  def __init__(self, edge_features, node_features,
-         num_positional_embeddings=16,
-         num_rbf=16, top_k=30,
-         augment_eps=0., num_chain_embeddings=16):
-
-    """ Extract protein features """
-    super(ProteinFeatures, self).__init__()
-    self.edge_features = edge_features
-    self.node_features = node_features
-    self.top_k = top_k
-    self.augment_eps = augment_eps 
-    self.num_rbf = num_rbf
-    self.num_positional_embeddings = num_positional_embeddings
-
-    self.embeddings = PositionalEncodings(num_positional_embeddings)
-    node_in, edge_in = 6, num_positional_embeddings + num_rbf*25
-    self.edge_embedding = hk.Linear(edge_features, with_bias=False, name='edge_embedding')
-    self.norm_edges = hk.LayerNorm(-1, create_scale=True, create_offset=True, name='norm_edges')
-
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-  def _get_edge_idx(self, X, mask, eps=1E-6):
-    ''' get edge index
-    input: mask.shape = (...,L), X.shape = (...,L,3)
-    return: (...,L,k)
-    '''
-    mask_2D = mask[...,None,:] * mask[...,:,None] 
-    dX = X[...,None,:,:] - X[...,:,None,:]
-    D = jnp.sqrt(jnp.square(dX).sum(-1) + eps)
-    D_masked = jnp.where(mask_2D,D,D.max(-1,keepdims=True))
-    k = min(self.top_k, X.shape[-2])
-    _, E_idx = jax.lax.approx_min_k(D_masked, k, reduction_dimension=-1)
-    return E_idx
-
-  def _rbf(self, D):
-    ''' radial basis function (RBF)
-    input: (...,L,k)
-    output: (...,L,k,?)
-    '''
-    D_min, D_max, D_count = 2., 22., self.num_rbf
-    D_mu = jnp.linspace(D_min, D_max, D_count)
-    D_sigma = (D_max - D_min) / D_count    
-    return jnp.exp(-((D[...,None] - D_mu) / D_sigma)**2)
-
-  def _get_rbf(self, A, B, E_idx):
-    D = jnp.sqrt(jnp.square(A[...,:,None,:] - B[...,None,:,:]).sum(-1) + 1e-6)
-    D_neighbors = gather_edges(D[...,None], E_idx)[...,0] #[...,L,K]
-    return self._rbf(D_neighbors)
-
-  def __call__(self, X, mask, residue_idx, chain_idx, offset=None):
-    if self.augment_eps > 0:
-      self.safe_key, use_key = self.safe_key.split()
-      X = X + self.augment_eps * jax.random.normal(use_key, X.shape)
-    
-    ##########################
-    # get atoms
-    ##########################
-    # N,Ca,C,O,Cb
-    Y = X.transpose((2,0,1,3))
-    if Y.shape[0] == 4:
-      # add Cb
-      b,c = (Y[1]-Y[0]),(Y[2]-Y[1])
-      Cb = -0.58273431*jnp.cross(b,c) + 0.56802827*b - 0.54067466*c + Y[1]
-      Y = jnp.concatenate([Y,Cb[None]],0)
-
-    ##########################
-    # gather edge features
-    ##########################
-    # get edge indices (based on ca-ca distances)
-    E_idx = self._get_edge_idx(Y[1], mask)
-
-    # rbf encode distances between atoms
-    edges = jnp.array([[1,1],[0,0],[2,2],[3,3],[4,4],
-                       [1,0],[1,2],[1,3],[1,4],[0,2],
-                       [0,3],[0,4],[4,2],[4,3],[3,2],
-                       [0,1],[2,1],[3,1],[4,1],[2,0],
-                       [3,0],[4,0],[2,4],[3,4],[2,3]])
-    RBF_all = jax.vmap(lambda x:self._get_rbf(Y[x[0]],Y[x[1]],E_idx))(edges)
-    RBF_all = RBF_all.transpose((1,2,3,0,4))
-    RBF_all = RBF_all.reshape(RBF_all.shape[:-2]+(-1,))
-
-    ##########################
-    # position embedding
-    ##########################
-    # residue index offset
-    if offset is None:
-      offset = (residue_idx[...,:,None] - residue_idx[...,None,:])
-    offset = gather_edges(offset[...,None], E_idx)[...,0] #[B, L, K]
-
-    # chain index offset
-    d_chains = (chain_idx[...,:,None] == chain_idx[...,None,:]).astype(int)
-    E_chains = gather_edges(d_chains[...,None], E_idx)[...,0]
-    E_positional = self.embeddings(offset, E_chains)
-
-    ##########################
-    # define edges
-    ##########################
-    E = jnp.concatenate((E_positional, RBF_all), -1)
-    E = self.edge_embedding(E)
-    E = self.norm_edges(E)
-    return E, E_idx 
-
-class EmbedToken(hk.Module):
-  def __init__(self, vocab_size, embed_dim):
-    super().__init__()
-    self.vocab_size = vocab_size
-    self.embed_dim = embed_dim
-    self.w_init = hk.initializers.TruncatedNormal()
-
-  @property
-  def embeddings(self):
-    return hk.get_parameter("W_s",
-                [self.vocab_size, self.embed_dim],
-                init=self.w_init)
-
-  def __call__(self, arr):
-    if jnp.issubdtype(arr.dtype, jnp.integer):
-      one_hot = jax.nn.one_hot(arr, self.vocab_size)
-    else:
-      one_hot = arr
-    return jnp.tensordot(one_hot, self.embeddings, 1)
-
-class ProteinMPNN(hk.Module, mpnn_sample):
-  def __init__(self, num_letters,
-         node_features, edge_features, hidden_dim,
-         num_encoder_layers=3, num_decoder_layers=3,
-         vocab=21, k_neighbors=64,
-         augment_eps=0.05, dropout=0.1):
-    super(ProteinMPNN, self).__init__()
-
-    # Hyperparameters
-    self.node_features = node_features
-    self.edge_features = edge_features
-    self.hidden_dim = hidden_dim
-
-    # Featurization layers
-    self.features = ProteinFeatures(edge_features,
-                    node_features,
-                    top_k=k_neighbors,
-                    augment_eps=augment_eps)
-
-    self.W_e = hk.Linear(hidden_dim, with_bias=True, name='W_e')
-    self.W_s = EmbedToken(vocab_size=vocab, embed_dim=hidden_dim)
-
-    # Encoder layers
-    self.encoder_layers = [
-      EncLayer(hidden_dim, hidden_dim*2, dropout=dropout, name='enc' + str(i))
-      for i in range(num_encoder_layers)
-    ]
-
-    # Decoder layers
-    self.decoder_layers = [
-      DecLayer(hidden_dim, hidden_dim*3, dropout=dropout, name='dec' + str(i))
-      for i in range(num_decoder_layers)
-    ]
-    self.W_out = hk.Linear(num_letters, with_bias=True, name='W_out')
-
-  def __call__(self, X, mask, residue_idx, chain_idx,
-               S=None, chain_M=None, randn=None,
-               ar_mask=None, decoding_order=None, offset=None):
-    """ Graph-conditioned sequence model """
-    # Prepare node and edge embeddings
-    E, E_idx = self.features(X, mask, residue_idx, chain_idx, offset=offset)
-    h_V = jnp.zeros((E.shape[0], E.shape[1], E.shape[-1]))
-    h_E = self.W_e(E)
-
-    # Encoder is unmasked self-attention
-    mask_attend = gather_nodes(mask[...,None],E_idx)[...,0]
-    mask_attend = mask[...,None] * mask_attend
-    for layer in self.encoder_layers:
-      h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)
-
-    # Build encoder embeddings
-    h_EX_encoder = cat_neighbors_nodes(jnp.zeros_like(h_V), h_E, E_idx)
-    h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)
-
-    if S is None:  
-      ##########################################
-      # unconditional_probs
-      ##########################################
-
-      # make an autogressive mask
-      ar_mask = jnp.zeros([X.shape[0], X.shape[1], X.shape[1]])
-
-      mask_attend = jnp.take_along_axis(ar_mask, E_idx, 2)[...,None]
-      mask_1D = mask.reshape([mask.shape[0], mask.shape[1], 1, 1])
-      mask_bw = mask_1D * mask_attend
-      mask_fw = mask_1D * (1. - mask_attend)
-
-      h_EXV_encoder_fw = mask_fw * h_EXV_encoder
-      for layer in self.decoder_layers:
-        h_V = layer(h_V, h_EXV_encoder_fw, mask)
-
-    else:
-      ##########################################
-      # conditional_probs
-      ##########################################
-
-      # Concatenate sequence embeddings for autoregressive decoder
-      h_S = self.W_s(S)
-      h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)
-
-      if ar_mask is None:
-        if decoding_order is None:
-          # update chain_M to include missing regions
-          chain_M = chain_M * mask
-          #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]
-          decoding_order = jnp.argsort((chain_M+0.0001)*(jnp.abs(randn)))
-        
-        # make an autogressive mask
-        ar_mask = get_ar_mask(decoding_order)
-              
-      mask_attend = jnp.take_along_axis(ar_mask, E_idx, 2)[...,None]
-      mask_1D = mask.reshape([mask.shape[0], mask.shape[1], 1, 1])
-      mask_bw = mask_1D * mask_attend
-      mask_fw = mask_1D * (1. - mask_attend)
-
-      h_EXV_encoder_fw = mask_fw * h_EXV_encoder
-      for layer in self.decoder_layers:
-        # Masked positions attend to encoder information, unmasked see. 
-        h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)
-        h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw
-        h_V = layer(h_V, h_ESV, mask)
-
-    logits = self.W_out(h_V)
-    log_probs = jax.nn.log_softmax(logits, axis=-1)
-    return logits, log_probs
\ No newline at end of file
diff --git a/colabdesign/mpnn/legacy/sample.py b/colabdesign/mpnn/legacy/sample.py
deleted file mode 100644
index 6b9c133..0000000
--- a/colabdesign/mpnn/legacy/sample.py
+++ /dev/null
@@ -1,214 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-import itertools
-
-from .utils import gather_nodes, cat_neighbors_nodes, scatter, get_ar_mask
-
-class mpnn_sample:
-  def sample(self, key, X, randn, S_true,
-         chain_mask, chain_idx, residue_idx,
-         mask=None, temperature=1.0, omit_AAs_np=None,
-         bias_AAs_np=None, chain_M_pos=None, omit_AA_mask=None,
-         pssm_coef=None, pssm_bias=None, pssm_multi=None,
-         pssm_log_odds_flag=None, pssm_log_odds_mask=None,
-         pssm_bias_flag=None, bias_by_res=None):
-
-    # Prepare node and edge embeddings
-    E, E_idx = self.features(X, mask, residue_idx, chain_idx)
-    h_V = jnp.zeros((E.shape[0], E.shape[1], E.shape[-1]))
-    h_E = self.W_e(E)
-
-    # Encoder is unmasked self-attention
-    mask_attend = gather_nodes(mask[...,None], E_idx)[...,0]
-    mask_attend = mask[...,None] * mask_attend
-    for layer in self.encoder_layers:
-      h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)
-
-    # Decoder uses masked self-attention
-    chain_mask = chain_mask * chain_M_pos * mask #update chain_M to include missing regions
-    decoding_order = jnp.argsort((chain_mask+0.0001)*(jnp.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]
-
-    ar_mask = get_ar_mask(decoding_order)
-
-    mask_attend = jnp.take_along_axis(ar_mask, E_idx, 2)[...,None]
-    mask_1D = mask.reshape([mask.shape[0], mask.shape[1], 1, 1])
-    mask_bw = mask_1D * mask_attend
-    mask_fw = mask_1D * (1. - mask_attend)
-
-    N_batch, N_nodes = X.shape[0], X.shape[1]
-    log_probs = jnp.zeros((N_batch, N_nodes, 21))
-    all_probs = jnp.zeros((N_batch, N_nodes, 21))
-    h_S = jnp.zeros_like(h_V,)
-    S = jnp.zeros((N_batch, N_nodes), dtype=jnp.int32)
-    h_V_stack = [h_V] + [jnp.zeros_like(h_V) for _ in range(len(self.decoder_layers))]
-    constant = jnp.array(omit_AAs_np)
-    constant_bias = jnp.array(bias_AAs_np)
-    #chain_mask_combined = chain_mask*chain_M_pos 
-    omit_AA_mask_flag = omit_AA_mask != None
-
-    h_EX_encoder = cat_neighbors_nodes(jnp.zeros_like(h_S), h_E, E_idx)
-    h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)
-    h_EXV_encoder_fw = mask_fw * h_EXV_encoder
-
-    for t_ in range(N_nodes):
-      t = decoding_order[:, t_]  # [B]
-      chain_mask_gathered = jnp.take_along_axis(chain_mask, t[:,None], 1)  # [B]
-      bias_by_res_gathered = jnp.take_along_axis(bias_by_res, jnp.tile(t[:,None, None], [1,1,21]), 1)[:,0,:]  # [B, 21]
-      if jnp.equal(chain_mask_gathered, 0).all():
-        S_t = jnp.take_along_axis(S_true, t[:,None], 1)
-      else:
-        # Hidden layers
-        E_idx_t = jnp.take_along_axis(E_idx, jnp.tile(t[:,None,None], [1,1,E_idx.shape[-1]]), 1)
-        h_E_t = jnp.take_along_axis(h_E, jnp.tile(t[:,None,None,None], [1,1,h_E.shape[-2], h_E.shape[-1]]), 1)
-        h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)
-        h_EXV_encoder_t = jnp.take_along_axis(h_EXV_encoder_fw, jnp.tile(t[:,None,None,None], [1,1,h_EXV_encoder_fw.shape[-2], h_EXV_encoder_fw.shape[-1]]), 1)
-        mask_t = jnp.take_along_axis(mask, t[:,None], 1)
-        for l, layer in enumerate(self.decoder_layers):
-          # Updated relational features for future states
-          h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)
-          h_V_t = jnp.take_along_axis(h_V_stack[l], jnp.tile(t[:,None,None], [1,1,h_V_stack[l].shape[-1]]), 1)
-          h_ESV_t = jnp.take_along_axis(mask_bw, jnp.tile(t[:,None,None,None], [1,1,mask_bw.shape[-2], mask_bw.shape[-1]]), 1) * h_ESV_decoder_t + h_EXV_encoder_t
-          h_V_stack[l+1] = scatter(h_V_stack[l+1], 1, jnp.tile(t[:,None,None], [1,1,h_V.shape[-1]]), layer(h_V_t, h_ESV_t, mask_V=mask_t))
-        # Sampling step
-        h_V_t = jnp.take_along_axis(h_V_stack[-1], jnp.tile(t[:,None,None], [1,1,h_V_stack[-1].shape[-1]]), 1)[:,0]
-        logits = self.W_out(h_V_t) / temperature
-        probs = jax.nn.softmax(logits-constant[None,:]*1e8+constant_bias[None,:]/temperature+bias_by_res_gathered/temperature, axis=-1)
-        if pssm_bias_flag:
-          pssm_coef_gathered = jnp.take_along_axis(pssm_coef, t[:,None], 1)[:,0]
-          pssm_bias_gathered = jnp.take_along_axis(pssm_bias, jnp.tile(t[:,None,None], [1,1,pssm_bias.shape[-1]]), 1)[:,0]
-          probs = (1-pssm_multi*pssm_coef_gathered[:,None])*probs + pssm_multi*pssm_coef_gathered[:,None]*pssm_bias_gathered
-        if pssm_log_odds_flag:
-          pssm_log_odds_mask_gathered = jnp.take_along_axis(pssm_log_odds_mask, jnp.tile(t[:,None, None], [1,1,pssm_log_odds_mask.shape[-1]]), 1)[:,0] #[B, 21]
-          probs_masked = probs*pssm_log_odds_mask_gathered
-          probs_masked += probs * 0.001
-          probs = probs_masked/jnp.sum(probs_masked, axis=-1, keepdims=True) #[B, 21]
-        if omit_AA_mask_flag:
-          omit_AA_mask_gathered = jnp.take_along_axis(omit_AA_mask, jnp.tile(t[:,None, None], [1,1,omit_AA_mask.shape[-1]]), 1)[:,0] #[B, 21]
-          probs_masked = probs*(1.0-omit_AA_mask_gathered)
-          probs = probs_masked/jnp.sum(probs_masked, axis=-1, keepdims=True) #[B, 21]
-        used_key = jax.random.split(key, probs.shape[0])
-        input = jnp.tile(jnp.arange(probs.shape[1])[None], [probs.shape[0], 1])
-        S_t = jax.vmap(lambda key, input, prob: jax.random.choice(key, input, p=prob),
-                 in_axes=(0, 0, 0), out_axes=0)(used_key, input, probs)
-        all_probs = scatter(all_probs, 1, jnp.tile(t[:,None,None], [1,1,21]),
-          (chain_mask_gathered[:,:,None,]*probs[:,None,:]))
-      S_true_gathered = jnp.take_along_axis(S_true, t[:,None], 1)
-      S_t = (S_t*chain_mask_gathered+S_true_gathered*(1.0-chain_mask_gathered)).astype(int)
-      temp1 = self.W_s(S_t)
-      h_S = scatter(h_S, 1, jnp.tile(t[:,None,None], [1,1,temp1.shape[-1]]), temp1)
-      S = scatter(S, 1, t[:,None], S_t)
-    output_dict = {"S": S, "probs": all_probs, "decoding_order": decoding_order}
-    return output_dict
-
-
-  def tied_sample(self, key, X, randn, S_true,
-          chain_mask, chain_idx, residue_idx,
-          mask=None, temperature=1.0, omit_AAs_np=None,
-          bias_AAs_np=None, chain_M_pos=None, omit_AA_mask=None,
-          pssm_coef=None, pssm_bias=None, pssm_multi=None,
-          pssm_log_odds_flag=None, pssm_log_odds_mask=None,
-          pssm_bias_flag=None, tied_pos=None, tied_beta=None,
-          bias_by_res=None):
-
-    # Prepare node and edge embeddings
-    E, E_idx = self.features(X, mask, residue_idx, chain_idx)
-    h_V = jnp.zeros((E.shape[0], E.shape[1], E.shape[-1]))
-    h_E = self.W_e(E)
-    # Encoder is unmasked self-attention 
-    mask_attend = gather_nodes(mask[...,None],E_idx)[...,0]
-    mask_attend = mask[...,None] * mask_attend
-    for layer in self.encoder_layers:
-      h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)
-
-    # Decoder uses masked self-attention
-    chain_mask = chain_mask*chain_M_pos*mask #update chain_M to include missing regions
-    decoding_order = jnp.argsort((chain_mask+0.0001)*(jnp.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]
-
-    new_decoding_order = []
-    for t_dec in list(decoding_order[0]):
-      if t_dec not in list(itertools.chain(*new_decoding_order)):
-        list_a = [item for item in tied_pos if t_dec in item]
-        if list_a:
-          new_decoding_order.append(list_a[0])
-        else:
-          new_decoding_order.append([t_dec])
-    decoding_order = jnp.tile(jnp.array(list(itertools.chain(*new_decoding_order)))[None,], [X.shape[0], 1])
-
-    ar_mask = get_ar_mask(decoding_order)
-
-    mask_attend = jnp.take_along_axis(ar_mask, E_idx, 2)[...,None]
-    mask_1D = mask.reshape([mask.shape[0], mask.shape[1], 1, 1])
-    mask_bw = mask_1D * mask_attend
-    mask_fw = mask_1D * (1. - mask_attend)
-
-    N_batch, N_nodes = X.shape[0], X.shape[1]
-    log_probs = jnp.zeros((N_batch, N_nodes, 21))
-    all_probs = jnp.zeros((N_batch, N_nodes, 21))
-    h_S = jnp.zeros_like(h_V)
-    S = jnp.zeros((N_batch, N_nodes), dtype=jnp.int32)
-    h_V_stack = [h_V] + [jnp.zeros_like(h_V) for _ in range(len(self.decoder_layers))]
-    constant = jnp.array(omit_AAs_np)
-    constant_bias = jnp.array(bias_AAs_np)
-    omit_AA_mask_flag = omit_AA_mask != None
-
-    h_EX_encoder = cat_neighbors_nodes(jnp.zeros_like(h_S), h_E, E_idx)
-    h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)
-    h_EXV_encoder_fw = mask_fw * h_EXV_encoder
-    for t_list in new_decoding_order:
-      logits = 0.0
-      logit_list = []
-      done_flag = False
-      for t in t_list:
-        if (chain_mask[:,t]==0).all():
-          S_t = S_true[:,t]
-          for t in t_list:
-            h_S[:,t,:] = self.W_s(S_t)
-            S[:,t] = S_t
-          done_flag = True
-          break
-        else:
-          E_idx_t = E_idx[:,t:t+1,:]
-          h_E_t = h_E[:,t:t+1,:,:]
-          h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)
-          h_EXV_encoder_t = h_EXV_encoder_fw[:,t:t+1,:,:]
-          mask_t = mask[:,t:t+1]
-          for l, layer in enumerate(self.decoder_layers):
-            h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)
-            h_V_t = h_V_stack[l][:,t:t+1,:]
-            h_ESV_t = mask_bw[:,t:t+1,:,:] * h_ESV_decoder_t + h_EXV_encoder_t
-            h_V_stack[l+1] = h_V_stack[l+1].at[:,t,:].set(layer(h_V_t, h_ESV_t, mask_V=mask_t).squeeze(1))
-            # h_V_stack[l+1][:,t,:] = layer(h_V_t, h_ESV_t, mask_V=mask_t).squeeze(1)
-          h_V_t = h_V_stack[-1][:,t,:]
-          logit_list.append((self.W_out(h_V_t) / temperature)/len(t_list))
-          logits += tied_beta[t]*(self.W_out(h_V_t) / temperature)/len(t_list)
-      if done_flag:
-        pass
-      else:
-        bias_by_res_gathered = bias_by_res[:,t,:] #[B, 21]
-        probs = jax.nn.softmax(logits-constant[None,:]*1e8+constant_bias[None,:]/temperature+bias_by_res_gathered/temperature, axis=-1)
-        if pssm_bias_flag:
-          pssm_coef_gathered = pssm_coef[:,t]
-          pssm_bias_gathered = pssm_bias[:,t]
-          probs = (1-pssm_multi*pssm_coef_gathered[:,None])*probs + pssm_multi*pssm_coef_gathered[:,None]*pssm_bias_gathered
-        if pssm_log_odds_flag:
-          pssm_log_odds_mask_gathered = pssm_log_odds_mask[:,t]
-          probs_masked = probs*pssm_log_odds_mask_gathered
-          probs_masked += probs * 0.001
-          probs = probs_masked/jnp.sum(probs_masked, aixs=-1, keepdims=True) #[B, 21]
-        if omit_AA_mask_flag:
-          omit_AA_mask_gathered = omit_AA_mask[:,t]
-          probs_masked = probs*(1.0-omit_AA_mask_gathered)
-          probs = probs_masked/jnp.sum(probs_masked, axis=-1, keepdims=True) #[B, 21]
-        
-        used_key = jax.random.split(key, probs.shape[0])
-        input = jnp.tile(jnp.arange(probs.shape[1])[None], [probs.shape[0], 1])
-        S_t_repeat = jax.vmap(lambda key, input, prob: jax.random.choice(key, input, p=prob),
-                    in_axes=(0, 0, 0), out_axes=0)(used_key, input, probs)
-
-        for t in t_list:
-          h_S = h_S.at[:,t,:].set(self.W_s(S_t_repeat))
-          S = S.at[:,t].set(S_t_repeat)
-          all_probs = all_probs.at[:,t,:].set(probs)
-    output_dict = {"S": S, "probs": all_probs, "decoding_order": decoding_order}
-    return output_dict
\ No newline at end of file
diff --git a/colabdesign/mpnn/legacy/utils.py b/colabdesign/mpnn/legacy/utils.py
deleted file mode 100644
index 9d35bb3..0000000
--- a/colabdesign/mpnn/legacy/utils.py
+++ /dev/null
@@ -1,469 +0,0 @@
-import numpy as np
-import itertools
-import jax.numpy as jnp
-import jax
-import time
-
-# The following gather functions
-def gather_edges(edges, neighbor_idx):
-  # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]
-  neighbors = jnp.tile(jnp.expand_dims(neighbor_idx, -1), [1, 1, 1, edges.shape[-1]])
-  edge_features = jnp.take_along_axis(edges, neighbors, 2)
-  return edge_features
-
-def gather_nodes(nodes, neighbor_idx):
-  # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]
-  # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]
-  neighbors_flat = neighbor_idx.reshape([neighbor_idx.shape[0], -1])
-  neighbors_flat = jnp.tile(jnp.expand_dims(neighbors_flat, -1),[1, 1, nodes.shape[2]])
-  # Gather and re-pack
-  neighbor_features = jnp.take_along_axis(nodes, neighbors_flat, 1)
-  neighbor_features = neighbor_features.reshape(list(neighbor_idx.shape[:3]) + [-1])
-  return neighbor_features
-
-def gather_nodes_t(nodes, neighbor_idx):
-  # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]
-  idx_flat = jnp.tile(jnp.expand_dims(neighbor_idx, -1),[1, 1, nodes.shape[2]])
-  neighbor_features = jnp.take_along_axis(nodes, idx_flat, 1)
-  return neighbor_features
-
-def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):
-  h_nodes = gather_nodes(h_nodes, E_idx)
-  h_nn = jnp.concatenate([h_neighbors, h_nodes], -1)
-  return h_nn
-
-def scatter(input, dim, index, src):
-  idx = jnp.indices(index.shape)
-  dim_num = idx.shape[0]  # dimension of the dim
-  a = idx.at[dim].set(index)
-  idx = jnp.moveaxis(idx, 0, -1).reshape((-1, dim_num))
-  a = jnp.moveaxis(a, 0, -1).reshape((-1, dim_num))
-  return input.at[tuple(a.T)].set(src[tuple(idx.T)])
-
-def get_ar_mask(order):
-  '''compute autoregressive mask, given order of positions'''
-  L = order.shape[-1]
-  oh_order = jax.nn.one_hot(order, L)
-  tri = jnp.tri(L, k=-1)
-  return jnp.einsum('ij,...iq,...jp->...qp', tri, oh_order, oh_order)
-
-class StructureDatasetPDB():
-  def __init__(self, pdb_dict_list, verbose=True, truncate=None, max_length=100,
-    alphabet='ACDEFGHIKLMNPQRSTVWYX-'):
-    alphabet_set = set([a for a in alphabet])
-    discard_count = {
-      'bad_chars': 0,
-      'too_long': 0,
-      'bad_seq_length': 0
-    }
-    self.data = []
-
-    start = time.time()
-    for i, entry in enumerate(pdb_dict_list):
-      seq = entry['seq']
-      name = entry['name']
-
-      bad_chars = set([s for s in seq]).difference(alphabet_set)
-      if len(bad_chars) == 0:
-        if len(entry['seq']) <= max_length:
-          self.data.append(entry)
-        else:
-          discard_count['too_long'] += 1
-      else:
-        discard_count['bad_chars'] += 1
-
-      # Truncate early
-      if truncate is not None and len(self.data) == truncate:
-        return
-
-      if verbose and (i + 1) % 1000 == 0:
-        elapsed = time.time() - start
-
-      #print('Discarded', discard_count)
-  def __len__(self):
-    return len(self.data)
-
-  def __getitem__(self, idx):
-    return self.data[idx]
-
-
-def _S_to_seq(S, mask):
-  alphabet = 'ACDEFGHIKLMNPQRSTVWYX'
-  seq = ''.join([alphabet[c] for c, m in zip(S.tolist(), mask.tolist()) if m > 0])
-  return seq
-
-
-def parse_PDB_biounits(x, atoms=['N', 'CA', 'C'], chain=None):
-  '''
-  input:  x = PDB filename
-      atoms = atoms to extract (optional)
-  output: (length, atoms, coords=(x,y,z)), sequence
-  '''
-
-  alpha_1 = list("ARNDCQEGHILKMFPSTWYV-")
-  states = len(alpha_1)
-  alpha_3 = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE',
-         'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'GAP']
-
-  aa_1_N = {a: n for n, a in enumerate(alpha_1)}
-  aa_3_N = {a: n for n, a in enumerate(alpha_3)}
-  aa_N_1 = {n: a for n, a in enumerate(alpha_1)}
-  aa_1_3 = {a: b for a, b in zip(alpha_1, alpha_3)}
-  aa_3_1 = {b: a for a, b in zip(alpha_1, alpha_3)}
-
-  def AA_to_N(x):
-    # ["ARND"] -> [[0,1,2,3]]
-    x = np.array(x)
-    if x.ndim == 0:
-      x = x[None]
-    return [[aa_1_N.get(a, states-1) for a in y] for y in x]
-
-  def N_to_AA(x):
-    # [[0,1,2,3]] -> ["ARND"]
-    x = np.array(x)
-    if x.ndim == 1:
-      x = x[None]
-    return ["".join([aa_N_1.get(a, "-") for a in y]) for y in x]
-
-  xyz, seq, min_resn, max_resn = {}, {}, 1e6, -1e6
-  for line in open(x, "rb"):
-    line = line.decode("utf-8", "ignore").rstrip()
-
-    if line[:6] == "HETATM" and line[17:17+3] == "MSE":
-      line = line.replace("HETATM", "ATOM  ")
-      line = line.replace("MSE", "MET")
-
-    if line[:4] == "ATOM":
-      ch = line[21:22]
-      if ch == chain or chain is None:
-        atom = line[12:12+4].strip()
-        resi = line[17:17+3]
-        resn = line[22:22+5].strip()
-        x, y, z = [float(line[i:(i+8)]) for i in [30, 38, 46]]
-
-        if resn[-1].isalpha():
-          resa, resn = resn[-1], int(resn[:-1])-1
-        else:
-          resa, resn = "", int(resn)-1
-#     resn = int(resn)
-        if resn < min_resn:
-          min_resn = resn
-        if resn > max_resn:
-          max_resn = resn
-        if resn not in xyz:
-          xyz[resn] = {}
-        if resa not in xyz[resn]:
-          xyz[resn][resa] = {}
-        if resn not in seq:
-          seq[resn] = {}
-        if resa not in seq[resn]:
-          seq[resn][resa] = resi
-
-        if atom not in xyz[resn][resa]:
-          xyz[resn][resa][atom] = np.array([x, y, z])
-
-  # convert to numpy arrays, fill in missing values
-  seq_, xyz_ = [], []
-  try:
-    for resn in range(min_resn, max_resn+1):
-      if resn in seq:
-        for k in sorted(seq[resn]):
-          seq_.append(aa_3_N.get(seq[resn][k], 20))
-      else:
-        seq_.append(20)
-      if resn in xyz:
-        for k in sorted(xyz[resn]):
-          for atom in atoms:
-            if atom in xyz[resn][k]:
-              xyz_.append(xyz[resn][k][atom])
-            else:
-              xyz_.append(np.full(3, np.nan))
-      else:
-        for atom in atoms:
-          xyz_.append(np.full(3, np.nan))
-    return np.array(xyz_).reshape(-1, len(atoms), 3), N_to_AA(np.array(seq_))
-  except TypeError:
-    return 'no_chain', 'no_chain'
-
-
-def parse_PDB(path_to_pdb, input_chain_list=None):
-  c = 0
-  pdb_dict_list = []
-  init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',
-           'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
-  extra_alphabet = [str(item) for item in list(np.arange(300))]
-  chain_alphabet = init_alphabet + extra_alphabet
-
-  if input_chain_list:
-    chain_alphabet = input_chain_list
-
-  biounit_names = [path_to_pdb]
-  for biounit in biounit_names:
-    my_dict = {}
-    s = 0
-    concat_seq = ''
-    concat_N = []
-    concat_CA = []
-    concat_C = []
-    concat_O = []
-    concat_mask = []
-    coords_dict = {}
-    for letter in chain_alphabet:
-      xyz, seq = parse_PDB_biounits(
-        biounit, atoms=['N', 'CA', 'C', 'O'], chain=letter)
-      if type(xyz) != str:
-        concat_seq += seq[0]
-        my_dict['seq_chain_'+letter] = seq[0]
-        coords_dict_chain = {}
-        coords_dict_chain['N_chain_'+letter] = xyz[:, 0, :].tolist()
-        coords_dict_chain['CA_chain_'+letter] = xyz[:, 1, :].tolist()
-        coords_dict_chain['C_chain_'+letter] = xyz[:, 2, :].tolist()
-        coords_dict_chain['O_chain_'+letter] = xyz[:, 3, :].tolist()
-        my_dict['coords_chain_'+letter] = coords_dict_chain
-        s += 1
-    fi = biounit.rfind("/")
-    my_dict['name'] = biounit[(fi+1):-4]
-    my_dict['num_of_chains'] = s
-    my_dict['seq'] = concat_seq
-    if s <= len(chain_alphabet):
-      pdb_dict_list.append(my_dict)
-      c += 1
-  return pdb_dict_list
-
-
-def tied_featurize(batch, chain_dict,
-           fixed_position_dict=None, omit_AA_dict=None,
-           tied_positions_dict=None, pssm_dict=None,
-           bias_by_res_dict=None):
-  """ Pack and pad batch into torch tensors """
-  alphabet = 'ACDEFGHIKLMNPQRSTVWYX'
-  B = len(batch)
-  lengths = np.array([len(b['seq']) for b in batch], int) #sum of chain seq lengths
-  L_max = max([len(b['seq']) for b in batch])
-  X = np.zeros([B, L_max, 4, 3])
-  residue_idx = -100*np.ones([B, L_max], int)
-  chain_M = np.zeros([B, L_max], int) #1.0 for the bits that need to be predicted
-  pssm_coef_all = np.zeros([B, L_max], float) #1.0 for the bits that need to be predicted
-  pssm_bias_all = np.zeros([B, L_max, 21], float) #1.0 for the bits that need to be predicted
-  pssm_log_odds_all = 10000.0*np.ones([B, L_max, 21], float) #1.0 for the bits that need to be predicted
-  chain_M_pos = np.zeros([B, L_max], int) #1.0 for the bits that need to be predicted
-  bias_by_res_all = np.zeros([B, L_max, 21], float)
-  chain_idx = np.zeros([B, L_max], int) #1.0 for the bits that need to be predicted
-  S = np.zeros([B, L_max], int)
-  omit_AA_mask = np.zeros([B, L_max, len(alphabet)], int)
-  # Build the batch
-  letter_list_list = []
-  visible_list_list = []
-  masked_list_list = []
-  masked_chain_length_list_list = []
-  tied_pos_list_of_lists_list = []
-  #shuffle all chains before the main loop
-  for i, b in enumerate(batch):
-    if chain_dict != None:
-      masked_chains, visible_chains = chain_dict[b['name']] #masked_chains a list of chain letters to predict [A, D, F]
-    else:
-      masked_chains = [item[-1:] for item in list(b) if item[:10]=='seq_chain_']
-      visible_chains = []
-    num_chains = b['num_of_chains']
-    all_chains = masked_chains + visible_chains
-    #random.shuffle(all_chains)
-  for i, b in enumerate(batch):
-    mask_dict = {}
-    a = 0
-    x_chain_list = []
-    chain_mask_list = []
-    chain_seq_list = []
-    chain_encoding_list = []
-    c = 1
-    letter_list = []
-    global_idx_start_list = [0]
-    visible_list = []
-    masked_list = []
-    masked_chain_length_list = []
-    fixed_position_mask_list = []
-    omit_AA_mask_list = []
-    pssm_coef_list = []
-    pssm_bias_list = []
-    pssm_log_odds_list = []
-    bias_by_res_list = []
-    l0 = 0
-    l1 = 0
-    for step, letter in enumerate(all_chains):
-      if letter in visible_chains:
-        letter_list.append(letter)
-        visible_list.append(letter)
-        chain_seq = b[f'seq_chain_{letter}']
-        chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])
-        chain_length = len(chain_seq)
-        global_idx_start_list.append(global_idx_start_list[-1]+chain_length)
-        chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary
-        chain_mask = np.zeros(chain_length) #0.0 for visible chains
-        x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]
-        x_chain_list.append(x_chain)
-        chain_mask_list.append(chain_mask)
-        chain_seq_list.append(chain_seq)
-        chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))
-        l1 += chain_length
-        residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)
-        l0 += chain_length
-        c+=1
-        fixed_position_mask = np.ones(chain_length)
-        fixed_position_mask_list.append(fixed_position_mask)
-        omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)
-        omit_AA_mask_list.append(omit_AA_mask_temp)
-        pssm_coef = np.zeros(chain_length)
-        pssm_bias = np.zeros([chain_length, 21])
-        pssm_log_odds = 10000.0*np.ones([chain_length, 21])
-        pssm_coef_list.append(pssm_coef)
-        pssm_bias_list.append(pssm_bias)
-        pssm_log_odds_list.append(pssm_log_odds)
-        bias_by_res_list.append(np.zeros([chain_length, 21]))
-      if letter in masked_chains:
-        masked_list.append(letter)
-        letter_list.append(letter)
-        chain_seq = b[f'seq_chain_{letter}']
-        chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])
-        chain_length = len(chain_seq)
-        global_idx_start_list.append(global_idx_start_list[-1]+chain_length)
-        masked_chain_length_list.append(chain_length)
-        chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary
-        chain_mask = np.ones(chain_length) #1.0 for masked
-        x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]
-        x_chain_list.append(x_chain)
-        chain_mask_list.append(chain_mask)
-        chain_seq_list.append(chain_seq)
-        chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))
-        l1 += chain_length
-        residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)
-        l0 += chain_length
-        c+=1
-        fixed_position_mask = np.ones(chain_length)
-        if fixed_position_dict!=None:
-          fixed_pos_list = fixed_position_dict[b['name']][letter]
-          if fixed_pos_list:
-            fixed_position_mask[np.array(fixed_pos_list)-1] = 0.0
-        fixed_position_mask_list.append(fixed_position_mask)
-        omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)
-        if omit_AA_dict!=None:
-          for item in omit_AA_dict[b['name']][letter]:
-            idx_AA = np.array(item[0])-1
-            AA_idx = np.array([np.argwhere(np.array(list(alphabet))== AA)[0][0] for AA in item[1]]).repeat(idx_AA.shape[0])
-            idx_ = np.array([[a, b] for a in idx_AA for b in AA_idx])
-            omit_AA_mask_temp[idx_[:,0], idx_[:,1]] = 1
-        omit_AA_mask_list.append(omit_AA_mask_temp)
-        pssm_coef = np.zeros(chain_length)
-        pssm_bias = np.zeros([chain_length, 21])
-        pssm_log_odds = 10000.0*np.ones([chain_length, 21])
-        if pssm_dict:
-          if pssm_dict[b['name']][letter]:
-            pssm_coef = pssm_dict[b['name']][letter]['pssm_coef']
-            pssm_bias = pssm_dict[b['name']][letter]['pssm_bias']
-            pssm_log_odds = pssm_dict[b['name']][letter]['pssm_log_odds']
-        pssm_coef_list.append(pssm_coef)
-        pssm_bias_list.append(pssm_bias)
-        pssm_log_odds_list.append(pssm_log_odds)
-        if bias_by_res_dict:
-          bias_by_res_list.append(bias_by_res_dict[b['name']][letter])
-        else:
-          bias_by_res_list.append(np.zeros([chain_length, 21]))
-
-     
-    letter_list_np = np.array(letter_list)
-    tied_pos_list_of_lists = []
-    tied_beta = np.ones(L_max)
-    if tied_positions_dict!=None:
-      tied_pos_list = tied_positions_dict[b['name']]
-      if tied_pos_list:
-        set_chains_tied = set(list(itertools.chain(*[list(item) for item in tied_pos_list])))
-        for tied_item in tied_pos_list:
-          one_list = []
-          for k, v in tied_item.items():
-            start_idx = global_idx_start_list[np.argwhere(letter_list_np == k)[0][0]]
-            if isinstance(v[0], list):
-              for v_count in range(len(v[0])):
-                one_list.append(start_idx+v[0][v_count]-1)#make 0 to be the first
-                tied_beta[start_idx+v[0][v_count]-1] = v[1][v_count]
-            else:
-              for v_ in v:
-                one_list.append(start_idx+v_-1)#make 0 to be the first
-          tied_pos_list_of_lists.append(one_list)
-    tied_pos_list_of_lists_list.append(tied_pos_list_of_lists)
-
-
- 
-    x = np.concatenate(x_chain_list,0) #[L, 4, 3]
-    all_sequence = "".join(chain_seq_list)
-    m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted
-    chain_encoding = np.concatenate(chain_encoding_list,0)
-    m_pos = np.concatenate(fixed_position_mask_list,0) #[L,], 1.0 for places that need to be predicted
-
-    pssm_coef_ = np.concatenate(pssm_coef_list,0) #[L,], 1.0 for places that need to be predicted
-    pssm_bias_ = np.concatenate(pssm_bias_list,0) #[L,], 1.0 for places that need to be predicted
-    pssm_log_odds_ = np.concatenate(pssm_log_odds_list,0) #[L,], 1.0 for places that need to be predicted
-
-    bias_by_res_ = np.concatenate(bias_by_res_list, 0)  #[L,21], 0.0 for places where AA frequencies don't need to be tweaked
-
-    l = len(all_sequence)
-    x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))
-    X[i,:,:,:] = x_pad
-
-    m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))
-    m_pos_pad = np.pad(m_pos, [[0,L_max-l]], 'constant', constant_values=(0.0, ))
-    omit_AA_mask_pad = np.pad(np.concatenate(omit_AA_mask_list,0), [[0,L_max-l]], 'constant', constant_values=(0.0, ))
-    chain_M[i,:] = m_pad
-    chain_M_pos[i,:] = m_pos_pad
-    omit_AA_mask[i,] = omit_AA_mask_pad
-
-    chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))
-    chain_idx[i,:] = chain_encoding_pad
-
-    pssm_coef_pad = np.pad(pssm_coef_, [[0,L_max-l]], 'constant', constant_values=(0.0, ))
-    pssm_bias_pad = np.pad(pssm_bias_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))
-    pssm_log_odds_pad = np.pad(pssm_log_odds_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))
-
-    pssm_coef_all[i,:] = pssm_coef_pad
-    pssm_bias_all[i,:] = pssm_bias_pad
-    pssm_log_odds_all[i,:] = pssm_log_odds_pad
-
-    bias_by_res_pad = np.pad(bias_by_res_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))
-    bias_by_res_all[i,:] = bias_by_res_pad
-
-    # Convert to labels
-    indices = np.asarray([alphabet.index(a) for a in all_sequence], int)
-    S[i, :l] = indices
-    letter_list_list.append(letter_list)
-    visible_list_list.append(visible_list)
-    masked_list_list.append(masked_list)
-    masked_chain_length_list_list.append(masked_chain_length_list)
-
-
-  isnan = np.isnan(X)
-  mask = np.isfinite(np.sum(X,(2,3))).astype(float)
-  X[isnan] = 0.
-
-  # Conversion
-  pssm_coef_all = jnp.array(pssm_coef_all, float)
-  pssm_bias_all = jnp.array(pssm_bias_all, float)
-  pssm_log_odds_all = jnp.array(pssm_log_odds_all, float)
-
-  tied_beta = jnp.array(tied_beta, float)
-
-  jumps = ((residue_idx[:,1:]-residue_idx[:,:-1])==1).astype(float)
-  bias_by_res_all = jnp.array(bias_by_res_all, float)
-  phi_mask = np.pad(jumps, [[0,0],[1,0]])
-  psi_mask = np.pad(jumps, [[0,0],[0,1]])
-  omega_mask = np.pad(jumps, [[0,0],[0,1]])
-  dihedral_mask = np.concatenate([phi_mask[:,:,None], psi_mask[:,:,None], omega_mask[:,:,None]], -1) #[B,L,3]
-  dihedral_mask = jnp.array(dihedral_mask, float)
-  residue_idx = jnp.array(residue_idx, int)
-  S = jnp.array(S, int)
-  X = jnp.array(X, float)
-  mask = jnp.array(mask, float)
-  chain_M = jnp.array(chain_M, float)
-  chain_M_pos = jnp.array(chain_M_pos, float)
-  omit_AA_mask = jnp.array(omit_AA_mask, float)
-  chain_idx = jnp.array(chain_idx, int)
-  return X, S, mask, lengths, chain_M, chain_idx, letter_list_list, \
-       visible_list_list, masked_list_list, masked_chain_length_list_list, \
-       chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, \
-       pssm_coef_all, pssm_bias_all, pssm_log_odds_all, bias_by_res_all, tied_beta
diff --git a/colabdesign/mpnn/legacy/wrapper.py b/colabdesign/mpnn/legacy/wrapper.py
deleted file mode 100644
index be239b8..0000000
--- a/colabdesign/mpnn/legacy/wrapper.py
+++ /dev/null
@@ -1,294 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-import re
-import copy
-import random
-import os
-import joblib
-from tqdm import tqdm
-
-from .modules import RunModel
-from .utils import parse_PDB, StructureDatasetPDB, tied_featurize, _S_to_seq
-from colabdesign.shared.prng import SafeKey
-from colabdesign.mpnn.jax_weights import __file__ as mpnn_path
-
-class MPNN_wrapper:  
-  def __init__(self,
-         model_name="v_48_020", verbose=False):
-    self.model_name = model_name
-
-    backbone_noise = 0.00  # Standard deviation of Gaussian noise to add to backbone atoms
-    hidden_dim = 128
-    num_layers = 3 
-
-    path = os.path.join(os.path.dirname(mpnn_path), f'{model_name}.pkl')    
-    checkpoint = joblib.load(path)
-    params = jax.tree_util.tree_map(jnp.array, checkpoint['model_state_dict'])
-
-    if verbose:
-      print('Number of edges:', checkpoint['num_edges'])
-      noise_level_print = checkpoint['noise_level']
-      print(f'Training noise level: {noise_level_print}A')
-
-    config = {'num_letters': 21,
-          'node_features': hidden_dim,
-          'edge_features': hidden_dim,
-          'hidden_dim': hidden_dim,
-          'num_encoder_layers': num_layers,
-          'num_decoder_layers': num_layers,
-          'augment_eps': backbone_noise,
-          'k_neighbors': checkpoint['num_edges'],
-          'dropout': 0.0
-         }
-
-    model = RunModel(config)
-    model.params = params
-    self.model = model
-
-    self.alphabet = 'ACDEFGHIKLMNPQRSTVWYX'
-    self.max_length = 20000
-
-    seed = random.randint(0,2147483647)
-    seed = jax.random.PRNGKey(seed)
-    self.safe_key = SafeKey(seed)
-  
-  def prep_inputs(self, pdb_path,
-          target_chain, fixed_chain=None,
-          ishomomer=False, omit_AAs='X'):
-    """generate input for score and sampling function
-
-    Args:
-      pdb_path (str): the path of the pdb file
-      target_chain (str): chain ID of the protein sequence
-      fixed_chain (str, optional): chain ID of the protein sequence that should be fixed. Defaults to None.
-      ishomomer (bool, optional): for tie sampling. Defaults to False.
-      omit_AAs (str, optional): aas should not be generated in sampling. Defaults to 'X'.
-
-    Returns:
-      dict: input dictionary
-    """    
-    # initialize some var
-    fixed_positions_dict = None
-    pssm_dict = None
-    omit_AA_dict = None
-    bias_by_res_dict = None
-    bias_AAs_np = np.zeros(len(self.alphabet))
-    pssm_threshold = 0.0
-    pssm_multi = 0.0
-    pssm_log_odds_flag = 0
-    pssm_bias_flag = 0
-
-    # fixed chain
-    if fixed_chain is None:
-      fixed_chain = ''
-      fixed_chain_list = []
-    else:
-      fixed_chain_list = re.sub("[^A-Za-z]+",",", fixed_chain).split(",")
-
-    # design chains
-    if target_chain == '':
-      designed_chain_list = []
-    else:
-      designed_chain_list = re.sub("[^A-Za-z]+",",", target_chain).split(",")
-
-    #chain list
-    chain_list = list(set(designed_chain_list + fixed_chain_list))
-
-    # omit AAs
-    omit_AAs_list = omit_AAs
-    omit_AAs_np = np.array([AA in omit_AAs_list for AA in self.alphabet]).astype(np.float32)
-  
-    # prepare input
-    pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)
-    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=self.max_length)
-
-    chain_id_dict = {}
-    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)
-
-    if ishomomer:
-      # haven't tested
-      tied_positions_dict = self.make_tied_positions_for_homomers(pdb_dict_list)
-    else:
-      tied_positions_dict = None
-
-    return {'dataset_valid': dataset_valid,
-        'chain_id_dict': chain_id_dict,
-        'fixed_positions_dict': fixed_positions_dict,
-        'omit_AA_dict': omit_AA_dict,
-        'tied_positions_dict': tied_positions_dict,
-        'pssm_dict': pssm_dict,
-        'bias_by_res_dict': bias_by_res_dict,
-        'pssm_threshold': pssm_threshold,
-        'omit_AAs_np': omit_AAs_np,
-        'bias_AAs_np': bias_AAs_np,
-        'pssm_multi': pssm_multi,
-        'pssm_log_odds_flag': pssm_log_odds_flag,
-        'pssm_bias_flag': pssm_bias_flag,
-         }
-  
-  def score(self, inputs, seq=None, order=None, key=None, unconditional=False):
-    """get the output of MPNN
-
-    Args:
-      inputs (dict): output of the prep_input function
-      seq (str, optional): the input sequence.
-                 If not provided, the original sequence will be used.
-                 Defaults to None.
-      order (array, optional): the decoding order.
-                   If not provided, the decoding order is random.
-                   Defaults to None.
-      key (jax.random.PRNGkey, optional): the random seed. Defaults to None.
-
-    Returns:
-      logits
-      log_probs
-    """    
-    protein = inputs['dataset_valid'][0]
-    batch_clones = [copy.deepcopy(protein)]
-    (X, S, mask, lengths, chain_M, chain_idx, chain_list_list,
-     visible_list_list, masked_list_list, masked_chain_length_list_list,
-     chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask,
-     tied_pos_list_of_lists_list, pssm_coef, pssm_bias,
-     pssm_log_odds_all, bias_by_res_all, tied_beta) = tied_featurize(batch_clones,
-                                     inputs['chain_id_dict'], inputs['fixed_positions_dict'],
-                                     inputs['omit_AA_dict'], inputs['tied_positions_dict'],
-                                     inputs['pssm_dict'], inputs['bias_by_res_dict'])
-    score_input = {'X': X,
-                 'S': S,
-                 'mask': mask,
-                 'chain_M': chain_M * chain_M_pos,
-                 'residue_idx': residue_idx,
-                 'chain_idx': chain_idx}
-
-    if unconditional:
-      score_input["S"] = None
-    else:
-      if seq is not None:
-        S = np.asarray([self.alphabet.index(a) for a in seq], dtype=np.int32)
-        S = S[None, :]
-        score_input['S'] = jnp.array(S)
-
-      if order is None:
-        if key is not None:
-          self.safe_key = SafeKey(key)
-        self.safe_key, used_key = self.safe_key.split()
-        order = jax.random.normal(used_key.get(), (chain_M.shape[1],))
-      score_input['randn'] = jnp.expand_dims(order, 0)
-       
-    self.safe_key, used_key = self.safe_key.split()
-    return self.model.score(self.model.params, used_key.get(), score_input)
-   
-  def sampling(self, inputs,
-         sample_num, batch_size,
-         sampling_temp=0.1, order=None, key=None):
-    """sample sequences from the given protein structure
-
-    Args:
-      inputs (dict): output of the prep_input function
-      sample_num (int): number of sequences you want to generate
-      batch_size (int): size of one batch
-      sampling_temp (float, optional): sampling temperature. Defaults to 0.1.
-      order (array, optional): the sampling order.
-                   If not provided, the order is random.
-                   Defaults to None.
-      key (jax.random.PRNGkey, optional): the random seed. Defaults to None.
-
-    Returns:
-      seq_gen (list): generated sequence
-    """
-    NUM_BATCHES = sample_num//batch_size
-    BATCH_COPIES = batch_size
-    if key is not None:
-      self.safe_key = SafeKey(key)
-
-    protein = inputs['dataset_valid'][0]
-    batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]
-    (X, S, mask, lengths, chain_M, chain_idx, chain_list_list,
-     visible_list_list, masked_list_list, masked_chain_length_list_list,
-     chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask,
-     tied_pos_list_of_lists_list, pssm_coef, pssm_bias,
-     pssm_log_odds_all, bias_by_res_all, tied_beta) = tied_featurize(batch_clones,
-                                     inputs['chain_id_dict'], inputs['fixed_positions_dict'],
-                                     inputs['omit_AA_dict'], inputs['tied_positions_dict'],
-                                     inputs['pssm_dict'], inputs['bias_by_res_dict'])
-    pssm_log_odds_mask = jax.lax.convert_element_type((pssm_log_odds_all > inputs['pssm_threshold']),
-                              jnp.float32)  # 1.0 for true, 0.0 for false
-
-    if order is None:
-      self.safe_key, used_key = self.safe_key.split()
-      order = jax.random.normal(used_key.get(), (chain_M.shape[1],))
-    randn_1 = jnp.expand_dims(order, 0)
-
-    # sample input
-    sample_input = {'X': X,
-            'randn': randn_1,
-            'S_true': S,
-            'chain_mask': chain_M,
-            'chain_idx': chain_idx,
-            'residue_idx': residue_idx,
-            'mask': mask,
-            'temperature': sampling_temp,
-            'omit_AAs_np': inputs['omit_AAs_np'],
-            'bias_AAs_np': inputs['bias_AAs_np'],
-            'chain_M_pos': chain_M_pos,
-            'omit_AA_mask': omit_AA_mask,
-            'pssm_coef': pssm_coef,
-            'pssm_bias': pssm_bias,
-            'pssm_multi': inputs['pssm_multi'],
-            'pssm_log_odds_flag': bool(inputs['pssm_log_odds_flag']),
-            'pssm_log_odds_mask': pssm_log_odds_mask,
-            'pssm_bias_flag': bool(inputs['pssm_bias_flag']),
-            'bias_by_res': bias_by_res_all
-            }
-    seq_gen = []
-    for _ in tqdm(range(NUM_BATCHES)):
-      self.safe_key, used_key = self.safe_key.split()
-      sample_input.update({'key': used_key.get()})
-
-      self.safe_key, used_key = self.safe_key.split()
-      if inputs['tied_positions_dict'] is None:
-        sample_dict = self.model.sample(self.model.params, used_key.get(), sample_input)
-      else:
-        sample_input.update({'tied_pos': tied_pos_list_of_lists_list[0],
-                   'tied_beta': tied_beta,
-                   'bias_by_res': bias_by_res_all,
-                  })
-        sample_dict = self.model.tied_sample(self.model.params, used_key.get(), sample_input)
-      S_sample = sample_dict["S"]
-      for b_ix in range(BATCH_COPIES):
-        masked_chain_length_list = masked_chain_length_list_list[b_ix]
-        masked_list = masked_list_list[b_ix]
-        seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])
-
-        start = 0
-        end = 0
-        list_of_AAs = []
-        for mask_l in masked_chain_length_list:
-          end += mask_l
-          list_of_AAs.append(seq[start:end])
-          start = end
-
-        seq = "".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))
-        l0 = 0
-        for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:
-          l0 += mc_length
-          seq = seq[:l0] + '/' + seq[l0:]
-          l0 += 1
-        seq_gen.append(seq)
-    return seq_gen
-
-  @staticmethod
-  def make_tied_positions_for_homomers(pdb_dict_list):
-    my_dict = {}
-    for result in pdb_dict_list:
-      all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain'])  # A, B, C, ...
-      tied_positions_list = []
-      chain_length = len(result[f"seq_chain_{all_chain_list[0]}"])
-      for i in range(1,chain_length+1):
-        temp_dict = {}
-        for j, chain in enumerate(all_chain_list):
-          temp_dict[chain] = [i] #needs to be a list
-        tied_positions_list.append(temp_dict)
-      my_dict[result['name']] = tied_positions_list
-    return my_dict
diff --git a/colabdesign/mpnn/model.py b/colabdesign/mpnn/model.py
deleted file mode 100644
index 5357525..0000000
--- a/colabdesign/mpnn/model.py
+++ /dev/null
@@ -1,324 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-import re
-import copy
-import random
-import os
-import joblib
-
-from .modules import RunModel
-
-from colabdesign.shared.prep import prep_pos
-from colabdesign.shared.utils import Key, copy_dict
-
-# borrow some stuff from AfDesign
-from colabdesign.af.prep import prep_pdb
-from colabdesign.af.alphafold.common import protein, residue_constants
-aa_order = residue_constants.restype_order
-order_aa = {b:a for a,b in aa_order.items()}
-
-from scipy.special import softmax, log_softmax
-
-class mk_mpnn_model():
-  def __init__(self, model_name="v_48_020",
-               backbone_noise=0.0, dropout=0.0,
-               seed=None, verbose=False, weights="original"): # weights can be set to either original or soluble
-    # load model
-    if weights == "original":
-      from .weights import __file__ as mpnn_path
-    elif weights == "soluble":
-      from .weights_soluble import __file__ as mpnn_path
-    else:
-      raise ValueError(f'Invalid value {weights} supplied for weights. Value must be either "original" or "soluble".')
-
-    path = os.path.join(os.path.dirname(mpnn_path), f'{model_name}.pkl')
-    checkpoint = joblib.load(path)
-    config = {'num_letters': 21,
-              'node_features': 128,
-              'edge_features': 128,
-              'hidden_dim': 128,
-              'num_encoder_layers': 3,
-              'num_decoder_layers': 3,
-              'augment_eps': backbone_noise,
-              'k_neighbors': checkpoint['num_edges'],
-              'dropout': dropout}
-    
-    self._model = RunModel(config)
-    self._model.params = jax.tree_map(np.array, checkpoint['model_state_dict'])
-    self._setup()
-    self.set_seed(seed)
-
-    self._num = 1
-    self._inputs = {}
-    self._tied_lengths = False
-
-  def prep_inputs(self, pdb_filename=None, chain=None, homooligomer=False,
-                  ignore_missing=True, fix_pos=None, inverse=False,
-                  rm_aa=None, verbose=False, **kwargs):
-    
-    '''get inputs from input pdb'''
-    pdb = prep_pdb(pdb_filename, chain, ignore_missing=ignore_missing)
-    atom_idx = tuple(residue_constants.atom_order[k] for k in ["N","CA","C","O"])
-    chain_idx = np.concatenate([[n]*l for n,l in enumerate(pdb["lengths"])])
-    self._lengths = pdb["lengths"]
-    L = sum(self._lengths)
-
-    self._inputs = {"X":           pdb["batch"]["all_atom_positions"][:,atom_idx],
-                    "mask":        pdb["batch"]["all_atom_mask"][:,1],
-                    "S":           pdb["batch"]["aatype"],
-                    "residue_idx": pdb["residue_index"],
-                    "chain_idx":   chain_idx,
-                    "lengths":     np.array(self._lengths),
-                    "bias":        np.zeros((L,20))}
-    
-
-    if rm_aa is not None:
-      for aa in rm_aa.split(","):
-        self._inputs["bias"][...,aa_order[aa]] -= 1e6
-    
-    if fix_pos is not None:
-      p = prep_pos(fix_pos, **pdb["idx"])["pos"]
-      if inverse:
-        p = np.delete(np.arange(L),p)
-      self._inputs["fix_pos"] = p
-      self._inputs["bias"][p] = 1e7 * np.eye(21)[self._inputs["S"]][p,:20]
-    
-    if homooligomer:
-      assert min(self._lengths) == max(self._lengths)
-      self._tied_lengths = True
-      self._len = self._lengths[0]
-    else:
-      self._tied_lengths = False    
-      self._len = sum(self._lengths)  
-
-    self.pdb = pdb
-    
-    if verbose:
-      print("lengths", self._lengths)
-      if "fix_pos" in self._inputs:
-        print("the following positions will be fixed:")
-        print(self._inputs["fix_pos"])
-
-  def get_af_inputs(self, af):
-    '''get inputs from alphafold model'''
-
-    self._lengths = af._lengths
-    self._len = af._len
-
-    self._inputs["residue_idx"] = af._inputs["residue_index"]
-    self._inputs["chain_idx"]   = af._inputs["asym_id"]
-    self._inputs["lengths"]     = np.array(self._lengths)
-
-    # set bias
-    L = sum(self._lengths)
-    self._inputs["bias"] = np.zeros((L,20))
-    self._inputs["bias"][-af._len:] = af._inputs["bias"]
-    
-    if "offset" in af._inputs:
-      self._inputs["offset"] = af._inputs["offset"]
-
-    if "batch" in af._inputs:
-      atom_idx = tuple(residue_constants.atom_order[k] for k in ["N","CA","C","O"])
-      batch = af._inputs["batch"]
-      self._inputs["X"]    = batch["all_atom_positions"][:,atom_idx]
-      self._inputs["mask"] = batch["all_atom_mask"][:,1]
-      self._inputs["S"]    = batch["aatype"]
-
-    # fix positions
-    if af.protocol == "binder":
-      p = np.arange(af._target_len)
-    else:
-      p = af.opt.get("fix_pos",None)
-    
-    if p is not None:
-      self._inputs["fix_pos"] = p
-      self._inputs["bias"][p] = 1e7 * np.eye(21)[self._inputs["S"]][p,:20]
-
-    # tie positions
-    if af._args["homooligomer"]:
-      assert min(self._lengths) == max(self._lengths)
-      self._tied_lengths = True
-    else:
-      self._tied_lengths = False
-
-  def sample(self, num=1, batch=1, temperature=0.1, rescore=False, **kwargs):
-    '''sample sequence'''
-    O = []
-    for _ in range(num):
-      O.append(self.sample_parallel(batch, temperature, rescore, **kwargs))
-    return jax.tree_map(lambda *x:np.concatenate(x,0),*O)    
-
-  def sample_parallel(self, batch=10, temperature=0.1, rescore=False, **kwargs):
-    '''sample new sequence(s) in parallel'''
-    I = copy_dict(self._inputs)
-    I.update(kwargs)
-    key = I.pop("key",self.key())
-    keys = jax.random.split(key,batch)
-    O = self._sample_parallel(keys, I, temperature, self._tied_lengths)
-    if rescore:
-      O = self._rescore_parallel(keys, I, O["S"], O["decoding_order"])
-    O = jax.tree_map(np.array, O)
-
-    # process outputs to human-readable form
-    O.update(self._get_seq(O))
-    O.update(self._get_score(I,O))
-    return O
-
-  def _get_seq(self, O):
-    ''' one_hot to amino acid sequence '''
-    def split_seq(seq):
-      if len(self._lengths) > 1:
-        seq = "".join(np.insert(list(seq),np.cumsum(self._lengths[:-1]),"/"))
-        if self._tied_lengths:
-          seq = seq.split("/")[0]
-      return seq
-    seqs, S = [], O["S"].argmax(-1)
-    if S.ndim == 1: S = [S]
-    for s in S:
-      seq = "".join([order_aa[a] for a in s])
-      seq = split_seq(seq)
-      seqs.append(seq)
-    
-    return {"seq": np.array(seqs)}
-
-  def _get_score(self, I, O):
-    ''' logits to score/sequence_recovery '''
-    mask = I["mask"].copy()
-    if "fix_pos" in I:
-      mask[I["fix_pos"]] = 0
-
-    log_q = log_softmax(O["logits"],-1)[...,:20]
-    q = softmax(O["logits"][...,:20],-1)
-    if "S" in O:
-      S = O["S"][...,:20]
-      score = -(S * log_q).sum(-1)
-      seqid = S.argmax(-1) == self._inputs["S"]
-    else:
-      score = -(q * log_q).sum(-1)
-      seqid = np.zeros_like(score)
-      
-    score = (score * mask).sum(-1) / (mask.sum() + 1e-8)
-    seqid = (seqid * mask).sum(-1) / (mask.sum() + 1e-8)
-
-    return {"score":score, "seqid":seqid}
-
-  def score(self, seq=None, **kwargs):
-    '''score sequence'''
-    I = copy_dict(self._inputs)
-    if seq is not None:
-      p = np.arange(I["S"].shape[0])
-      if self._tied_lengths and len(seq) == self._lengths[0]:
-        seq = seq * len(self._lengths)
-      if "fix_pos" in I and len(seq) == (I["S"].shape[0] - I["fix_pos"].shape[0]):
-        p = np.delete(p,I["fix_pos"])
-      I["S"][p] = np.array([aa_order.get(aa,-1) for aa in seq])
-    I.update(kwargs)
-    key = I.pop("key",self.key())
-    O = jax.tree_map(np.array, self._score(**I, key=key))
-    O.update(self._get_score(I,O))
-    return O
-
-  def get_logits(self, **kwargs):
-    '''get logits'''
-    return self.score(**kwargs)["logits"]
-
-  def get_unconditional_logits(self, **kwargs):
-    L = self._inputs["X"].shape[0]
-    kwargs["ar_mask"] = np.zeros((L,L))
-    return self.score(**kwargs)["logits"]
-
-  def set_seed(self, seed=None):
-    np.random.seed(seed=seed)
-    self.key = Key(seed=seed).get
-
-  def _setup(self):
-    def _score(X, mask, residue_idx, chain_idx, key, **kwargs):
-      I = {'X': X,
-           'mask': mask,
-           'residue_idx': residue_idx,
-           'chain_idx': chain_idx}
-      I.update(kwargs)
-
-      # define decoding order
-      if "decoding_order" not in I:
-        key, sub_key = jax.random.split(key)
-        randn = jax.random.uniform(sub_key, (I["X"].shape[0],))    
-        randn = jnp.where(I["mask"], randn, randn+1)
-        if "fix_pos" in I: randn = randn.at[I["fix_pos"]].add(-1)        
-        I["decoding_order"] = randn.argsort()
-
-      for k in ["S","bias"]:
-        if k in I: I[k] = _aa_convert(I[k])
-
-      O = self._model.score(self._model.params, key, I)
-      O["S"] = _aa_convert(O["S"], rev=True)
-      O["logits"] = _aa_convert(O["logits"], rev=True)
-      return O
-    
-    def _sample(X, mask, residue_idx, chain_idx, key,
-                temperature=0.1, tied_lengths=False, **kwargs):
-      I = {'X': X,
-           'mask': mask,
-           'residue_idx': residue_idx,
-           'chain_idx': chain_idx,
-           'temperature': temperature}
-      I.update(kwargs)
-
-      # define decoding order
-      if "decoding_order" in I:
-        if I["decoding_order"].ndim == 1:
-          I["decoding_order"] = I["decoding_order"][:,None]
-      else:
-        key, sub_key = jax.random.split(key)
-        randn = jax.random.uniform(sub_key, (I["X"].shape[0],))    
-        randn = jnp.where(I["mask"], randn, randn+1)
-        if "fix_pos" in I: randn = randn.at[I["fix_pos"]].add(-1)        
-        if tied_lengths:
-          copies = I["lengths"].shape[0]
-          decoding_order_tied = randn.reshape(copies,-1).mean(0).argsort()
-          I["decoding_order"] = jnp.arange(I["X"].shape[0]).reshape(copies,-1).T[decoding_order_tied]
-        else:
-          I["decoding_order"] = randn.argsort()[:,None]
-
-      for k in ["S","bias"]:
-        if k in I: I[k] = _aa_convert(I[k])
-      
-      O = self._model.sample(self._model.params, key, I)
-      O["S"] = _aa_convert(O["S"], rev=True)
-      O["logits"] = _aa_convert(O["logits"], rev=True)
-      return O
-
-    self._score = jax.jit(_score)
-    self._sample = jax.jit(_sample, static_argnames=["tied_lengths"])
-
-    def _sample_parallel(key, inputs, temperature, tied_lengths=False):
-      inputs.pop("temperature",None)
-      inputs.pop("key",None)
-      return _sample(**inputs, key=key, temperature=temperature, tied_lengths=tied_lengths)
-    fn = jax.vmap(_sample_parallel, in_axes=[0,None,None,None])
-    self._sample_parallel = jax.jit(fn, static_argnames=["tied_lengths"])
-
-    def _rescore_parallel(key, inputs, S, decoding_order):
-      inputs.pop("S",None)
-      inputs.pop("decoding_order",None)
-      inputs.pop("key",None)
-      return _score(**inputs, key=key, S=S, decoding_order=decoding_order)
-    fn = jax.vmap(_rescore_parallel, in_axes=[0,None,0,0])
-    self._rescore_parallel = jax.jit(fn)
-
-#######################################################################################
-
-def _aa_convert(x, rev=False):
-  mpnn_alphabet = 'ACDEFGHIKLMNPQRSTVWYX'
-  af_alphabet =   'ARNDCQEGHILKMFPSTWYVX'
-  if x is None:
-    return x
-  else:
-    if rev:
-      return x[...,tuple(mpnn_alphabet.index(k) for k in af_alphabet)]
-    else:
-      x = jax.nn.one_hot(x,21) if jnp.issubdtype(x.dtype, jnp.integer) else x
-      if x.shape[-1] == 20:
-        x = jnp.pad(x,[[0,0],[0,1]])
-      return x[...,tuple(af_alphabet.index(k) for k in mpnn_alphabet)]
\ No newline at end of file
diff --git a/colabdesign/mpnn/modules.py b/colabdesign/mpnn/modules.py
deleted file mode 100644
index 462246c..0000000
--- a/colabdesign/mpnn/modules.py
+++ /dev/null
@@ -1,332 +0,0 @@
-import functools
-import haiku as hk
-import jax
-import jax.numpy as jnp
-import numpy as np
-import joblib
-
-from colabdesign.shared.prng import SafeKey
-from .utils import cat_neighbors_nodes, get_ar_mask
-from .sample import mpnn_sample
-from .score import mpnn_score
-
-Gelu = functools.partial(jax.nn.gelu, approximate=False)
-
-class dropout_cust(hk.Module):
-  def __init__(self, rate) -> None:
-    super().__init__()
-    self.rate = rate
-    self.safe_key = SafeKey(hk.next_rng_key())
-  
-  def __call__(self, x):
-    self.safe_key, use_key = self.safe_key.split()
-    return hk.dropout(use_key.get(), self.rate, x)
-
-
-class EncLayer(hk.Module):
-  def __init__(self, num_hidden,
-         num_in, dropout=0.1,
-         num_heads=None, scale=30,
-         name=None):
-    super(EncLayer, self).__init__()
-    self.num_hidden = num_hidden
-    self.num_in = num_in
-    self.scale = scale
-
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-    self.dropout1 = dropout_cust(dropout)
-    self.dropout2 = dropout_cust(dropout)
-    self.dropout3 = dropout_cust(dropout)
-    self.norm1 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm1')
-    self.norm2 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm2')
-    self.norm3 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm3')
-
-    self.W1 = hk.Linear(num_hidden, with_bias=True, name=name + '_W1')
-    self.W2 = hk.Linear(num_hidden, with_bias=True, name=name + '_W2')
-    self.W3 = hk.Linear(num_hidden, with_bias=True, name=name + '_W3')
-    self.W11 = hk.Linear(num_hidden, with_bias=True, name=name + '_W11')
-    self.W12 = hk.Linear(num_hidden, with_bias=True, name=name + '_W12')
-    self.W13 = hk.Linear(num_hidden, with_bias=True, name=name + '_W13')
-    self.act = Gelu
-    self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4,
-                       name=name + '_dense')
-
-  def __call__(self, h_V, h_E, E_idx,
-               mask_V=None, mask_attend=None):
-    """ Parallel computation of full transformer layer """
-
-    h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, h_EV.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_EV], -1)
-
-    h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))
-    if mask_attend is not None:
-      h_message = jnp.expand_dims(mask_attend, -1)* h_message
-    dh = jnp.sum(h_message, -2) / self.scale
-    h_V = self.norm1(h_V + self.dropout1(dh))
-
-    dh = self.dense(h_V)
-    h_V = self.norm2(h_V + self.dropout2(dh))
-    if mask_V is not None:
-      mask_V = jnp.expand_dims(mask_V, -1)
-      h_V = mask_V * h_V
-
-    h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, h_EV.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_EV], -1)
-
-    h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))
-    h_E = self.norm3(h_E + self.dropout3(h_message))
-    return h_V, h_E
-
-class DecLayer(hk.Module):
-  def __init__(self, num_hidden, num_in,
-         dropout=0.1, num_heads=None,
-         scale=30, name=None):
-    super(DecLayer, self).__init__()
-    self.num_hidden = num_hidden
-    self.num_in = num_in
-    self.scale = scale
-    self.dropout1 = dropout_cust(dropout)
-    self.dropout2 = dropout_cust(dropout)
-    self.norm1 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm1')
-    self.norm2 = hk.LayerNorm(-1, create_scale=True, create_offset=True,
-                  name=name + '_norm2')
-
-    self.W1 = hk.Linear(num_hidden, with_bias=True, name=name + '_W1')
-    self.W2 = hk.Linear(num_hidden, with_bias=True, name=name + '_W2')
-    self.W3 = hk.Linear(num_hidden, with_bias=True, name=name + '_W3')
-    self.act = Gelu
-    self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4,
-                       name=name + '_dense')
-
-  
-  def __call__(self, h_V, h_E,
-         mask_V=None, mask_attend=None):
-    """ Parallel computation of full transformer layer """
-
-    # Concatenate h_V_i to h_E_ij
-    h_V_expand = jnp.tile(jnp.expand_dims(h_V, -2),[1, h_E.shape[-2], 1])
-    h_EV = jnp.concatenate([h_V_expand, h_E], -1)
-
-    h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))
-    if mask_attend is not None:
-      h_message = jnp.expand_dims(mask_attend, -1) * h_message
-    dh = jnp.sum(h_message, -2) / self.scale
-
-    h_V = self.norm1(h_V + self.dropout1(dh))
-
-    # Position-wise feedforward
-    dh = self.dense(h_V)
-    h_V = self.norm2(h_V + self.dropout2(dh))
-
-    if mask_V is not None:
-      mask_V = jnp.expand_dims(mask_V, -1)
-      h_V = mask_V * h_V
-    return h_V 
-
-class PositionWiseFeedForward(hk.Module):
-  def __init__(self, num_hidden, num_ff, name=None):
-    super(PositionWiseFeedForward, self).__init__()
-    self.W_in = hk.Linear(num_ff, with_bias=True, name=name + '_W_in')
-    self.W_out = hk.Linear(num_hidden, with_bias=True, name=name + '_W_out')
-    self.act = Gelu
-  def __call__(self, h_V):
-    h = self.act(self.W_in(h_V), approximate=False)
-    h = self.W_out(h)
-    return h
-
-class PositionalEncodings(hk.Module):
-  def __init__(self, num_embeddings, max_relative_feature=32):
-    super(PositionalEncodings, self).__init__()
-    self.num_embeddings = num_embeddings
-    self.max_relative_feature = max_relative_feature
-    self.linear = hk.Linear(num_embeddings, name='embedding_linear')
-
-  def __call__(self, offset, mask):
-    d = jnp.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature) * mask + \
-      (1 - mask) * (2*self.max_relative_feature + 1)
-    d_onehot = jax.nn.one_hot(d, 2*self.max_relative_feature + 1 + 1)
-    E = self.linear(d_onehot)
-    return E
-
-class RunModel:
-  def __init__(self, config) -> None:
-    self.config = config
-
-    def _forward_score(inputs):
-      model = ProteinMPNN(**self.config)
-      return model.score(inputs)
-    self.score = hk.transform(_forward_score).apply
-
-    def _forward_sample(inputs):
-      model = ProteinMPNN(**self.config)
-      return model.sample(inputs)
-    self.sample = hk.transform(_forward_sample).apply
-
-  def load_params(self, path):
-    self.params = joblib.load(path)
-
-class ProteinFeatures(hk.Module):
-  def __init__(self, edge_features, node_features,
-         num_positional_embeddings=16,
-         num_rbf=16, top_k=30,
-         augment_eps=0., num_chain_embeddings=16):
-
-    """ Extract protein features """
-    super(ProteinFeatures, self).__init__()
-    self.edge_features = edge_features
-    self.node_features = node_features
-    self.top_k = top_k
-    self.augment_eps = augment_eps 
-    self.num_rbf = num_rbf
-    self.num_positional_embeddings = num_positional_embeddings
-
-    self.embeddings = PositionalEncodings(num_positional_embeddings)
-    node_in, edge_in = 6, num_positional_embeddings + num_rbf*25
-    self.edge_embedding = hk.Linear(edge_features, with_bias=False, name='edge_embedding')
-    self.norm_edges = hk.LayerNorm(-1, create_scale=True, create_offset=True, name='norm_edges')
-
-    self.safe_key = SafeKey(hk.next_rng_key())
-
-  def _get_edge_idx(self, X, mask, eps=1E-6):
-    ''' get edge index
-    input: mask.shape = (...,L), X.shape = (...,L,3)
-    return: (...,L,k)
-    '''
-    mask_2D = mask[...,None,:] * mask[...,:,None] 
-    dX = X[...,None,:,:] - X[...,:,None,:]
-    D = jnp.sqrt(jnp.square(dX).sum(-1) + eps)
-    D_masked = jnp.where(mask_2D,D,D.max(-1,keepdims=True))
-    k = min(self.top_k, X.shape[-2])
-    return jax.lax.approx_min_k(D_masked, k, reduction_dimension=-1)[1]
-
-  def _rbf(self, D):
-    ''' radial basis function (RBF)
-    input: (...,L,k)
-    output: (...,L,k,?)
-    '''
-    D_min, D_max, D_count = 2., 22., self.num_rbf
-    D_mu = jnp.linspace(D_min, D_max, D_count)
-    D_sigma = (D_max - D_min) / D_count    
-    return jnp.exp(-((D[...,None] - D_mu) / D_sigma)**2)
-
-  def _get_rbf(self, A, B, E_idx):
-    D = jnp.sqrt(jnp.square(A[...,:,None,:] - B[...,None,:,:]).sum(-1) + 1e-6)
-    D_neighbors = jnp.take_along_axis(D, E_idx, 1)
-    return self._rbf(D_neighbors)
-
-  def __call__(self, I):
-    if self.augment_eps > 0:
-      self.safe_key, use_key = self.safe_key.split()
-      X = I["X"] + self.augment_eps * jax.random.normal(use_key.get(), I["X"].shape)
-    else:
-      X = I["X"]
-    
-    ##########################
-    # get atoms
-    ##########################
-    # N,Ca,C,O,Cb
-    Y = X.swapaxes(0,1) #(length, atoms, 3) -> (atoms, length, 3)
-    if Y.shape[0] == 4:
-      # add Cb
-      b,c = (Y[1]-Y[0]),(Y[2]-Y[1])
-      Cb = -0.58273431*jnp.cross(b,c) + 0.56802827*b - 0.54067466*c + Y[1]
-      Y = jnp.concatenate([Y,Cb[None]],0)
-
-    ##########################
-    # gather edge features
-    ##########################
-    # get edge indices (based on ca-ca distances)
-    E_idx = self._get_edge_idx(Y[1], I["mask"])
-
-    # rbf encode distances between atoms
-    edges = jnp.array([[1,1],[0,0],[2,2],[3,3],[4,4],
-                       [1,0],[1,2],[1,3],[1,4],[0,2],
-                       [0,3],[0,4],[4,2],[4,3],[3,2],
-                       [0,1],[2,1],[3,1],[4,1],[2,0],
-                       [3,0],[4,0],[2,4],[3,4],[2,3]])
-    RBF_all = jax.vmap(lambda x:self._get_rbf(Y[x[0]],Y[x[1]],E_idx))(edges)
-    RBF_all = RBF_all.transpose((1,2,0,3))
-    RBF_all = RBF_all.reshape(RBF_all.shape[:-2]+(-1,))
-
-    ##########################
-    # position embedding
-    ##########################
-    # residue index offset
-    if "offset" not in I:
-      I["offset"] = I["residue_idx"][:,None] - I["residue_idx"][None,:]
-    offset = jnp.take_along_axis(I["offset"], E_idx, 1)
-
-    # chain index offset
-    E_chains = (I["chain_idx"][:,None] == I["chain_idx"][None,:]).astype(int)
-    E_chains = jnp.take_along_axis(E_chains, E_idx, 1)
-    E_positional = self.embeddings(offset, E_chains)
-
-    ##########################
-    # define edges
-    ##########################
-    E = jnp.concatenate((E_positional, RBF_all), -1)
-    E = self.edge_embedding(E)
-    E = self.norm_edges(E)
-    return E, E_idx
-
-class EmbedToken(hk.Module):
-  def __init__(self, vocab_size, embed_dim):
-    super().__init__()
-    self.vocab_size = vocab_size
-    self.embed_dim = embed_dim
-    self.w_init = hk.initializers.TruncatedNormal()
-
-  @property
-  def embeddings(self):
-    return hk.get_parameter("W_s",
-                [self.vocab_size, self.embed_dim],
-                init=self.w_init)
-
-  def __call__(self, arr):
-    if jnp.issubdtype(arr.dtype, jnp.integer):
-      one_hot = jax.nn.one_hot(arr, self.vocab_size)
-    else:
-      one_hot = arr
-    return jnp.tensordot(one_hot, self.embeddings, 1)
-
-class ProteinMPNN(hk.Module, mpnn_sample, mpnn_score):
-  def __init__(self, num_letters,
-         node_features, edge_features, hidden_dim,
-         num_encoder_layers=3, num_decoder_layers=3,
-         vocab=21, k_neighbors=64,
-         augment_eps=0.05, dropout=0.1):
-    super(ProteinMPNN, self).__init__()
-
-    # Hyperparameters
-    self.node_features = node_features
-    self.edge_features = edge_features
-    self.hidden_dim = hidden_dim
-
-    # Featurization layers
-    self.features = ProteinFeatures(edge_features,
-                    node_features,
-                    top_k=k_neighbors,
-                    augment_eps=augment_eps)
-
-    self.W_e = hk.Linear(hidden_dim, with_bias=True, name='W_e')
-    self.W_s = EmbedToken(vocab_size=vocab, embed_dim=hidden_dim)
-
-    # Encoder layers
-    self.encoder_layers = [
-      EncLayer(hidden_dim, hidden_dim*2, dropout=dropout, name='enc' + str(i))
-      for i in range(num_encoder_layers)
-    ]
-
-    # Decoder layers
-    self.decoder_layers = [
-      DecLayer(hidden_dim, hidden_dim*3, dropout=dropout, name='dec' + str(i))
-      for i in range(num_decoder_layers)
-    ]
-    self.W_out = hk.Linear(num_letters, with_bias=True, name='W_out')
\ No newline at end of file
diff --git a/colabdesign/mpnn/sample.py b/colabdesign/mpnn/sample.py
deleted file mode 100644
index 401e188..0000000
--- a/colabdesign/mpnn/sample.py
+++ /dev/null
@@ -1,107 +0,0 @@
-import jax
-import jax.numpy as jnp
-import haiku as hk
-import numpy as np
-
-from .utils import cat_neighbors_nodes, get_ar_mask
-
-class mpnn_sample:
-  def sample(self, I):
-    """
-    I = {
-         [[required]]
-         'X' = (L,4,3) 
-         'mask' = (L,)
-         'residue_index' = (L,)
-         'chain_idx' = (L,)
-         'decoding_order' = (L,)
-         
-         [[optional]]
-         'ar_mask' = (L,L)
-         'bias' = (L,21)
-         'temperature' = 1.0
-        }
-    """
-
-    key = hk.next_rng_key()
-    L = I["X"].shape[0]
-    temperature = I.get("temperature",1.0)
-
-    # prepare node and edge embeddings
-    E, E_idx = self.features(I)
-    h_V = jnp.zeros((E.shape[0], E.shape[-1]))
-    h_E = self.W_e(E)
-
-    ##############
-    # encoder
-    ##############
-    mask_attend = jnp.take_along_axis(I["mask"][:,None] * I["mask"][None,:], E_idx, 1)
-    for layer in self.encoder_layers:
-      h_V, h_E = layer(h_V, h_E, E_idx, I["mask"], mask_attend)
-
-    # get autoregressive mask  
-    ar_mask = I.get("ar_mask",get_ar_mask(I["decoding_order"]))
-    
-    mask_attend = jnp.take_along_axis(ar_mask, E_idx, 1)
-    mask_1D = I["mask"][:,None]
-    mask_bw = mask_1D * mask_attend
-    mask_fw = mask_1D * (1 - mask_attend)
-    
-    h_EX_encoder = cat_neighbors_nodes(jnp.zeros_like(h_V), h_E, E_idx)
-    h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)
-    h_EXV_encoder = mask_fw[...,None] * h_EXV_encoder
-
-    def fwd(x, t, key):
-      h_EXV_encoder_t = h_EXV_encoder[t] 
-      E_idx_t         = E_idx[t]
-      mask_t          = I["mask"][t]
-      mask_bw_t       = mask_bw[t]      
-      h_ES_t          = cat_neighbors_nodes(x["h_S"], h_E[t], E_idx_t)
-
-      ##############
-      # decoder
-      ##############
-      for l,layer in enumerate(self.decoder_layers):
-        h_V = x["h_V"][l]
-        h_ESV_decoder_t = cat_neighbors_nodes(h_V, h_ES_t, E_idx_t)
-        h_ESV_t = mask_bw_t[...,None] * h_ESV_decoder_t + h_EXV_encoder_t
-        h_V_t = layer(h_V[t], h_ESV_t, mask_V=mask_t)
-        # update
-        x["h_V"] = x["h_V"].at[l+1,t].set(h_V_t)
-
-      logits_t = self.W_out(h_V_t)
-      x["logits"] = x["logits"].at[t].set(logits_t)
-
-      ##############
-      # sample
-      ##############
-      
-      # add bias
-      if "bias" in I: logits_t += I["bias"][t]          
-
-      # sample character
-      logits_t = logits_t/temperature + jax.random.gumbel(key, logits_t.shape)
-
-      # tie positions
-      logits_t = logits_t.mean(0, keepdims=True)
-
-      S_t = jax.nn.one_hot(logits_t[...,:20].argmax(-1), 21)
-
-      # update
-      x["h_S"] = x["h_S"].at[t].set(self.W_s(S_t))
-      x["S"]   = x["S"].at[t].set(S_t)
-      return x, None
-    
-    # initial values
-    X = {"h_S":    jnp.zeros_like(h_V),
-         "h_V":    jnp.array([h_V] + [jnp.zeros_like(h_V)] * len(self.decoder_layers)),
-         "S":      jnp.zeros((L,21)),
-         "logits": jnp.zeros((L,21))}
-
-    # scan over decoding order
-    t = I["decoding_order"]
-    if t.ndim == 1: t = t[:,None]
-    XS = {"t":t, "key":jax.random.split(key,t.shape[0])}
-    X = hk.scan(lambda x, xs: fwd(x, xs["t"], xs["key"]), X, XS)[0]
-    
-    return {"S":X["S"], "logits":X["logits"], "decoding_order":t}
\ No newline at end of file
diff --git a/colabdesign/mpnn/score.py b/colabdesign/mpnn/score.py
deleted file mode 100644
index 575d389..0000000
--- a/colabdesign/mpnn/score.py
+++ /dev/null
@@ -1,80 +0,0 @@
-import jax
-import jax.numpy as jnp
-import haiku as hk
-import numpy as np
-
-from .utils import cat_neighbors_nodes, get_ar_mask
-
-class mpnn_score:
-  def score(self, I):
-    """
-    I = {
-         [[required]]
-         'X' = (L,4,3) 
-         'mask' = (L,)
-         'residue_index' = (L,)
-         'chain_idx' = (L,)
-         
-         [[optional]]
-         'S' = (L,21)
-         'decoding_order' = (L,)
-         'ar_mask' = (L,L)
-        }
-    """
-    
-    key = hk.next_rng_key()
-    # Prepare node and edge embeddings
-    E, E_idx = self.features(I)
-    h_V = jnp.zeros((E.shape[0], E.shape[-1]))
-    h_E = self.W_e(E)
-    
-    # Encoder is unmasked self-attention
-    mask_attend = jnp.take_along_axis(I["mask"][:,None] * I["mask"][None,:], E_idx, 1)
-    
-    for layer in self.encoder_layers:
-      h_V, h_E = layer(h_V, h_E, E_idx, I["mask"], mask_attend)
-
-    # Build encoder embeddings
-    h_EX_encoder = cat_neighbors_nodes(jnp.zeros_like(h_V), h_E, E_idx)
-    h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)
-
-    if "S" not in I:
-      ##########################################
-      # unconditional_probs
-      ##########################################      
-      h_EXV_encoder_fw = h_EXV_encoder
-      for layer in self.decoder_layers:
-        h_V = layer(h_V, h_EXV_encoder_fw, I["mask"])
-      decoding_order = None
-    else:
-      ##########################################
-      # conditional_probs
-      ##########################################
-
-      # Concatenate sequence embeddings for autoregressive decoder
-      h_S = self.W_s(I["S"])
-      h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)
-
-      # get autoregressive mask
-      if "ar_mask" in I:
-        decoding_order = None
-        ar_mask = I["ar_mask"]
-      else:
-        decoding_order = I["decoding_order"]
-        ar_mask = get_ar_mask(decoding_order)
-              
-      mask_attend = jnp.take_along_axis(ar_mask, E_idx, 1)
-      mask_1D = I["mask"][:,None]
-      mask_bw = mask_1D * mask_attend
-      mask_fw = mask_1D * (1 - mask_attend)
-
-      h_EXV_encoder_fw = mask_fw[...,None] * h_EXV_encoder
-      for layer in self.decoder_layers:
-        # Masked positions attend to encoder information, unmasked see. 
-        h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)
-        h_ESV = mask_bw[...,None] * h_ESV + h_EXV_encoder_fw
-        h_V = layer(h_V, h_ESV, I["mask"])
-    
-    logits = self.W_out(h_V)
-    S = I.get("S",None)
-    return {"logits": logits, "decoding_order":decoding_order, "S":S}
\ No newline at end of file
diff --git a/colabdesign/mpnn/utils.py b/colabdesign/mpnn/utils.py
deleted file mode 100644
index 2c49296..0000000
--- a/colabdesign/mpnn/utils.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import jax.numpy as jnp
-import jax
-
-def gather_nodes(nodes, neighbor_idx):
-  # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]
-  # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]
-  neighbors_flat = neighbor_idx.reshape([neighbor_idx[None].shape[0], -1])
-  neighbors_flat = jnp.tile(jnp.expand_dims(neighbors_flat, -1),[1, 1, nodes[None].shape[2]])
-  # Gather and re-pack
-  neighbor_features = jnp.take_along_axis(nodes[None], neighbors_flat, 1)
-  neighbor_features = neighbor_features.reshape(list(neighbor_idx[None].shape[:3]) + [-1])
-  return neighbor_features[0]
-
-def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):
-  h_nodes = gather_nodes(h_nodes, E_idx)[None]
-  h_nn = jnp.concatenate([h_neighbors[None], h_nodes], -1)
-  return h_nn[0]
-
-def get_ar_mask(order):
-  '''compute autoregressive mask, given order of positions'''
-  order = order.flatten()
-  L = order.shape[-1]
-  tri = jnp.tri(L, k=-1)
-  idx = order.argsort()
-  ar_mask = tri[idx,:][:,idx]
-  return ar_mask
\ No newline at end of file
diff --git a/colabdesign/mpnn/weights/__init__.py b/colabdesign/mpnn/weights/__init__.py
deleted file mode 100644
index 8b13789..0000000
--- a/colabdesign/mpnn/weights/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/colabdesign/mpnn/weights_soluble/__init__.py b/colabdesign/mpnn/weights_soluble/__init__.py
deleted file mode 100644
index 8b13789..0000000
--- a/colabdesign/mpnn/weights_soluble/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/colabdesign/rf/designability_test.py b/colabdesign/rf/designability_test.py
index a33e6b3..c0c2721 100644
--- a/colabdesign/rf/designability_test.py
+++ b/colabdesign/rf/designability_test.py
@@ -1,9 +1,8 @@
 import os,sys
-
-from colabdesign.mpnn import mk_mpnn_model
-from colabdesign.af import mk_af_model
-from colabdesign.shared.protein import pdb_to_string
-from colabdesign.shared.parse_args import parse_args
+import jax
+import glob
+from colabdesign.af.model import mk_af_model
+from colabdesign.shared.model import parse_args
 
 import pandas as pd
 import numpy as np
@@ -36,11 +35,13 @@ def main(argv):
   ag.add(["pdb="          ],  None,   str, ["input pdb"])
   ag.add(["loc="          ],  None,   str, ["location to save results"])
   ag.add(["contigs="      ],  None,   str, ["contig definition"])
+  ag.add(["fasta="], None, str, ["input FASTA file"])
+  ag.add(["final_output="], "./final_output", str, ["Final output directory"])
   ag.txt("-------------------------------------------------------------------------------------")
   ag.txt("OPTIONAL")
   ag.txt("-------------------------------------------------------------------------------------")
   ag.add(["copies="       ],         1,    int, ["number of repeating copies"])
-  ag.add(["num_seqs="     ],         8,    int, ["number of mpnn designs to evaluate"])
+  ag.add(["num_seqs="     ],         1,    int, ["number of mpnn designs to evaluate"])
   ag.add(["initial_guess" ],     False,   None, ["initialize previous coordinates"])
   ag.add(["use_multimer"  ],     False,   None, ["use alphafold_multimer_v3"])
   ag.add(["use_soluble"   ],     False,   None, ["use solubleMPNN"])
@@ -96,7 +97,7 @@ def main(argv):
                   "binder_chain":",".join(binder_chains),
                   "rm_aa":o.rm_aa}
     opt_extra = {}
-  
+
   elif sum(fixed_pos) > 0:
     protocol = "partial"
     print("protocol=partial")
@@ -120,12 +121,9 @@ def main(argv):
                   "rm_aa":o.rm_aa}
 
   batch_size = 8
-  if o.num_seqs < batch_size:    
+  if o.num_seqs < batch_size:
     batch_size = o.num_seqs
-  
-  print("running proteinMPNN...")
-  sampling_temp = o.mpnn_sampling_temp
-  mpnn_model = mk_mpnn_model(weights="soluble" if o.use_soluble else "original")
+
   outs = []
   pdbs = []
   for m in range(o.num_designs):
@@ -134,13 +132,6 @@ def main(argv):
     else:
       pdb_filename = o.pdb.replace("_0.pdb",f"_{m}.pdb")
     pdbs.append(pdb_filename)
-    af_model.prep_inputs(pdb_filename, **prep_flags)
-    if protocol == "partial":
-      p = np.where(fixed_pos)[0]
-      af_model.opt["fix_pos"] = p[p < af_model._len]
-
-    mpnn_model.get_af_inputs(af_model)
-    outs.append(mpnn_model.sample(num=o.num_seqs//batch_size, batch=batch_size, temperature=sampling_temp))
 
   if protocol == "binder":
     af_terms = ["plddt","i_ptm","i_pae","rmsd"]
@@ -153,6 +144,31 @@ def main(argv):
   data = []
   best = {"rmsd":np.inf,"design":0,"n":0}
   print("running AlphaFold...")
+  #jax.config.update("jax_disable_jit", True)
+  sequences = []
+  fasta_path = o.fasta
+
+  if os.path.exists(fasta_path) and os.path.isdir(fasta_path):
+    print(f"Provided path '{fasta_path}' is a directory. Searching for FASTA files...")
+
+    # Find all .fasta files in the directory
+    fasta_files = glob.glob(os.path.join(fasta_path, "*.fasta")) + glob.glob(os.path.join(fasta_path, "*.fa"))
+
+    if not fasta_files:
+      print(f"Error: No FASTA files found in {fasta_path}")
+      return  # Exit if no files are found
+
+    # Automatically pick the first FASTA file found
+    fasta_path = fasta_files[0]
+    print(f"Using FASTA file: {fasta_path}")
+
+  print('fasta_path', fasta_path)
+  with open(fasta_path, "r") as fasta:
+    for line in fasta:
+      line = line.strip()
+      if not line.startswith(">"):
+        sequences.append(line)
+  outs = [{'seq': np.array(sequences)}]
   os.system(f"mkdir -p {o.loc}/all_pdb")
   with open(f"{o.loc}/design.fasta","w") as fasta:
     for m,(out,pdb_filename) in enumerate(zip(outs,pdbs)):
@@ -164,36 +180,9 @@ def main(argv):
         out["design"].append(m)
         out["n"].append(n)
         sub_seq = out["seq"][n].replace("/","")[-af_model._len:]
+        print('desinability_test: af_model_predict_before')
         af_model.predict(seq=sub_seq, num_recycles=o.num_recycles, verbose=False)
-        for t in af_terms: out[t].append(af_model.aux["log"][t])
-        if "i_pae" in out:
-          out["i_pae"][-1] = out["i_pae"][-1] * 31
-        if "pae" in out:
-          out["pae"][-1] = out["pae"][-1] * 31
-        rmsd = out["rmsd"][-1]
-        if rmsd < best["rmsd"]:
-          best = {"design":m,"n":n,"rmsd":rmsd}
-        af_model.save_current_pdb(f"{o.loc}/all_pdb/design{m}_n{n}.pdb")
-        af_model._save_results(save_best=True, verbose=False)
-        af_model._k += 1
-        score_line = [f'design:{m} n:{n}',f'mpnn:{out["score"][n]:.3f}']
-        for t in af_terms:
-          score_line.append(f'{t}:{out[t][n]:.3f}')
-        print(" ".join(score_line)+" "+out["seq"][n])
-        line = f'>{"|".join(score_line)}\n{out["seq"][n]}'
-        fasta.write(line+"\n")
-      data += [[out[k][n] for k in labels] for n in range(o.num_seqs)]
-      af_model.save_pdb(f"{o.loc}/best_design{m}.pdb")
-
-  # save best
-  with open(f"{o.loc}/best.pdb", "w") as handle:
-    remark_text = f"design {best['design']} N {best['n']} RMSD {best['rmsd']:.3f}"
-    handle.write(f"REMARK 001 {remark_text}\n")
-    handle.write(open(f"{o.loc}/best_design{best['design']}.pdb", "r").read())
-    
-  labels[2] = "mpnn"
-  df = pd.DataFrame(data, columns=labels)
-  df.to_csv(f'{o.loc}/mpnn_results.csv')
+        print('desinability_test: af_model_predict_after')
 
 if __name__ == "__main__":
    main(sys.argv[1:])
diff --git a/colabdesign/seq/__init__.py b/colabdesign/seq/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/colabdesign/seq/kmeans.py b/colabdesign/seq/kmeans.py
deleted file mode 100644
index 03b7c49..0000000
--- a/colabdesign/seq/kmeans.py
+++ /dev/null
@@ -1,133 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy
-from math import log
-
-def _kmeans(X, X_weight, n_clusters=8, n_init=10, max_iter=300, tol=1e-4, seed=0):
-  '''kmeans implemented in jax'''
-  
-  def _dist(a,b):
-    sm = a @ b.T
-
-    a_norm = jnp.square(a).sum(-1)
-    b_norm = jnp.square(b).sum(-1)
-    
-    return jnp.abs(a_norm[:,None] + b_norm[None,:] - 2 * sm)
-
-  def _kmeans_plus_plus(key, X, X_weight, n_clusters):
-    '''kmeans++ implemented in jax, for initialization'''
-    n_samples, n_features = X.shape
-    n_candidates = 2 + int(log(n_clusters))
-    
-    def loop(m,c):
-      n,k = c
-      
-      inf_mask = jnp.inf * (jnp.arange(n_clusters) > n)
-      p = (inf_mask + _dist(X,m)).min(-1)
-
-      # sample candidates
-      candidates = jax.random.choice(k, jnp.arange(n_samples),
-                                     shape=(n_candidates,),
-                                     p=p/p.sum(), replace=False)
-      
-      # pick sample that decreases inertia the most
-      dist = jnp.minimum(p[:,None],_dist(X,X[candidates]))
-      i = candidates[(X_weight[:,None] * dist).sum(0).argmin()]
-      return m.at[n].set(X[i]), None
-
-    i = jax.random.choice(key,jnp.arange(n_samples))
-    init_means = jnp.zeros((n_clusters,n_features)).at[0].set(X[i])
-    carry = (jnp.arange(1,n_clusters), jax.random.split(key, n_clusters-1))
-    return jax.lax.scan(loop, init_means, carry)[0]
-
-  def _E(means):
-    # get labels
-    return _dist(X,means).argmin(-1)
-
-  def _M(labels):
-    # get means
-    labels = jax.nn.one_hot(labels, n_clusters)
-    labels = labels * X_weight[:,None]
-    labels /= labels.sum(0) + 1e-8
-    return labels.T @ X
-  
-  def _inertia(means):
-    # compute score: sum(min(dist(X,means)))
-    sco = _dist(X,means).min(-1)
-    return (X_weight * sco).sum()
-
-  def single_run(key):
-    # initialize
-    init_means = _kmeans_plus_plus(key, X, X_weight, n_clusters)
-
-    # run EM
-    if tol == 0:
-      means = jax.lax.scan(lambda mu,_:(_M(_E(mu)),None), init_means,
-                           None, length=max_iter)[0]
-    else:
-      def EM(x):
-        old_mu, old_sco, _, n = x
-        new_mu = _M(_E(old_mu))
-        new_sco = _inertia(new_mu)
-        return new_mu, new_sco, old_sco, n+1
-      def check(x):
-        _, new_sco, old_sco, n = x
-        return ((old_sco-new_sco) > tol) & (n < max_iter)
-      init = EM((init_means,jnp.inf,None,0))
-      means = jax.lax.while_loop(check, EM, init)[0]
-
-    return {"labels":_E(means),
-            "means":means,
-            "inertia":_inertia(means)}
-
-  # mulitple runs
-  key = jax.random.PRNGKey(seed)
-  if n_init > 0:
-    out = jax.vmap(single_run)(jax.random.split(key,n_init))
-    i = out["inertia"].argmin()
-    out = jax.tree_map(lambda x:x[i],out)
-  else:
-    out = single_run(key)
-
-  labels = jax.nn.one_hot(out["labels"],n_clusters)
-  cat = (labels * X_weight[:,None]).sum(0) / X_weight.sum()
-  return {**out, "cat":cat}
-
-def kmeans(x, x_weights, k, seed=0, max_iter=300):
-  N,L,A = x.shape
-  if k == 1:
-    kms = {"means":(x*x_weights[:,None,None]).sum(0,keepdims=True)/x_weights.sum(),
-           "labels":jnp.zeros(N,dtype=int),
-           "cat":jnp.ones((1,))}
-  else:
-    kms = _kmeans(x.reshape(N,-1), x_weights, n_clusters=k, max_iter=max_iter, seed=seed)
-    kms["means"] = kms["means"].reshape(k,L,A)
-  return kms
-
-def kmeans_sample(msa, msa_weights, k=1, samples=None, seed=0):
-
-  assert k > 0
-  
-  # run kmeans
-  kms = kmeans(jnp.asarray(msa), jnp.asarray(msa_weights), k=k, seed=seed)
-
-  # sample sequences from kmeans
-  key = jax.random.PRNGKey(seed)
-  N,L,A = msa.shape
-  if samples is None:
-    # if number of samples is undefined, set to size of input MSA
-    samples = N
-    sampled_labels = kms["labels"]
-  else:
-    # sample labels
-    key, key_ = jax.random.split(key)
-    sampled_labels = jnp.sort(jax.random.choice(key_,jnp.arange(k),shape=(samples,),p=kms["cat"]))
-
-  # sample MSA
-  sampled_msa = kms["means"][sampled_labels]
-  sampled_msa = (sampled_msa.cumsum(-1) >= jax.random.uniform(key, shape=(samples,L,1))).argmax(-1)
-  o = {"kms":kms,
-       "sampled_labels":sampled_labels,
-       "sampled_msa":sampled_msa}
-  
-  return jax.tree_map(lambda x:np.asarray(x),o)
\ No newline at end of file
diff --git a/colabdesign/seq/learn_msa.py b/colabdesign/seq/learn_msa.py
deleted file mode 100644
index 380383b..0000000
--- a/colabdesign/seq/learn_msa.py
+++ /dev/null
@@ -1,89 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy
-
-from colabdesign.seq.kmeans import kmeans
-from colabdesign.seq.stats import get_stats, get_eff
-
-# LEARN SEQUENCES
-# "parameter-free" model, where we learn msa to match the statistics.
-# We can take kmeans to the "next" level and directly optimize sequences to match desired stats.
-
-class LEARN_MSA:
-  def __init__(self, X, X_weight=None, samples=None,               
-               mode="tied", k=1,
-               seed=0, learning_rate=1e-3):
-    
-    assert mode in ["tied","full"]
-    assert k > 0
-
-    key = jax.random.PRNGKey(seed)
-    self.k = k
-
-    # collect X stats 
-    N,L,A = X.shape
-    if samples is None: samples = N
-    X = jnp.asarray(X)
-    X_weight = get_eff(X) if X_weight is None else jnp.asarray(X_weight)
-
-    # run kmeans
-    self.kms = kmeans(X, X_weight, k=self.k)
-    stats_args = dict(add_f_ij=True, add_mf_ij=(mode=="full"), add_c=True)
-    self.X_stats = get_stats(X, X_weight, labels=jax.nn.one_hot(self.kms["labels"],self.k), **stats_args)
-
-    if samples == N:
-      self.Y_labels = self.kms["labels"]
-    else:
-      # sample labels
-      key,key_ = jax.random.split(key)
-      self.Y_labels = jnp.sort(jax.random.choice(key_, jnp.arange(k), shape=(samples,), p=self.kms["cat"]))
-
-    key, key_ = jax.random.split(key)
-    Neff = X_weight.sum()
-    Y_logits = jnp.log(self.kms["means"] * Neff + 0.01 * jnp.log(Neff))[self.Y_labels]
-    Y = jax.nn.softmax(Y_logits + jax.random.gumbel(key_,(samples,L,A)))
-
-    # setup the model
-    def model(params, X_stats):
-      # categorical reparameterization of Y
-      Y_hard = jax.nn.one_hot(params["Y"].argmax(-1),A)
-      Y = jax.lax.stop_gradient(Y_hard - params["Y"]) + params["Y"]
-
-      # collect Y stats
-      Y_stats = get_stats(Y, labels=jax.nn.one_hot(self.Y_labels, self.k), **stats_args)
-      
-      # define loss function
-      i,ij = ("f_i","c_ij") if k == 1 else ("mf_i",("c_ij" if mode == "tied" else "mc_ij"))
-      loss_i = jnp.square(X_stats[i] - Y_stats[i]).sum((-1,-2))
-      loss_ij = jnp.square(X_stats[ij] - Y_stats[ij]).sum((-1,-2,-3)).mean(-1)
-      
-      if self.k > 1:
-        loss_i = (loss_i * self.kms["cat"]).sum()
-        if mode == "full":
-          loss_ij = (loss_ij * self.kms["cat"]).sum()
-      
-      loss = loss_i + loss_ij
-
-      aux = {"r":get_r(X_stats["c_ij"], Y_stats["c_ij"])}
-      return loss, aux
-
-    # setup optimizer
-    self.n = 0
-    init_fun, self.update_fun, self.get_params = adam(learning_rate)
-    self.state = init_fun({"Y":Y})
-    self.grad = jax.jit(jax.value_and_grad(model, has_aux=True))
-
-  def get_msa(self):
-    Y = np.array(self.get_params(self.state)["Y"])
-    return {"kms":self.kms,
-            "sampled_msa":Y.argmax(-1),
-            "sampled_labels":self.Y_labels}
-      
-  def fit(self, steps=100, verbose=True):
-    '''train model'''
-    for n in range(steps):
-      (loss, aux), grad = self.grad(self.get_params(self.state), self.X_stats)
-      self.state = self.update_fun(self.n, grad, self.state)
-      self.n += 1
-      if (n+1) % (steps // 10) == 0:
-        print(self.n, loss, aux["r"])
\ No newline at end of file
diff --git a/colabdesign/seq/mrf.py b/colabdesign/seq/mrf.py
deleted file mode 100644
index db61db1..0000000
--- a/colabdesign/seq/mrf.py
+++ /dev/null
@@ -1,314 +0,0 @@
-############################
-# TODO: remove reference to laxy, clean up the code
-############################
-
-def sample_msa(samples=10000, burn_in=1, temp=1.0,
-               order=None, ar=False, diff=False, seq=True):
-
-  def sample_cat(key, logits=None, probs=None):
-    if logits is not None:
-      hard = jax.nn.one_hot(jax.random.categorical(key,logits/temp),logits.shape[-1])
-      probs = jax.nn.softmax(logits,-1)
-    elif probs is not None:
-      hard = (probs.cumsum(-1) >= jax.random.uniform(key, shape=probs.shape[:-1])).argmax(-1)
-    if diff: hard = jax.lax.stop_gradient(hard - probs) + probs
-    return hard
-
-  def sample_pll(key,msa,par):
-    N,L,A = msa.shape    
-    if seq and ("w" in par or "mw" in par):
-      # sequential sampling
-      # burn_in = 1: autoregressive
-      # burn_in > 1: gibbs
-      def loop(m,x):
-        i,k = x
-        m_logits = []
-        if "w" in par: m_logits.append(jnp.einsum("njb,ajb->na", m, par["w"][i]))
-        if "b" in par: m_logits.append(par["b"][i])
-        if "mw" in par: m_logits.append(jnp.einsum("nc,njb,cajb->na", par["labels"], m, par["mw"][:,i]))
-        if "mb" in par: m_logits.append(jnp.einsum("nc,ca->na", par["labels"], par["mb"][:,i]))
-        return m.at[:,i].set(sample_cat(k,sum(m_logits))), None
-      # scan over positions
-      if order is not None: i = order
-      elif ar: i = jnp.arange(L)
-      else: i = jax.random.permutation(key,jnp.arange(L))
-      k = jax.random.split(key,L)
-      return jax.lax.scan(loop,msa,(i,k))[0]
-    else:
-      # sample all position independently
-      logits = []
-      if "b" in par: logits.append(par["b"])
-      if "mb" in par: logits.append(jnp.einsum("nc,cia->nia", par["labels"], par["mb"]))
-      if burn_in > 1:
-        if "w" in par: logits.append(jnp.einsum("njb,iajb->nia", msa, par["w"]))
-        if "mw" in par: logits.append(jnp.einsum("nc,njb,ciajb->nia", par["labels"], msa, par["mw"]))
-      return sample_cat(key,sum(logits))
-
-  def sample(key, params):
-    for p in ["b","w","mb","mw"]:
-      if p in params:
-        L,A = params[p].shape[-2:]
-        break
-    msa = jnp.zeros((samples,L,A))
-
-    # sample from mixture
-    if "c" in params and ("mb" in params or "mw" in params):
-      c = params["c"]
-      labels_logits = jnp.tile(c,(samples,1))
-      params["labels"] = jax.nn.one_hot(jax.random.categorical(key,labels_logits),c.shape[0])
-      if diff:
-        labels_soft = jax.nn.softmax(labels_logits,-1)
-        params["labels"] = jax.lax.stop_gradient(params["labels"] - labels_soft) + labels_soft
-    else:
-      params["labels"] = None
-
-    # number of iterations (burn-in)
-    sample_loop = lambda m,k:(sample_pll(k,m,params),None)
-    iters = jax.random.split(key,burn_in)
-    msa = jax.lax.scan(sample_loop,msa,iters)[0]
-    return {"msa":msa, "labels":params["labels"]}
-
-  return sample
-
-def reg_loss(params, lam):
-  reg_loss = []
-  if "b" in params:
-    reg_loss.append(lam * jnp.square(params["b"]).sum())
-  if "w" in params:
-    L,A = params["w"].shape[-2:]
-    reg_loss.append(lam/2*(L-1)*(A-1) * jnp.square(params["w"]).sum())
-  if "mb" in params:
-    reg_loss.append(lam * jnp.square(params["mb"]).sum())
-  if "mw" in params:
-    L,A = params["mw"].shape[-2:]
-    reg_loss.append(lam/2*(L-1)*(A-1) * jnp.square(params["mw"]).sum())
-  return sum(reg_loss)
-
-def pll_loss(params, inputs, order=None, labels=None):
-  logits = []
-
-  L = inputs["x"].shape[1]
-  w_mask = 1-jnp.eye(L)
-  if order is not None:
-    w_mask *= ar_mask(order)
-
-  if "b" in params:
-    logits.append(params["b"])
-  if "w" in params:
-    w = params["w"]
-    w = 0.5 * (w + w.transpose([2,3,0,1])) * w_mask[:,None,:,None]
-    logits.append(jnp.einsum("nia,iajb->njb", inputs["x"], w))
-
-  # MIXTURES
-  if "mb" in params:
-    logits.append(jnp.einsum("nc,cia->nia", labels, params["mb"]))
-
-  if "mw" in params:
-    mw = params["mw"]
-    mw = 0.5 * (mw + mw.transpose([0,3,4,1,2])) * w_mask[None,:,None,:,None]
-    logits.append(jnp.einsum("nc,nia,ciajb->njb", labels, inputs["x"], mw))
-      
-  # categorical-crossentropy (or pseudo-likelihood)
-  cce_loss = -(inputs["x"] * jax.nn.log_softmax(sum(logits))).sum([1,2])
-
-  return (cce_loss*inputs["x_weight"]).sum()
-
-class MRF:  
-  def __init__(self, X, X_weight=None,
-               batch_size=None,
-               ar=False, ar_ent=False,
-               lam=0.01,
-               k=1, lr=0.1, shared=False, tied=True, full=False):
-    
-    ## MODE ##
-    inc = ["b","w"] if (tied or full) else ["b"]
-    if k > 1:
-      if shared:
-        if tied: inc += ["mb"]
-        if full: inc += ["mb","mw"]
-      else:
-        if tied: inc = ["mb","w"]
-        if full: inc = ["mb","mw"]
-
-    N,L,A = X.shape
-    self.batch_size = batch_size
-    self.k = k
-
-    # weight per sequence
-    X = jnp.asarray(X)
-    X_weight = get_eff(X) if X_weight is None else jnp.asarray(X_weight)
-    self.Neff = X_weight.sum()
-
-    if batch_size is None:
-      learning_rate = lr * np.log(N)/L
-    else:
-      lam = lam * batch_size/N
-      learning_rate = lr * jnp.log(batch_size)/L
-
-    if ar:
-      self.order = jnp.arange(L)
-    elif ar_ent:
-      f_i = (X * X_weight[:,None,None]).sum(0)/self.Neff
-      self.order = (-f_i * jnp.log(f_i + 1e-8)).sum(-1).argsort()
-    else:
-      self.order = None
-
-    # setup the model
-    def model(params, inputs):
-      labels = inputs["labels"] if "labels" in inputs else None
-      pll = pll_loss(params, inputs, self.order, labels)
-      reg = reg_loss(params, lam)
-      loss = pll + reg
-      return None, loss
-
-    # initialize inputs
-    self.inputs = {"x":X, "x_weight":X_weight}
-
-    # initialize params
-    self.params = {}
-    if "w" in inc: self.params["w"] = jnp.zeros((L,A,L,A))
-    if "mw" in inc: self.params["mw"] = jnp.zeros((k,L,A,L,A))
-    if "b" in inc:
-      b = jnp.log((X * X_weight[:,None,None]).sum(0) + (lam+1e-8) * jnp.log(self.Neff))
-      self.params["b"] = b - b.mean(-1,keepdims=True)
-    if "mb" in inc or "mw" in inc:
-      kms = kmeans(X, X_weight, k=k)
-      self.inputs["labels"] = kms["labels"]
-      mb = jnp.log(kms["means"] * self.Neff + (lam+1e-8) * jnp.log(self.Neff))
-      self.params["mb"] = mb - mb.mean(-1,keepdims=True)
-      if "b" in self.params:
-        self.params["mb"] -= self.params["b"]
-
-    # setup optimizer
-    self.opt = laxy.OPT(model, self.params, lr=learning_rate)
-
-  def get_msa(self, samples=1000, burn_in=1):
-    self.params = self.opt.get_params()
-    if "labels" in self.inputs:
-      self.params["c"] = jnp.log((self.inputs["x_weight"][:,None] * self.inputs["labels"]).sum(0) + 1e-8)
-    
-    key = laxy.get_random_key()
-    return sample_msa(samples=samples,burn_in=burn_in,order=self.order)(key, self.params)
-  
-  def get_w(self):
-    self.params = self.opt.get_params()
-    w = []
-    if "w" in self.params: w.append(self.params["w"])
-    if "mw" in self.params: w.append(self.params["mw"].sum(0))
-    w = sum(w)
-    w = (w + w.transpose(2,3,0,1))/2
-    w = w - w.mean((1,3),keepdims=True)
-    return w
-    
-  def fit(self, steps=100, verbose=True, return_losses=False):
-    '''train model'''
-    losses = self.opt.fit(self.inputs, steps=steps, batch_size=self.batch_size,
-                         verbose=verbose, return_losses=return_losses)
-    if return_losses: return losses
-
-class MRF_BM:
-  def __init__(self, X, X_weight=None, samples=1000,
-               burn_in=1, temp=1.0,
-               ar=False, ar_ent=True,
-               lr=0.05, lam=0.01,
-               k=1, mode="tied"):
-
-    ## MODE ##
-    inc = ["b","mb"] if k > 1 else ["b"]
-    if mode == "tied": inc += ["w"]
-    if mode == "full": inc += ["w","mw"] if k > 1 else ["w"]
-
-    self.X = jnp.asarray(X)
-    N,L,A = self.X.shape
-    learning_rate = lr * np.log(N)/L
-
-    # weight per sequence
-    self.X_weight = get_eff(X) if X_weight is None else jnp.asarray(X_weight)
-    self.Neff = self.X_weight.sum()
-
-    # collect stats 
-    if k > 1:
-      self.kms = kmeans(self.X, self.X_weight, k=k)
-      self.labels = self.kms["labels"]
-      self.inputs = get_stats(self.X, self.X_weight, labels=self.kms["labels"],
-                              add_mf_ij=("mw" in inc))
-      self.inputs["c"] = self.kms["cat"]
-    else:
-      self.labels = None
-      self.inputs = get_stats(self.X, self.X_weight)
-    
-    # low entropy to high entropy
-    if ar_ent:
-      ent = -(self.inputs["f_i"] * jnp.log(self.inputs["f_i"] + 1e-8)).sum(-1)
-      self.order = ent.argsort()
-    elif ar: self.order = jnp.arange(L)
-    else: self.order = None
-
-    self.burn_in = burn_in
-    self.temp = temp
-
-    # setup the model
-    def model(params, inputs):
-
-      # sample msa
-      sample = sample_msa(samples=samples, burn_in=burn_in,
-                          temp=temp, order=self.order)(inputs["key"], params)
-
-      # compute stats
-      stats = get_stats(sample["msa"], labels=sample["labels"],
-                        add_mf_ij=("mw" in params))
-
-      # define gradients
-      grad = {}
-      I = (1-jnp.eye(L))[:,None,:,None]
-      if "c" in params: grad["c"] = sample["labels"].mean(0) - inputs["c"]
-      if "b" in params: grad["b"] = stats["f_i"] - inputs["f_i"]
-      if "w" in params: grad["w"] = (stats["f_ij"] - inputs["f_ij"]) * I
-      if "mb" in params: grad["mb"] = stats["mf_i"] - inputs["mf_i"]
-      if "mw" in params: grad["mw"] = (stats["mf_ij"] - inputs["mf_ij"]) * I[None]
-
-      # add regularization
-      reg_grad = jax.grad(reg_loss)(params,lam)
-
-      for g in grad.keys():
-        if g in reg_grad: grad[g] = grad[g] * self.Neff + reg_grad[g]
-        else: grad[g] = grad[g] * self.Neff
-
-      return None, None, grad
-
-    # initialize model params
-    self.params = {}
-    if "w" in inc:
-      self.params["w"] = jnp.zeros((L,A,L,A))
-    if "b" in inc:
-      b = jnp.log(self.inputs["f_i"] * self.Neff + (lam+1e-8) * jnp.log(self.Neff))
-      self.params["b"] = b - b.mean(-1,keepdims=True)
-
-    # setup mixture params
-    if "mb" in inc or "mw" in inc:
-      c = jnp.log(self.inputs["c"] * self.Neff + 1e-8)
-      self.params["c"] = c - c.mean(-1,keepdims=True)
-
-    if "mw" in inc:
-      self.params["mw"] = jnp.zeros((k,L,A,L,A))
-    if "mb" in inc:
-      mb = jnp.log(self.inputs["mf_i"] * self.Neff + (lam+1e-8) * jnp.log(self.Neff))
-      self.params["mb"] = mb - mb.mean(-1,keepdims=True)
-      if "b" in self.params: self.params["mb"] -= self.params["b"]
-
-    # setup optimizer
-    self.opt = laxy.OPT(model, self.params, lr=learning_rate, has_grad=True)
-  
-  def get_msa(self, samples=1000, burn_in=None, temp=None, seed=0):
-    if burn_in is None: burn_in = self.burn_in
-    if temp is None: temp = self.temp
-
-    self.params = self.opt.get_params()
-    key = jax.random.PRNGKey(seed)
-    return sample_msa(samples=samples,
-                      burn_in=self.burn_in,
-                      order=self.order)(key, self.params)["msa"]
-
-  def fit(self, steps=1000, verbose=True):
-    '''train model'''
-    self.opt.fit(self.inputs, steps=steps, verbose=verbose)
\ No newline at end of file
diff --git a/colabdesign/seq/stats.py b/colabdesign/seq/stats.py
deleted file mode 100644
index 83046a6..0000000
--- a/colabdesign/seq/stats.py
+++ /dev/null
@@ -1,77 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-
-def get_stats(X, X_weight=None, labels=None, add_f_ij=True, add_mf_ij=False, add_c=False):
-  '''compute f_i/f_ij/f_ijk given msa '''
-  n = None
-  if X_weight is None:
-    Xn = Xs = X
-  else:
-    Xn, Xs = X*X_weight[:,n,n], X*jnp.sqrt(X_weight[:,n,n])
-  f_i = Xn.sum(0)
-  o = {"f_i": f_i / f_i.sum(1,keepdims=True)}
-  
-  if add_f_ij:
-    f_ij = jnp.tensordot(Xs,Xs,[0,0])
-    o["f_ij"] = f_ij / f_ij.sum((1,3),keepdims=True)
-    if add_c: o["c_ij"] = o["f_ij"] - o["f_i"][:,:,n,n] * o["f_i"][n,n,:,:]
-
-  if labels is not None:
-    # compute mixture stats
-    if jnp.issubdtype(labels, jnp.integer):
-      labels = jax.nn.one_hot(labels,labels.max()+1)
-    mf_i = jnp.einsum("nc,nia->cia", labels, Xn)
-    o["mf_i"] = mf_i/mf_i.sum((0,2),keepdims=True)
-    if add_mf_ij:
-      mf_ij = jnp.einsum("nc,nia,njb->ciajb", labels, Xs, Xs)
-      o["mf_ij"] = mf_ij/mf_ij.sum((0,2,4),keepdims=True)   
-      if add_c: o["mc_ij"] = o["mf_ij"] - o["mf_i"][:,:,:,n,n] * o["mf_i"][:,n,n,:,:]
-  return o
-    
-def get_r(a,b):
-  a = jnp.array(a).flatten()
-  b = jnp.array(b).flatten()
-  return jnp.corrcoef(a,b)[0,1]
-
-def inv_cov(X, X_weight=None):
-  X = jnp.asarray(X)
-  N,L,A = X.shape
-  if X_weight is None:
-    num_points = N
-  else:
-    X_weight = jnp.asarray(X_weight)
-    num_points = X_weight.sum()
-  c = get_stats(X, X_weight, add_mf_ij=True, add_c=True)["c_ij"]
-  c = c.reshape(L*A,L*A)
-  shrink = 4.5/jnp.sqrt(num_points) * jnp.eye(c.shape[0])
-  ic = jnp.linalg.inv(c + shrink)
-  return ic.reshape(L,A,L,A)
-
-def get_mtx(W):
-  W = jnp.asarray(W)
-  # l2norm of 20x20 matrices (note: we ignore gaps)
-  raw = jnp.sqrt(jnp.sum(np.square(W[:,1:,:,1:]),(1,3)))
-  raw = raw.at[jnp.diag_indices_from(raw)].set(0)
-
-  # apc (average product correction)
-  ap = raw.sum(0,keepdims=True) * raw.sum(1,keepdims=True) / raw.sum()
-  apc = raw - ap
-  apc = apc.at[jnp.diag_indices_from(apc)].set(0)
-  return raw, apc
-
-def con_auc(true, pred, mask=None):
-  '''compute agreement between predicted and measured contact map'''
-  true = jnp.asarray(true)
-  pred = jnp.asarray(pred)
-  if mask is not None:
-    mask = jnp.asarray(mask)
-    idx = mask.sum(-1) > 0
-    true = true[idx,:][:,idx]
-    pred = pred[idx,:][:,idx]
-  eval_idx = jnp.triu_indices_from(true, 6)
-  pred_, true_ = pred[eval_idx], true[eval_idx] 
-  L = (jnp.linspace(0.1,1.0,10)*len(true)).astype(jnp.int32)
-  sort_idx = jnp.argsort(pred_)[::-1]
-  return jnp.asarray([true_[sort_idx[:l]].mean() for l in L])
-
diff --git a/colabdesign/seq/utils.py b/colabdesign/seq/utils.py
deleted file mode 100644
index 579638f..0000000
--- a/colabdesign/seq/utils.py
+++ /dev/null
@@ -1,60 +0,0 @@
-import os, string
-import numpy as np
-import jax
-import jax.numpy as jnp
-
-ALPHABET = list("ARNDCQEGHILKMFPSTWYV-")
-
-def parse_fasta(filename, a3m=False, stop=100000):
-  '''function to parse fasta file'''
-  
-  if a3m:
-    # for a3m files the lowercase letters are removed
-    # as these do not align to the query sequence
-    rm_lc = str.maketrans(dict.fromkeys(string.ascii_lowercase))
-    
-  header, sequence = [],[]
-  lines = open(filename, "r")
-  for line in lines:
-    line = line.rstrip()
-    if len(line) > 0:
-      if line[0] == ">":
-        if len(header) == stop:
-          break
-        else:
-          header.append(line[1:])
-          sequence.append([])
-      else:
-        if a3m: line = line.translate(rm_lc)
-        else: line = line.upper()
-        sequence[-1].append(line)
-  lines.close()
-  sequence = [''.join(seq) for seq in sequence]
-  
-  return header, sequence
-  
-def mk_msa(seqs):
-  '''one hot encode msa'''
-  states = len(ALPHABET)  
-  a2n = {a:n for n,a in enumerate(ALPHABET)}
-  msa_ori = np.array([[a2n.get(aa, states-1) for aa in seq] for seq in seqs])
-  return np.eye(states)[msa_ori]
-
-def get_eff(msa, eff_cutoff=0.8):
-  '''compute weight per sequence'''
-  if msa.shape[0] > 10000:
-    # loop one-to-all (to avoid memory issues)
-    msa = msa.argmax(-1)
-    def get_w(seq): return 1/((seq==msa).mean(-1) > eff_cutoff).sum()
-    return jax.lax.scan(lambda _,x:(_,get_w(x)),None,msa,unroll=2)[1]
-  else:
-    # all-to-all
-    msa_ident = jnp.tensordot(msa,msa,[[1,2],[1,2]])/msa.shape[1]
-    return 1/(msa_ident >= eff_cutoff).sum(-1)   
-
-def ar_mask(order, diag=True):
-  '''compute autoregressive mask, given order of positions'''
-  L = order.shape[0]
-  r = order[::-1].argsort()
-  tri = jnp.triu(jnp.ones((L,L)),k=not diag)
-  return tri[r[None,:],r[:,None]]
diff --git a/colabdesign/shared/model.py b/colabdesign/shared/model.py
index e02a369..a0aef6e 100644
--- a/colabdesign/shared/model.py
+++ b/colabdesign/shared/model.py
@@ -1,14 +1,174 @@
+#!/usr/bin/env python
+# encoding: utf-8
+import random
 import jax
 import jax.numpy as jnp
 import numpy as np
 import optax
+import getopt
 
-from colabdesign.shared.utils import copy_dict, update_dict, softmax, Key
-from colabdesign.shared.prep import rewire
 from colabdesign.af.alphafold.common import residue_constants
+from string import ascii_uppercase, ascii_lowercase
+alphabet_list = list(ascii_uppercase+ascii_lowercase)
 
 aa_order = residue_constants.restype_order
 order_aa = {b:a for a,b in aa_order.items()}
+########################################################################
+######## shared-utils
+########################################################################
+
+def update_dict(D, *args, **kwargs):
+  '''robust function for updating dictionary'''
+  def set_dict(d, x, override=False):
+    for k,v in x.items():
+      if v is not None:
+        if k in d:
+          if isinstance(v, dict):
+            set_dict(d[k], x[k], override=override)
+          elif override or d[k] is None:
+            d[k] = v
+          elif isinstance(d[k],(np.ndarray,jnp.ndarray)):
+            d[k] = np.asarray(v)
+          elif isinstance(d[k], dict):
+            d[k] = jax.tree_map(lambda x: type(x)(v), d[k])
+          else:
+            d[k] = type(d[k])(v)
+        else:
+          print(f"ERROR: '{k}' not found in {list(d.keys())}")
+  override = kwargs.pop("override", False)
+  while len(args) > 0 and isinstance(args[0],str):
+    D,args = D[args[0]],args[1:]
+  for a in args:
+    if isinstance(a, dict): set_dict(D, a, override=override)
+  set_dict(D, kwargs, override=override)
+
+def copy_dict(x):
+  '''deepcopy dictionary'''
+  return jax.tree_map(lambda y:y, x)
+
+def to_float(x):
+  '''convert to float'''
+  if hasattr(x,"tolist"): x = x.tolist()
+  if isinstance(x,dict): x = {k:to_float(y) for k,y in x.items()}
+  elif hasattr(x,"__iter__"): x = [to_float(y) for y in x]
+  else: x = float(x)
+  return x
+
+def dict_to_str(x, filt=None, keys=None, ok=None, print_str=None, f=2):
+  '''convert dictionary to string for print out'''
+  if keys is None: keys = []
+  if filt is None: filt = {}
+  if print_str is None: print_str = ""
+  if ok is None: ok = []
+
+  # gather keys
+  for k in x.keys():
+    if k not in keys:
+      keys.append(k)
+
+  for k in keys:
+    if k in x and (filt.get(k,True) or k in ok):
+      v = x[k]
+      if isinstance(v,float):
+        if int(v) == v:
+          print_str += f" {k} {int(v)}"
+        else:
+          print_str += f" {k} {v:.{f}f}"
+      else:
+        print_str += f" {k} {v}"
+  return print_str
+
+class Key():
+  '''random key generator'''
+  def __init__(self, key=None, seed=None):
+    if key is None:
+      self.seed = random.randint(0,2147483647) if seed is None else seed
+      self.key = jax.random.PRNGKey(self.seed)
+    else:
+      self.key = key
+  def get(self, num=1):
+    if num > 1:
+      self.key, *sub_keys = jax.random.split(self.key, num=(num+1))
+      return sub_keys
+    else:
+      self.key, sub_key = jax.random.split(self.key)
+      return sub_key
+
+def softmax(x, axis=-1):
+  x = x - x.max(axis,keepdims=True)
+  x = np.exp(x)
+  return x / x.sum(axis,keepdims=True)
+
+def categorical(p):
+  return (p.cumsum(-1) >= np.random.uniform(size=p.shape[:-1])[..., None]).argmax(-1)
+
+def to_list(xs):
+  if not isinstance(xs,list): xs = [xs]
+  return [x for x in xs if x is not None]
+
+def copy_missing(a,b):
+  for i,v in a.items():
+    if i not in b:
+      b[i] = v
+    elif isinstance(v,dict):
+      copy_missing(v,b[i])
+#######################################################################
+### shared-model.py
+######################################################################
+
+class parse_args:
+  def __init__(self):
+    self.long,self.short = [],[]
+    self.info,self.help = [],[]
+
+  def txt(self,help):
+    self.help.append(["txt",help])
+
+  def add(self, arg, default, type, help=None):
+    self.long.append(arg[0])
+    key = arg[0].replace("=","")
+    self.info.append({"key":key, "type":type,
+                      "value":default, "arg":[f"--{key}"]})
+    if len(arg) == 2:
+      self.short.append(arg[1])
+      s_key = arg[1].replace(":","")
+      self.info[-1]["arg"].append(f"-{s_key}")
+    if help is not None:
+      self.help.append(["opt",[arg,help]])
+
+  def parse(self,argv):
+    for opt, arg in getopt.getopt(argv,"".join(self.short),self.long)[0]:
+      for x in self.info:
+        if opt in x["arg"]:
+          if x["type"] is None: x["value"] = (x["value"] == False)
+          else: x["value"] = x["type"](arg)
+
+    opts = {x["key"]:x["value"] for x in self.info}
+    print(str(opts).replace(" ",""))
+    return dict2obj(opts)
+
+  def usage(self, err):
+    for type,info in self.help:
+      if type == "txt": print(info)
+      if type == "opt":
+        arg, helps = info
+        help = helps[0]
+        if len(arg) == 1: print("--%-15s : %s" % (arg[0],help))
+        if len(arg) == 2: print("--%-10s -%-3s : %s" % (arg[0],arg[1].replace(":",""),help))
+        for help in helps[1:]: print("%19s %s" % ("",help))
+    print(f"< {err} >")
+    print(" "+"-"*(len(err)+2))
+    print("        \   ^__^               ")
+    print("         \  (oo)\_______       ")
+    print("            (__)\       )\/\   ")
+    print("                ||----w |      ")
+    print("                ||     ||      ")
+    sys.exit()
+
+class dict2obj():
+  def __init__(self, dictionary):
+    for key in dictionary:
+      setattr(self, key, dictionary[key])
 
 class design_model:
   def set_weights(self, *args, **kwargs):
@@ -53,7 +213,7 @@ class design_model:
 
     # decide on shape
     shape = (self._num, self._len, self._args.get("alphabet_size",20))
-    
+
     # initialize bias
     if bias is None:
       b = np.zeros(shape[1:])
@@ -64,7 +224,7 @@ class design_model:
     if rm_aa is not None:
       for aa in rm_aa.split(","):
         b[...,aa_order[aa]] -= 1e6
-        
+
     # use wildtype sequence
     if ("wildtype" in mode or "wt" in mode) and hasattr(self,"_wt_aatype"):
       wt_seq = np.eye(shape[-1])[self._wt_aatype]
@@ -74,7 +234,7 @@ class design_model:
         seq[:,self.opt["pos"],:] = wt_seq
       else:
         seq = wt_seq
-    
+
     # initialize sequence
     if seq is None:
       if hasattr(self,"key"):
@@ -99,10 +259,10 @@ class design_model:
         seq_ = np.eye(shape[-1])[seq]
         seq_[seq == -1] = 0
         seq = seq_
-      
+
       if kwargs.pop("add_seq",False):
         b = b + seq * 1e7
-      
+
       if seq.ndim == 2:
         x = np.pad(seq[None],[[0,shape[0]-1],[0,0],[0,0]])
       elif shape[0] > seq.shape[0]:
@@ -118,24 +278,24 @@ class design_model:
         y = x + y_gumbel / self.opt["alpha"]
       else:
         y = x + y_gumbel
-      
+
       x = np.where(x.sum(-1,keepdims=True) == 1, x, y)
 
     # set seq/bias/state
     self._params["seq"] = x
-    self._inputs["bias"] = b 
+    self._inputs["bias"] = b
 
   def _norm_seq_grad(self):
     g = self.aux["grad"]["seq"]
     eff_L = (np.square(g).sum(-1,keepdims=True) > 0).sum(-2,keepdims=True)
     gn = np.linalg.norm(g,axis=(-1,-2),keepdims=True)
-    self.aux["grad"]["seq"] = g * np.sqrt(eff_L) / (gn + 1e-7)  
+    self.aux["grad"]["seq"] = g * np.sqrt(eff_L) / (gn + 1e-7)
 
   def set_optimizer(self, optimizer=None, learning_rate=None, norm_seq_grad=None, **kwargs):
     '''
     set/reset optimizer
     ----------------------------------
-    supported optimizers include: [adabelief, adafactor, adagrad, adam, adamw, 
+    supported optimizers include: [adabelief, adafactor, adagrad, adam, adamw,
     fromage, lamb, lars, noisy_sgd, dpsgd, radam, rmsprop, sgd, sm3, yogi]
     '''
     optimizers = {'adabelief':optax.adabelief,'adafactor':optax.adafactor,
@@ -145,7 +305,7 @@ class design_model:
                   'noisy_sgd':optax.noisy_sgd,'dpsgd':optax.dpsgd,
                   'radam':optax.radam,'rmsprop':optax.rmsprop,
                   'sgd':optax.sgd,'sm3':optax.sm3,'yogi':optax.yogi}
-    
+
     if optimizer is None: optimizer = self._args["optimizer"]
     if learning_rate is not None: self.opt["learning_rate"] = learning_rate
     if norm_seq_grad is not None: self.opt["norm_seq_grad"] = norm_seq_grad
@@ -157,13 +317,13 @@ class design_model:
       updates, state = o.update(grad, state, params)
       grad = jax.tree_map(lambda x:-x, updates)
       return state, grad
-    
+
     self._optimizer = jax.jit(update_grad)
 
   def set_seed(self, seed=None):
     np.random.seed(seed=seed)
     self.key = Key(seed=seed).get
-    
+
   def get_seq(self, get_best=True):
     '''
     get sequences as strings
@@ -172,7 +332,7 @@ class design_model:
     aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
     x = aux["seq"]["hard"].argmax(-1)
     return ["".join([order_aa[a] for a in s]) for s in x]
-  
+
   def get_seqs(self, get_best=True):
     return self.get_seq(get_best)
 
@@ -221,3 +381,187 @@ def soft_seq(x, bias, opt, key=None, num_seq=None, shuffle_first=True):
   seq["pseudo"] = opt["soft"] * seq["soft"] + (1-opt["soft"]) * seq["input"]
   seq["pseudo"] = opt["hard"] * seq["hard"] + (1-opt["hard"]) * seq["pseudo"]
   return seq
+
+###########################################################################
+####### shared-protein
+##########################################################################
+MODRES = {'MSE':'MET','MLY':'LYS','FME':'MET','HYP':'PRO',
+          'TPO':'THR','CSO':'CYS','SEP':'SER','M3L':'LYS',
+          'HSK':'HIS','SAC':'SER','PCA':'GLU','DAL':'ALA',
+          'CME':'CYS','CSD':'CYS','OCS':'CYS','DPR':'PRO',
+          'B3K':'LYS','ALY':'LYS','YCM':'CYS','MLZ':'LYS',
+          '4BF':'TYR','KCX':'LYS','B3E':'GLU','B3D':'ASP',
+          'HZP':'PRO','CSX':'CYS','BAL':'ALA','HIC':'HIS',
+          'DBZ':'ALA','DCY':'CYS','DVA':'VAL','NLE':'LEU',
+          'SMC':'CYS','AGM':'ARG','B3A':'ALA','DAS':'ASP',
+          'DLY':'LYS','DSN':'SER','DTH':'THR','GL3':'GLY',
+          'HY3':'PRO','LLP':'LYS','MGN':'GLN','MHS':'HIS',
+          'TRQ':'TRP','B3Y':'TYR','PHI':'PHE','PTR':'TYR',
+          'TYS':'TYR','IAS':'ASP','GPL':'LYS','KYN':'TRP',
+          'CSD':'CYS','SEC':'CYS'}
+
+def pdb_to_string(pdb_file, chains=None, models=None):
+  '''read pdb file and return as string'''
+
+  if chains is not None:
+    if "," in chains: chains = chains.split(",")
+    if not isinstance(chains,list): chains = [chains]
+  if models is not None:
+    if not isinstance(models,list): models = [models]
+
+  modres = {**MODRES}
+  lines = []
+  seen = []
+  model = 1
+
+  if "\n" in pdb_file:
+    old_lines = pdb_file.split("\n")
+  else:
+    with open(pdb_file,"rb") as f:
+      old_lines = [line.decode("utf-8","ignore").rstrip() for line in f]
+  for line in old_lines:
+    if line[:5] == "MODEL":
+      model = int(line[5:])
+    if models is None or model in models:
+      if line[:6] == "MODRES":
+        k = line[12:15]
+        v = line[24:27]
+        if k not in modres and v in residue_constants.restype_3to1:
+          modres[k] = v
+      if line[:6] == "HETATM":
+        k = line[17:20]
+        if k in modres:
+          line = "ATOM  "+line[6:17]+modres[k]+line[20:]
+      if line[:4] == "ATOM":
+        chain = line[21:22]
+        if chains is None or chain in chains:
+          atom = line[12:12+4].strip()
+          resi = line[17:17+3]
+          resn = line[22:22+5].strip()
+          if resn[-1].isalpha(): # alternative atom
+            resn = resn[:-1]
+            line = line[:26]+" "+line[27:]
+          key = f"{model}_{chain}_{resn}_{resi}_{atom}"
+          if key not in seen: # skip alternative placements
+            lines.append(line)
+            seen.append(key)
+      if line[:5] == "MODEL" or line[:3] == "TER" or line[:6] == "ENDMDL":
+        lines.append(line)
+  return "\n".join(lines)
+
+def renum_pdb_str(pdb_str, Ls=None, renum=True, offset=1):
+  if Ls is not None:
+    L_init = 0
+    new_chain = {}
+    for L,c in zip(Ls, alphabet_list):
+      new_chain.update({i:c for i in range(L_init,L_init+L)})
+      L_init += L
+
+  n,num,pdb_out = 0,offset,[]
+  resnum_ = None
+  chain_ = None
+  new_chain_ = new_chain[0]
+  for line in pdb_str.split("\n"):
+    if line[:4] == "ATOM":
+      chain = line[21:22]
+      resnum = int(line[22:22+5])
+      if resnum_ is None: resnum_ = resnum
+      if chain_ is None: chain_ = chain
+      if resnum != resnum_ or chain != chain_:
+        num += (resnum - resnum_)
+        n += 1
+        resnum_,chain_ = resnum,chain
+      if Ls is not None:
+        if new_chain[n] != new_chain_:
+          num = offset
+          new_chain_ = new_chain[n]
+      N = num if renum else resnum
+      if Ls is None: pdb_out.append("%s%4i%s" % (line[:22],N,line[26:]))
+      else: pdb_out.append("%s%s%4i%s" % (line[:21],new_chain[n],N,line[26:]))
+  return "\n".join(pdb_out)
+
+
+def _np_norm(x, axis=-1, keepdims=True, eps=1e-8, use_jax=True):
+  '''compute norm of vector'''
+  _np = jnp if use_jax else np
+  return _np.sqrt(_np.square(x).sum(axis,keepdims=keepdims) + 1e-8)
+def _np_extend(a,b,c, L,A,D, use_jax=True):
+  '''
+  given coordinates a-b-c,
+  c-d (L)ength, b-c-d (A)ngle, and a-b-c-d (D)ihedral
+  return 4th coordinate d
+  '''
+  _np = jnp if use_jax else np
+  normalize = lambda x: x/_np_norm(x, use_jax=use_jax)
+  bc = normalize(b-c)
+  n = normalize(_np.cross(b-a, bc))
+  return c + sum([L * _np.cos(A) * bc,
+                  L * _np.sin(A) * _np.cos(D) * _np.cross(n, bc),
+                  L * _np.sin(A) * _np.sin(D) * -n])
+
+def _np_get_cb(N,CA,C, use_jax=True):
+  '''compute CB placement from N, CA, C'''
+  return _np_extend(C, N, CA, 1.522, 1.927, -2.143, use_jax=use_jax)
+
+#########################################################################
+##### shared- prep
+#########################################################################
+
+def prep_pos(pos, residue, chain):
+  '''
+  given input [pos]itions (a string of segment ranges seperated by comma,
+  for example: "1,3-4,10-15"), return list of indices to constrain.
+  '''
+  residue_set = []
+  chain_set = []
+  len_set = []
+  for idx in pos.split(","):
+    i,j = idx.split("-") if "-" in idx else (idx, None)
+
+    if i.isalpha() and j is None:
+      residue_set += [None]
+      chain_set += [i]
+      len_set += [i]
+    else:
+      # if chain defined
+      if i[0].isalpha():
+        c,i = i[0], int(i[1:])
+      else:
+        c,i = chain[0],int(i)
+      if j is None:
+        j = i
+      else:
+        j = int(j[1:] if j[0].isalpha() else j)
+      residue_set += list(range(i,j+1))
+      chain_set += [c] * (j-i+1)
+      len_set += [j-i+1]
+
+  residue = np.asarray(residue)
+  chain = np.asarray(chain)
+  pos_set = []
+  for i,c in zip(residue_set, chain_set):
+    if i is None:
+      idx = np.where(chain == c)[0]
+      assert len(idx) > 0, f'ERROR: chain {c} not found'
+      pos_set += [n for n in idx]
+      len_set[len_set.index(c)] = len(idx)
+    else:
+      idx = np.where((residue == i) & (chain == c))[0]
+      assert len(idx) == 1, f'ERROR: positions {i} and chain {c} not found'
+      pos_set.append(idx[0])
+
+  return {"residue":np.array(residue_set),
+          "chain":np.array(chain_set),
+          "length":np.array(len_set),
+          "pos":np.asarray(pos_set)}
+
+def _np_kabsch(a, b, return_v=False, use_jax=True):
+  '''get alignment matrix for two sets of coodinates'''
+  _np = jnp if use_jax else np
+  ab = a.swapaxes(-1,-2) @ b
+  u, s, vh = _np.linalg.svd(ab, full_matrices=False)
+  flip = _np.linalg.det(u @ vh) < 0
+  u_ = _np.where(flip, -u[...,-1].T, u[...,-1].T).T
+  if use_jax: u = u.at[...,-1].set(u_)
+  else: u[...,-1] = u_
+  return u if return_v else (u @ vh)
diff --git a/colabdesign/shared/parse_args.py b/colabdesign/shared/parse_args.py
deleted file mode 100644
index 45c2844..0000000
--- a/colabdesign/shared/parse_args.py
+++ /dev/null
@@ -1,55 +0,0 @@
-import sys, getopt
-# class for parsing arguments
-class parse_args:
-  def __init__(self):
-    self.long,self.short = [],[]
-    self.info,self.help = [],[]
-
-  def txt(self,help):
-    self.help.append(["txt",help])
-
-  def add(self, arg, default, type, help=None):
-    self.long.append(arg[0])
-    key = arg[0].replace("=","")
-    self.info.append({"key":key, "type":type,
-                      "value":default, "arg":[f"--{key}"]})
-    if len(arg) == 2:
-      self.short.append(arg[1])
-      s_key = arg[1].replace(":","")
-      self.info[-1]["arg"].append(f"-{s_key}")
-    if help is not None:
-      self.help.append(["opt",[arg,help]])
-
-  def parse(self,argv):
-    for opt, arg in getopt.getopt(argv,"".join(self.short),self.long)[0]:
-      for x in self.info:
-        if opt in x["arg"]:
-          if x["type"] is None: x["value"] = (x["value"] == False)
-          else: x["value"] = x["type"](arg)
-
-    opts = {x["key"]:x["value"] for x in self.info}
-    print(str(opts).replace(" ",""))
-    return dict2obj(opts)
-
-  def usage(self, err):
-    for type,info in self.help:
-      if type == "txt": print(info)
-      if type == "opt":
-        arg, helps = info
-        help = helps[0]
-        if len(arg) == 1: print("--%-15s : %s" % (arg[0],help))
-        if len(arg) == 2: print("--%-10s -%-3s : %s" % (arg[0],arg[1].replace(":",""),help))
-        for help in helps[1:]: print("%19s %s" % ("",help))
-    print(f"< {err} >")
-    print(" "+"-"*(len(err)+2))
-    print("        \   ^__^               ")
-    print("         \  (oo)\_______       ")
-    print("            (__)\       )\/\   ")
-    print("                ||----w |      ")
-    print("                ||     ||      ")
-    sys.exit()
-
-class dict2obj():
-  def __init__(self, dictionary):
-    for key in dictionary:
-      setattr(self, key, dictionary[key])
\ No newline at end of file
diff --git a/colabdesign/shared/plot.py b/colabdesign/shared/plot.py
deleted file mode 100644
index e0c31e6..0000000
--- a/colabdesign/shared/plot.py
+++ /dev/null
@@ -1,336 +0,0 @@
-# import matplotlib
-import numpy as np
-from scipy.special import expit as sigmoid
-from colabdesign.shared.protein import _np_kabsch, alphabet_list
-
-import matplotlib
-import matplotlib.pyplot as plt
-import matplotlib.patheffects
-from matplotlib import animation
-from matplotlib.gridspec import GridSpec 
-from matplotlib import collections as mcoll
-try:
-  import py3Dmol
-except:
-  print("py3Dmol not installed")
-  
-pymol_color_list = ["#33ff33","#00ffff","#ff33cc","#ffff00","#ff9999","#e5e5e5","#7f7fff","#ff7f00",
-                    "#7fff7f","#199999","#ff007f","#ffdd5e","#8c3f99","#b2b2b2","#007fff","#c4b200",
-                    "#8cb266","#00bfbf","#b27f7f","#fcd1a5","#ff7f7f","#ffbfdd","#7fffff","#ffff7f",
-                    "#00ff7f","#337fcc","#d8337f","#bfff3f","#ff7fff","#d8d8ff","#3fffbf","#b78c4c",
-                    "#339933","#66b2b2","#ba8c84","#84bf00","#b24c66","#7f7f7f","#3f3fa5","#a5512b"]
-
-jalview_color_list = {"Clustal":           ["#80a0f0","#f01505","#00ff00","#c048c0","#f08080","#00ff00","#c048c0","#f09048","#15a4a4","#80a0f0","#80a0f0","#f01505","#80a0f0","#80a0f0","#ffff00","#00ff00","#00ff00","#80a0f0","#15a4a4","#80a0f0"],
-                      "Zappo":             ["#ffafaf","#6464ff","#00ff00","#ff0000","#ffff00","#00ff00","#ff0000","#ff00ff","#6464ff","#ffafaf","#ffafaf","#6464ff","#ffafaf","#ffc800","#ff00ff","#00ff00","#00ff00","#ffc800","#ffc800","#ffafaf"],
-                      "Taylor":            ["#ccff00","#0000ff","#cc00ff","#ff0000","#ffff00","#ff00cc","#ff0066","#ff9900","#0066ff","#66ff00","#33ff00","#6600ff","#00ff00","#00ff66","#ffcc00","#ff3300","#ff6600","#00ccff","#00ffcc","#99ff00"],
-                      "Hydrophobicity":    ["#ad0052","#0000ff","#0c00f3","#0c00f3","#c2003d","#0c00f3","#0c00f3","#6a0095","#1500ea","#ff0000","#ea0015","#0000ff","#b0004f","#cb0034","#4600b9","#5e00a1","#61009e","#5b00a4","#4f00b0","#f60009","#0c00f3","#680097","#0c00f3"],
-                      "Helix Propensity":  ["#e718e7","#6f906f","#1be41b","#778877","#23dc23","#926d92","#ff00ff","#00ff00","#758a75","#8a758a","#ae51ae","#a05fa0","#ef10ef","#986798","#00ff00","#36c936","#47b847","#8a758a","#21de21","#857a85","#49b649","#758a75","#c936c9"],
-                      "Strand Propensity": ["#5858a7","#6b6b94","#64649b","#2121de","#9d9d62","#8c8c73","#0000ff","#4949b6","#60609f","#ecec13","#b2b24d","#4747b8","#82827d","#c2c23d","#2323dc","#4949b6","#9d9d62","#c0c03f","#d3d32c","#ffff00","#4343bc","#797986","#4747b8"],
-                      "Turn Propensity":   ["#2cd3d3","#708f8f","#ff0000","#e81717","#a85757","#3fc0c0","#778888","#ff0000","#708f8f","#00ffff","#1ce3e3","#7e8181","#1ee1e1","#1ee1e1","#f60909","#e11e1e","#738c8c","#738c8c","#9d6262","#07f8f8","#f30c0c","#7c8383","#5ba4a4"],
-                      "Buried Index":      ["#00a35c","#00fc03","#00eb14","#00eb14","#0000ff","#00f10e","#00f10e","#009d62","#00d52a","#0054ab","#007b84","#00ff00","#009768","#008778","#00e01f","#00d52a","#00db24","#00a857","#00e619","#005fa0","#00eb14","#00b649","#00f10e"]}
-
-pymol_cmap = matplotlib.colors.ListedColormap(pymol_color_list)
-    
-def show_pdb(pdb_str, show_sidechains=False, show_mainchains=False,
-             color="pLDDT", chains=None, Ls=None, vmin=50, vmax=90,
-             color_HP=False, size=(800,480), hbondCutoff=4.0,
-             animate=False):
-  
-  if chains is None:
-    chains = 1 if Ls is None else len(Ls)
-
-  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js', width=size[0], height=size[1])
-  if animate:
-    view.addModelsAsFrames(pdb_str,'pdb',{'hbondCutoff':hbondCutoff})
-  else:
-    view.addModel(pdb_str,'pdb',{'hbondCutoff':hbondCutoff})
-  if color == "pLDDT":
-    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':vmin,'max':vmax}}})
-  elif color == "rainbow":
-    view.setStyle({'cartoon': {'color':'spectrum'}})
-  elif color == "chain":
-    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):
-       view.setStyle({'chain':chain},{'cartoon': {'color':color}})
-  if show_sidechains:
-    BB = ['C','O','N']
-    HP = ["ALA","GLY","VAL","ILE","LEU","PHE","MET","PRO","TRP","CYS","TYR"]
-    if color_HP:
-      view.addStyle({'and':[{'resn':HP},{'atom':BB,'invert':True}]},
-                    {'stick':{'colorscheme':"yellowCarbon",'radius':0.3}})
-      view.addStyle({'and':[{'resn':HP,'invert':True},{'atom':BB,'invert':True}]},
-                    {'stick':{'colorscheme':"whiteCarbon",'radius':0.3}})
-      view.addStyle({'and':[{'resn':"GLY"},{'atom':'CA'}]},
-                    {'sphere':{'colorscheme':"yellowCarbon",'radius':0.3}})
-      view.addStyle({'and':[{'resn':"PRO"},{'atom':['C','O'],'invert':True}]},
-                    {'stick':{'colorscheme':"yellowCarbon",'radius':0.3}})
-    else:
-      view.addStyle({'and':[{'resn':["GLY","PRO"],'invert':True},{'atom':BB,'invert':True}]},
-                    {'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
-      view.addStyle({'and':[{'resn':"GLY"},{'atom':'CA'}]},
-                    {'sphere':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
-      view.addStyle({'and':[{'resn':"PRO"},{'atom':['C','O'],'invert':True}]},
-                    {'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})  
-  if show_mainchains:
-    BB = ['C','O','N','CA']
-    view.addStyle({'atom':BB},{'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
-  view.zoomTo()
-  if animate: view.animate()
-  return view
-
-def plot_pseudo_3D(xyz, c=None, ax=None, chainbreak=5, Ls=None,
-                   cmap="gist_rainbow", line_w=2.0,
-                   cmin=None, cmax=None, zmin=None, zmax=None,
-                   shadow=0.95):
-
-  def rescale(a, amin=None, amax=None):
-    a = np.copy(a)
-    if amin is None: amin = a.min()
-    if amax is None: amax = a.max()
-    a[a < amin] = amin
-    a[a > amax] = amax
-    return (a - amin)/(amax - amin)
-
-  # make segments and colors for each segment
-  xyz = np.asarray(xyz)
-  if Ls is None:
-    seg = np.concatenate([xyz[:,None],np.roll(xyz,1,0)[:,None]],axis=1)
-    c_seg = np.arange(len(seg))[::-1] if c is None else (c + np.roll(c,1,0))/2
-  else:
-    Ln = 0
-    seg = []
-    c_seg = []
-    for L in Ls:
-      sub_xyz = xyz[Ln:Ln+L]
-      seg.append(np.concatenate([sub_xyz[:,None],np.roll(sub_xyz,1,0)[:,None]],axis=1))
-      if c is not None:
-        sub_c = c[Ln:Ln+L]
-        c_seg.append((sub_c + np.roll(sub_c,1,0))/2)
-      Ln += L
-    seg = np.concatenate(seg,0)
-    c_seg = np.arange(len(seg))[::-1] if c is None else np.concatenate(c_seg,0)
-  
-  # set colors
-  c_seg = rescale(c_seg,cmin,cmax)  
-  if isinstance(cmap, str):
-    if cmap == "gist_rainbow": 
-      c_seg *= 0.75
-    colors = matplotlib.cm.get_cmap(cmap)(c_seg)
-  else:
-    colors = cmap(c_seg)
-  
-  # remove segments that aren't connected
-  seg_len = np.sqrt(np.square(seg[:,0] - seg[:,1]).sum(-1))
-  if chainbreak is not None:
-    idx = seg_len < chainbreak
-    seg = seg[idx]
-    seg_len = seg_len[idx]
-    colors = colors[idx]
-
-  seg_mid = seg.mean(1)
-  seg_xy = seg[...,:2]
-  seg_z = seg[...,2].mean(-1)
-  order = seg_z.argsort()
-
-  # add shade/tint based on z-dimension
-  z = rescale(seg_z,zmin,zmax)[:,None]
-
-  # add shadow (make lines darker if they are behind other lines)
-  seg_len_cutoff = (seg_len[:,None] + seg_len[None,:]) / 2
-  seg_mid_z = seg_mid[:,2]
-  seg_mid_dist = np.sqrt(np.square(seg_mid[:,None] - seg_mid[None,:]).sum(-1))
-  shadow_mask = sigmoid(seg_len_cutoff * 2.0 - seg_mid_dist) * (seg_mid_z[:,None] < seg_mid_z[None,:])
-  np.fill_diagonal(shadow_mask,0.0)
-  shadow_mask = shadow ** shadow_mask.sum(-1,keepdims=True)
-
-  seg_mid_xz = seg_mid[:,:2]
-  seg_mid_xydist = np.sqrt(np.square(seg_mid_xz[:,None] - seg_mid_xz[None,:]).sum(-1))
-  tint_mask = sigmoid(seg_len_cutoff/2 - seg_mid_xydist) * (seg_mid_z[:,None] < seg_mid_z[None,:])
-  np.fill_diagonal(tint_mask,0.0)
-  tint_mask = 1 - tint_mask.max(-1,keepdims=True)
-
-  colors[:,:3] = colors[:,:3] + (1 - colors[:,:3]) * (0.50 * z + 0.50 * tint_mask) / 3
-  colors[:,:3] = colors[:,:3] * (0.20 + 0.25 * z + 0.55 * shadow_mask)
-
-  set_lim = False
-  if ax is None:
-    fig, ax = plt.subplots()
-    fig.set_figwidth(5)
-    fig.set_figheight(5)
-    set_lim = True
-  else:
-    fig = ax.get_figure()
-    if ax.get_xlim() == (0,1):
-      set_lim = True
-      
-  if set_lim:
-    xy_min = xyz[:,:2].min() - line_w
-    xy_max = xyz[:,:2].max() + line_w
-    ax.set_xlim(xy_min,xy_max)
-    ax.set_ylim(xy_min,xy_max)
-
-  ax.set_aspect('equal')
-    
-  # determine linewidths
-  width = fig.bbox_inches.width * ax.get_position().width
-  linewidths = line_w * 72 * width / np.diff(ax.get_xlim())
-
-  lines = mcoll.LineCollection(seg_xy[order], colors=colors[order], linewidths=linewidths,
-                               path_effects=[matplotlib.patheffects.Stroke(capstyle="round")])
-  
-  return ax.add_collection(lines)
-
-def plot_ticks(ax, Ls, Ln=None, add_yticks=False):
-  if Ln is None: Ln = sum(Ls)
-  L_prev = 0
-  for L_i in Ls[:-1]:
-    L = L_prev + L_i
-    L_prev += L_i
-    ax.plot([0,Ln],[L,L],color="black")
-    ax.plot([L,L],[0,Ln],color="black")
-  
-  if add_yticks:
-    ticks = np.cumsum([0]+Ls)
-    ticks = (ticks[1:] + ticks[:-1])/2
-    ax.yticks(ticks,alphabet_list[:len(ticks)])
-
-def make_animation(seq, con=None, xyz=None, plddt=None, pae=None,
-                   losses=None, pos_ref=None, line_w=2.0,
-                   dpi=100, interval=60, color_msa="Taylor",
-                   length=None, align_xyz=True, color_by="plddt", **kwargs):
-
-  def nankabsch(a,b,**kwargs):
-    ok = np.isfinite(a).all(axis=1) & np.isfinite(b).all(axis=1)
-    a,b = a[ok],b[ok]
-    return _np_kabsch(a,b,**kwargs)
-  
-  if xyz is not None:
-    if pos_ref is None:
-      pos_ref = xyz[-1]
-
-    if length is None:
-      L = len(pos_ref)
-      Ls = None
-    elif isinstance(length, list):
-      L = length[0]
-      Ls = length
-    else:
-      L = length
-      Ls = None
-
-    # align to reference
-    if align_xyz:
-        
-      pos_ref_trim = pos_ref[:L]
-      pos_ref_trim_mu = np.nanmean(pos_ref_trim,0)
-      pos_ref_trim = pos_ref_trim - pos_ref_trim_mu
-
-      # align to reference position
-      new_pos = []
-      for x in xyz:
-        x_mu = np.nanmean(x[:L],0)
-        aln = nankabsch(x[:L]-x_mu, pos_ref_trim, use_jax=False)
-        new_pos.append((x-x_mu) @ aln)
-
-      pos = np.array(new_pos)
-
-      # rotate for best view
-      pos_mean = np.concatenate(pos,0)
-      m = np.nanmean(pos_mean,0)
-      rot_mtx = nankabsch(pos_mean - m, pos_mean - m, return_v=True, use_jax=False)
-      pos = (pos - m) @ rot_mtx
-      pos_ref_full = ((pos_ref - pos_ref_trim_mu) - m) @ rot_mtx
-    
-    else:
-      # rotate for best view
-      pos_mean = np.concatenate(xyz,0)
-      m = np.nanmean(pos_mean,0)
-      aln = nankabsch(pos_mean - m, pos_mean - m, return_v=True, use_jax=False)
-      pos = [(x - m) @ aln for x in xyz]
-      pos_ref_full = (pos_ref - m) @ aln
-
-  # initialize figure
-  if pae is not None and len(pae) == 0: pae = None
-  fig = plt.figure()
-  gs = GridSpec(4,3, figure=fig)
-  if pae is not None:
-    ax1, ax2, ax3 = fig.add_subplot(gs[:3,:2]), fig.add_subplot(gs[3:,:]), fig.add_subplot(gs[:3,2:])
-  else:
-    ax1, ax2 = fig.add_subplot(gs[:3,:]), fig.add_subplot(gs[3:,:])
-
-  fig.subplots_adjust(top=0.95,bottom=0.1,right=0.95,left=0.05,hspace=0,wspace=0)
-  fig.set_figwidth(8); fig.set_figheight(6); fig.set_dpi(dpi)
-  ax2.set_xlabel("positions"); ax2.set_yticks([])
-  if seq[0].shape[0] > 1: ax2.set_ylabel("sequences")
-  else: ax2.set_ylabel("amino acids")
-
-  if xyz is None:
-    ax1.set_title("predicted contact map")
-  else:
-    ax1.set_title("N→C") if plddt is None else ax1.set_title("pLDDT")
-  if pae is not None:
-    ax3.set_title("pAE")
-    ax3.set_xticks([])
-    ax3.set_yticks([])
-
-  # set bounderies
-  if xyz is not None:
-    main_pos = pos_ref_full[np.isfinite(pos_ref_full).all(1)]
-    pred_pos = [np.isfinite(x).all(1) for x in pos]
-    x_min,y_min,z_min = np.minimum(np.mean([x.min(0) for x in pred_pos],0),main_pos.min(0)) - 5
-    x_max,y_max,z_max = np.maximum(np.mean([x.max(0) for x in pred_pos],0),main_pos.max(0)) + 5
-
-    x_pad = ((y_max - y_min) * 2 - (x_max - x_min)) / 2
-    y_pad = ((x_max - x_min) / 2 - (y_max - y_min)) / 2
-    if x_pad > 0:
-      x_min -= x_pad
-      x_max += x_pad
-    else:
-      y_min -= y_pad
-      y_max += y_pad
-
-    ax1.set_xlim(x_min, x_max)
-    ax1.set_ylim(y_min, y_max)
-  ax1.set_xticks([])
-  ax1.set_yticks([])
-
-  # get animation frames
-  ims = []
-  for k in range(len(seq)):
-    ims.append([])
-    if xyz is not None:
-      flags = dict(ax=ax1, line_w=line_w, zmin=z_min, zmax=z_max)
-      if color_by == "plddt" and plddt is not None:
-        ims[-1].append(plot_pseudo_3D(pos[k], c=plddt[k], Ls=Ls, cmin=0.5, cmax=0.9, **flags))
-      elif color_by == "chain":
-        c = np.concatenate([[n]*L for n,L in enumerate(length)])
-        ims[-1].append(plot_pseudo_3D(pos[k], c=c,  Ls=Ls, cmap=pymol_cmap, cmin=0, cmax=39, **flags))
-      else:
-        L = pos[k].shape[0]
-        ims[-1].append(plot_pseudo_3D(pos[k], c=np.arange(L)[::-1],  Ls=Ls, cmin=0, cmax=L, **flags))  
-    else:
-      L = con[k].shape[0]
-      ims[-1].append(ax1.imshow(con[k], animated=True, cmap="Greys",vmin=0, vmax=1, extent=(0, L, L, 0)))
-
-    if seq[k].shape[0] == 1:
-      ims[-1].append(ax2.imshow(seq[k][0].T, animated=True, cmap="bwr_r",vmin=-1, vmax=1))
-    else:
-      cmap = matplotlib.colors.ListedColormap(jalview_color_list[color_msa])
-      vmax = len(jalview_color_list[color_msa]) - 1
-      ims[-1].append(ax2.imshow(seq[k].argmax(-1), animated=True, cmap=cmap, vmin=0, vmax=vmax, interpolation="none"))
-    
-    if pae is not None:
-      L = pae[k].shape[0]
-      ims[-1].append(ax3.imshow(pae[k], animated=True, cmap="bwr",vmin=0, vmax=30, extent=(0, L, L, 0)))
-
-  # add lines
-  if length is not None:
-    Ls = length if isinstance(length, list) else [length,None]
-    if con is not None:
-      plot_ticks(ax1, Ls, con[0].shape[0])
-    if pae is not None:
-      plot_ticks(ax3, Ls, pae[0].shape[0])
-
-  # make animation!
-  ani = animation.ArtistAnimation(fig, ims, blit=True, interval=interval)
-  plt.close()
-  return ani.to_html5_video()
\ No newline at end of file
diff --git a/colabdesign/shared/prep.py b/colabdesign/shared/prep.py
deleted file mode 100644
index 437a811..0000000
--- a/colabdesign/shared/prep.py
+++ /dev/null
@@ -1,72 +0,0 @@
-import numpy as np
-def prep_pos(pos, residue, chain):
-  '''
-  given input [pos]itions (a string of segment ranges seperated by comma,
-  for example: "1,3-4,10-15"), return list of indices to constrain.
-  '''
-  residue_set = []
-  chain_set = []
-  len_set = []
-  for idx in pos.split(","):
-    i,j = idx.split("-") if "-" in idx else (idx, None)
-
-    if i.isalpha() and j is None:
-      residue_set += [None]
-      chain_set += [i]
-      len_set += [i]
-    else:
-      # if chain defined
-      if i[0].isalpha():
-        c,i = i[0], int(i[1:])
-      else:
-        c,i = chain[0],int(i)
-      if j is None:
-        j = i
-      else:
-        j = int(j[1:] if j[0].isalpha() else j)
-      residue_set += list(range(i,j+1))
-      chain_set += [c] * (j-i+1)
-      len_set += [j-i+1]
-
-  residue = np.asarray(residue)
-  chain = np.asarray(chain)
-  pos_set = []
-  for i,c in zip(residue_set, chain_set):
-    if i is None:
-      idx = np.where(chain == c)[0]
-      assert len(idx) > 0, f'ERROR: chain {c} not found'
-      pos_set += [n for n in idx]
-      len_set[len_set.index(c)] = len(idx)
-    else:
-      idx = np.where((residue == i) & (chain == c))[0]
-      assert len(idx) == 1, f'ERROR: positions {i} and chain {c} not found'
-      pos_set.append(idx[0])
-
-  return {"residue":np.array(residue_set),
-          "chain":np.array(chain_set),
-          "length":np.array(len_set),
-          "pos":np.asarray(pos_set)}
-
-def rewire(length, order=None, loops=0, offset=0):
-  '''
-  Given a list of segment [length]s, move them around given an [offset], [order] and [loop] lengths.
-  The [order] of the segments and the length of [loops] between segments can be controlled.
-  '''
-  seg_len = [length] if isinstance(length,int) else length
-  num_seg = len(seg_len)
-
-  # define order of segments
-  if order is None: order = list(range(num_seg))
-  assert len(order) == num_seg
-
-  # define loop lengths between segments
-  loop_len = ([loops] * (num_seg - 1)) if isinstance(loops, int) else loops
-  assert len(loop_len) == num_seg - 1
-
-  # get positions we want to restrain/constrain within hallucinated protein 
-  l,new_pos = offset,[]
-  for n,i in enumerate(np.argsort(order)):
-    new_pos.append(l + np.arange(seg_len[i]))
-    if n < num_seg - 1: l += seg_len[i] + loop_len[n] 
-
-  return np.concatenate([new_pos[i] for i in order])
\ No newline at end of file
diff --git a/colabdesign/shared/prng.py b/colabdesign/shared/prng.py
deleted file mode 100644
index b574787..0000000
--- a/colabdesign/shared/prng.py
+++ /dev/null
@@ -1,29 +0,0 @@
-import jax
-
-# adopted from https://github.com/deepmind/alphafold/blob/main/alphafold/model/prng.py
-class SafeKey:
-  """Safety wrapper for PRNG keys."""
-
-  def __init__(self, key):
-    self._key = key
-    self._used = False
-
-  def _assert_not_used(self):
-    if self._used:
-      raise RuntimeError('Random key has been used previously.')
-
-  def get(self):
-    self._assert_not_used()
-    self._used = True
-    return self._key
-
-  def split(self, num_keys=2):
-    self._assert_not_used()
-    self._used = True
-    new_keys = jax.random.split(self._key, num_keys)
-    return jax.tree_map(SafeKey, tuple(new_keys))
-
-  def duplicate(self, num_keys=2):
-    self._assert_not_used()
-    self._used = True
-    return tuple(SafeKey(self._key) for _ in range(num_keys))
diff --git a/colabdesign/shared/protein.py b/colabdesign/shared/protein.py
deleted file mode 100644
index e75d4bb..0000000
--- a/colabdesign/shared/protein.py
+++ /dev/null
@@ -1,288 +0,0 @@
-import jax
-import jax.numpy as jnp
-import numpy as np
-
-from colabdesign.af.alphafold.common import residue_constants
-from string import ascii_uppercase, ascii_lowercase
-alphabet_list = list(ascii_uppercase+ascii_lowercase)
-
-MODRES = {'MSE':'MET','MLY':'LYS','FME':'MET','HYP':'PRO',
-          'TPO':'THR','CSO':'CYS','SEP':'SER','M3L':'LYS',
-          'HSK':'HIS','SAC':'SER','PCA':'GLU','DAL':'ALA',
-          'CME':'CYS','CSD':'CYS','OCS':'CYS','DPR':'PRO',
-          'B3K':'LYS','ALY':'LYS','YCM':'CYS','MLZ':'LYS',
-          '4BF':'TYR','KCX':'LYS','B3E':'GLU','B3D':'ASP',
-          'HZP':'PRO','CSX':'CYS','BAL':'ALA','HIC':'HIS',
-          'DBZ':'ALA','DCY':'CYS','DVA':'VAL','NLE':'LEU',
-          'SMC':'CYS','AGM':'ARG','B3A':'ALA','DAS':'ASP',
-          'DLY':'LYS','DSN':'SER','DTH':'THR','GL3':'GLY',
-          'HY3':'PRO','LLP':'LYS','MGN':'GLN','MHS':'HIS',
-          'TRQ':'TRP','B3Y':'TYR','PHI':'PHE','PTR':'TYR',
-          'TYS':'TYR','IAS':'ASP','GPL':'LYS','KYN':'TRP',
-          'CSD':'CYS','SEC':'CYS'}
-
-def pdb_to_string(pdb_file, chains=None, models=None):
-  '''read pdb file and return as string'''
-
-  if chains is not None:
-    if "," in chains: chains = chains.split(",")
-    if not isinstance(chains,list): chains = [chains]
-  if models is not None:
-    if not isinstance(models,list): models = [models]
-
-  modres = {**MODRES}
-  lines = []
-  seen = []
-  model = 1
-  
-  if "\n" in pdb_file:
-    old_lines = pdb_file.split("\n")
-  else:
-    with open(pdb_file,"rb") as f:
-      old_lines = [line.decode("utf-8","ignore").rstrip() for line in f]  
-  for line in old_lines:
-    if line[:5] == "MODEL":
-      model = int(line[5:])
-    if models is None or model in models:
-      if line[:6] == "MODRES":
-        k = line[12:15]
-        v = line[24:27]
-        if k not in modres and v in residue_constants.restype_3to1:
-          modres[k] = v
-      if line[:6] == "HETATM":
-        k = line[17:20]
-        if k in modres:
-          line = "ATOM  "+line[6:17]+modres[k]+line[20:]
-      if line[:4] == "ATOM":
-        chain = line[21:22]
-        if chains is None or chain in chains:
-          atom = line[12:12+4].strip()
-          resi = line[17:17+3]
-          resn = line[22:22+5].strip()
-          if resn[-1].isalpha(): # alternative atom
-            resn = resn[:-1]
-            line = line[:26]+" "+line[27:]
-          key = f"{model}_{chain}_{resn}_{resi}_{atom}"
-          if key not in seen: # skip alternative placements
-            lines.append(line)
-            seen.append(key)
-      if line[:5] == "MODEL" or line[:3] == "TER" or line[:6] == "ENDMDL":
-        lines.append(line)
-  return "\n".join(lines)
-
-def renum_pdb_str(pdb_str, Ls=None, renum=True, offset=1):
-  if Ls is not None:
-    L_init = 0
-    new_chain = {}
-    for L,c in zip(Ls, alphabet_list):
-      new_chain.update({i:c for i in range(L_init,L_init+L)})
-      L_init += L  
-
-  n,num,pdb_out = 0,offset,[]
-  resnum_ = None
-  chain_ = None
-  new_chain_ = new_chain[0]
-  for line in pdb_str.split("\n"):
-    if line[:4] == "ATOM":
-      chain = line[21:22]
-      resnum = int(line[22:22+5])
-      if resnum_ is None: resnum_ = resnum
-      if chain_ is None: chain_ = chain
-      if resnum != resnum_ or chain != chain_:
-        num += (resnum - resnum_)  
-        n += 1
-        resnum_,chain_ = resnum,chain
-      if Ls is not None:
-        if new_chain[n] != new_chain_:
-          num = offset
-          new_chain_ = new_chain[n]
-      N = num if renum else resnum
-      if Ls is None: pdb_out.append("%s%4i%s" % (line[:22],N,line[26:]))
-      else: pdb_out.append("%s%s%4i%s" % (line[:21],new_chain[n],N,line[26:]))        
-  return "\n".join(pdb_out)
-
-#################################################################################
-
-def _np_len_pw(x, use_jax=True):
-  '''compute pairwise distance'''
-  _np = jnp if use_jax else np
-
-  x_norm = _np.square(x).sum(-1)
-  xx = _np.einsum("...ia,...ja->...ij",x,x)
-  sq_dist = x_norm[...,:,None] + x_norm[...,None,:] - 2 * xx
-
-  # due to precision errors the values can sometimes be negative
-  if use_jax: sq_dist = jax.nn.relu(sq_dist)
-  else: sq_dist[sq_dist < 0] = 0
-
-  # return euclidean pairwise distance matrix
-  return _np.sqrt(sq_dist + 1e-8)
-
-def _np_rmsdist(true, pred, use_jax=True):
-  '''compute RMSD of distance matrices'''
-  _np = jnp if use_jax else np
-  t = _np_len_pw(true, use_jax=use_jax)
-  p = _np_len_pw(pred, use_jax=use_jax)
-  return _np.sqrt(_np.square(t-p).mean() + 1e-8)
-
-def _np_kabsch(a, b, return_v=False, use_jax=True):
-  '''get alignment matrix for two sets of coodinates'''
-  _np = jnp if use_jax else np
-  ab = a.swapaxes(-1,-2) @ b
-  u, s, vh = _np.linalg.svd(ab, full_matrices=False)
-  flip = _np.linalg.det(u @ vh) < 0
-  u_ = _np.where(flip, -u[...,-1].T, u[...,-1].T).T
-  if use_jax: u = u.at[...,-1].set(u_)
-  else: u[...,-1] = u_
-  return u if return_v else (u @ vh)
-
-def _np_rmsd(true, pred, use_jax=True):
-  '''compute RMSD of coordinates after alignment'''
-  _np = jnp if use_jax else np
-  p = true - true.mean(-2,keepdims=True)
-  q = pred - pred.mean(-2,keepdims=True)
-  p = p @ _np_kabsch(p, q, use_jax=use_jax)
-  return _np.sqrt(_np.square(p-q).sum(-1).mean(-1) + 1e-8)
-
-def _np_norm(x, axis=-1, keepdims=True, eps=1e-8, use_jax=True):
-  '''compute norm of vector'''
-  _np = jnp if use_jax else np
-  return _np.sqrt(_np.square(x).sum(axis,keepdims=keepdims) + 1e-8)
-  
-def _np_len(a, b, use_jax=True):
-  '''given coordinates a-b, return length or distance'''
-  return _np_norm(a-b, use_jax=use_jax)
-
-def _np_ang(a, b, c, use_acos=False, use_jax=True):
-  '''given coordinates a-b-c, return angle'''  
-  _np = jnp if use_jax else np
-  norm = lambda x: _np_norm(x, use_jax=use_jax)
-  ba, bc = b-a, b-c
-  cos_ang = (ba * bc).sum(-1,keepdims=True) / (norm(ba) * norm(bc))
-  # note the derivative at acos(-1 or 1) is inf, to avoid nans we use cos(ang)
-  if use_acos: return _np.arccos(cos_ang)
-  else: return cos_ang
-  
-def _np_dih(a, b, c, d, use_atan2=False, standardize=False, use_jax=True):
-  '''given coordinates a-b-c-d, return dihedral'''
-  _np = jnp if use_jax else np
-  normalize = lambda x: x/_np_norm(x, use_jax=use_jax)
-  ab, bc, cd = normalize(a-b), normalize(b-c), normalize(c-d)
-  n1,n2 = _np.cross(ab, bc), _np.cross(bc, cd)
-  sin_ang = (_np.cross(n1, bc) * n2).sum(-1,keepdims=True)
-  cos_ang = (n1 * n2).sum(-1,keepdims=True)
-  if use_atan2:
-    return _np.arctan2(sin_ang, cos_ang)
-  else:
-    angs = _np.concatenate([sin_ang, cos_ang],-1)
-    if standardize: return normalize(angs)
-    else: return angs
-
-def _np_extend(a,b,c, L,A,D, use_jax=True):
-  '''
-  given coordinates a-b-c,
-  c-d (L)ength, b-c-d (A)ngle, and a-b-c-d (D)ihedral
-  return 4th coordinate d
-  '''
-  _np = jnp if use_jax else np
-  normalize = lambda x: x/_np_norm(x, use_jax=use_jax)
-  bc = normalize(b-c)
-  n = normalize(_np.cross(b-a, bc))
-  return c + sum([L * _np.cos(A) * bc,
-                  L * _np.sin(A) * _np.cos(D) * _np.cross(n, bc),
-                  L * _np.sin(A) * _np.sin(D) * -n])
-
-def _np_get_cb(N,CA,C, use_jax=True):
-  '''compute CB placement from N, CA, C'''
-  return _np_extend(C, N, CA, 1.522, 1.927, -2.143, use_jax=use_jax)
-  
-def _np_get_6D(all_atom_positions, all_atom_mask=None, use_jax=True, for_trrosetta=False):
-  '''get 6D features (see TrRosetta paper)'''
-
-  # get CB coordinate
-  atom_idx = {k:residue_constants.atom_order[k] for k in ["N","CA","C"]}
-  out = {k:all_atom_positions[...,i,:] for k,i in atom_idx.items()}
-  out["CB"] = _np_get_cb(**out, use_jax=use_jax)
-  
-  if all_atom_mask is not None:
-    idx = np.fromiter(atom_idx.values(),int)
-    out["CB_mask"] = all_atom_mask[...,idx].prod(-1)
-
-  # get pairwise features
-  N,A,B = (out[k] for k in ["N","CA","CB"])
-  n0 = N[...,:,None,:]
-  a0,a1 = A[...,:,None,:],A[...,None,:,:]
-  b0,b1 = B[...,:,None,:],B[...,None,:,:]
-  
-  if for_trrosetta:
-    out.update({"dist":  _np_len(b0,b1,       use_jax=use_jax),
-                "phi":   _np_ang(a0,b0,b1,    use_jax=use_jax, use_acos=True),
-                "omega": _np_dih(a0,b0,b1,a1, use_jax=use_jax, use_atan2=True),
-                "theta": _np_dih(n0,a0,b0,b1, use_jax=use_jax, use_atan2=True)})  
-  else:
-    out.update({"dist":  _np_len(b0,b1,       use_jax=use_jax),
-                "phi":   _np_ang(a0,b0,b1,    use_jax=use_jax, use_acos=False),
-                "omega": _np_dih(a0,b0,b1,a1, use_jax=use_jax, use_atan2=False),
-                "theta": _np_dih(n0,a0,b0,b1, use_jax=use_jax, use_atan2=False)})  
-  return out
-
-####################
-# losses
-####################
-
-# RMSD
-def jnp_rmsdist(true, pred):
-  return _np_rmsdist(true, pred)
-
-def jnp_rmsd(true, pred, add_dist=False):
-  rmsd = _np_rmsd(true, pred)
-  if add_dist: rmsd = (rmsd + _np_rmsdist(true, pred))/2
-  return rmsd
-
-def jnp_kabsch_w(a, b, weights):
-  return _np_kabsch(a * weights[:,None], b)
-
-def jnp_rmsd_w(true, pred, weights):
-  p = true - (true * weights[:,None]).sum(0,keepdims=True)/weights.sum()
-  q = pred - (pred * weights[:,None]).sum(0,keepdims=True)/weights.sum()
-  p = p @ _np_kabsch(p * weights[:,None], q)
-  return jnp.sqrt((weights*jnp.square(p-q).sum(-1)).sum()/weights.sum() + 1e-8)
-
-# 6D (see TrRosetta paper)
-def _np_get_6D_loss(true, pred, mask=None, use_theta=True, use_dist=False, use_jax=True):
-  _np = jnp if use_jax else np
-
-  f = {"T":_np_get_6D(true, mask, use_jax=use_jax),
-       "P":_np_get_6D(pred, use_jax=use_jax)}
-
-  for k in f: f[k]["dist"] /= 10.0
-
-  keys = ["omega","phi"]
-  if use_theta: keys.append("theta")
-  if use_dist: keys.append("dist")
-  sq_diff = sum([_np.square(f["T"][k]-f["P"][k]).sum(-1) for k in keys])
-
-  mask = _np.ones(true.shape[0]) if mask is None else f["T"]["CB_mask"]
-  mask = mask[:,None] * mask[None,:]
-  loss = (sq_diff * mask).sum((-1,-2)) / mask.sum((-1,-2))
-
-  return _np.sqrt(loss + 1e-8).mean()
-
-def _np_get_6D_binned(all_atom_positions, all_atom_mask, use_jax=None):
-  # TODO: make differentiable, add use_jax option
-  ref = _np_get_6D(all_atom_positions,
-                   all_atom_mask,
-                   use_jax=False, for_trrosetta=True)
-  ref = jax.tree_map(jnp.squeeze,ref)
-
-  def mtx2bins(x_ref, start, end, nbins, mask):
-    bins = np.linspace(start, end, nbins)
-    x_true = np.digitize(x_ref, bins).astype(np.uint8)
-    x_true = np.where(mask,0,x_true)
-    return np.eye(nbins+1)[x_true][...,:-1]
-
-  mask = (ref["dist"] > 20) | (np.eye(ref["dist"].shape[0]) == 1)
-  return {"dist": mtx2bins(ref["dist"],    2.0,  20.0,  37,  mask=mask),
-          "omega":mtx2bins(ref["omega"], -np.pi, np.pi, 25,  mask=mask),
-          "theta":mtx2bins(ref["theta"], -np.pi, np.pi, 25,  mask=mask),
-          "phi":  mtx2bins(ref["phi"],      0.0, np.pi, 13,  mask=mask)}
\ No newline at end of file
diff --git a/colabdesign/shared/utils.py b/colabdesign/shared/utils.py
deleted file mode 100644
index 31670ff..0000000
--- a/colabdesign/shared/utils.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import random
-import jax
-import numpy as np
-import jax.numpy as jnp
-import sys, gc
-
-def clear_mem():
-  # clear vram (GPU)
-  backend = jax.lib.xla_bridge.get_backend()
-  if hasattr(backend,'live_buffers'):
-    for buf in backend.live_buffers():
-      buf.delete()
-
-  # TODO: clear ram (CPU)
-  gc.collect()
-  
-def update_dict(D, *args, **kwargs):
-  '''robust function for updating dictionary'''
-  def set_dict(d, x, override=False):
-    for k,v in x.items():
-      if v is not None:
-        if k in d:
-          if isinstance(v, dict):
-            set_dict(d[k], x[k], override=override)
-          elif override or d[k] is None:
-            d[k] = v
-          elif isinstance(d[k],(np.ndarray,jnp.ndarray)):
-            d[k] = np.asarray(v)
-          elif isinstance(d[k], dict):
-            d[k] = jax.tree_map(lambda x: type(x)(v), d[k])
-          else:
-            d[k] = type(d[k])(v)
-        else:
-          print(f"ERROR: '{k}' not found in {list(d.keys())}")  
-  override = kwargs.pop("override", False)
-  while len(args) > 0 and isinstance(args[0],str):
-    D,args = D[args[0]],args[1:]
-  for a in args:
-    if isinstance(a, dict): set_dict(D, a, override=override)
-  set_dict(D, kwargs, override=override)
-
-def copy_dict(x):
-  '''deepcopy dictionary'''
-  return jax.tree_map(lambda y:y, x)
-
-def to_float(x):
-  '''convert to float'''
-  if hasattr(x,"tolist"): x = x.tolist()
-  if isinstance(x,dict): x = {k:to_float(y) for k,y in x.items()}
-  elif hasattr(x,"__iter__"): x = [to_float(y) for y in x]
-  else: x = float(x)
-  return x
-
-def dict_to_str(x, filt=None, keys=None, ok=None, print_str=None, f=2):
-  '''convert dictionary to string for print out'''  
-  if keys is None: keys = []
-  if filt is None: filt = {}
-  if print_str is None: print_str = ""
-  if ok is None: ok = []
-
-  # gather keys
-  for k in x.keys():
-    if k not in keys:
-      keys.append(k)
-
-  for k in keys:
-    if k in x and (filt.get(k,True) or k in ok):
-      v = x[k]
-      if isinstance(v,float):
-        if int(v) == v:
-          print_str += f" {k} {int(v)}"
-        else:
-          print_str += f" {k} {v:.{f}f}"
-      else:
-        print_str += f" {k} {v}"
-  return print_str
-
-class Key():
-  '''random key generator'''
-  def __init__(self, key=None, seed=None):
-    if key is None:
-      self.seed = random.randint(0,2147483647) if seed is None else seed
-      self.key = jax.random.PRNGKey(self.seed) 
-    else:
-      self.key = key
-  def get(self, num=1):
-    if num > 1:
-      self.key, *sub_keys = jax.random.split(self.key, num=(num+1))
-      return sub_keys
-    else:
-      self.key, sub_key = jax.random.split(self.key)
-      return sub_key
-
-def softmax(x, axis=-1):
-  x = x - x.max(axis,keepdims=True)
-  x = np.exp(x)
-  return x / x.sum(axis,keepdims=True)
-
-def categorical(p):
-  return (p.cumsum(-1) >= np.random.uniform(size=p.shape[:-1])[..., None]).argmax(-1)
-
-def to_list(xs):
-  if not isinstance(xs,list): xs = [xs]
-  return [x for x in xs if x is not None]
-
-def copy_missing(a,b):
-  for i,v in a.items():
-    if i not in b:
-      b[i] = v
-    elif isinstance(v,dict):
-      copy_missing(v,b[i])
\ No newline at end of file
diff --git a/colabdesign/tr/__init__.py b/colabdesign/tr/__init__.py
deleted file mode 100644
index 712bf30..0000000
--- a/colabdesign/tr/__init__.py
+++ /dev/null
@@ -1,14 +0,0 @@
-import os,jax
-# disable triton_gemm for jax versions > 0.3
-if int(jax.__version__.split(".")[1]) > 3:
-  os.environ["XLA_FLAGS"] = "--xla_gpu_enable_triton_gemm=false"
-
-import warnings
-warnings.simplefilter(action='ignore', category=FutureWarning)
-
-from colabdesign.shared.utils import clear_mem
-from colabdesign.tr.model import mk_tr_model
-from colabdesign.tr.joint_model import mk_af_tr_model
-
-# backward compatability
-mk_design_model = mk_trdesign_model = mk_tr_model
\ No newline at end of file
diff --git a/colabdesign/tr/joint_model.py b/colabdesign/tr/joint_model.py
deleted file mode 100644
index 801a7ee..0000000
--- a/colabdesign/tr/joint_model.py
+++ /dev/null
@@ -1,66 +0,0 @@
-from colabdesign.af.model import mk_af_model
-from colabdesign.tr.model import mk_tr_model
-
-class mk_af_tr_model:
-  def __init__(self, protocol="fixbb", use_templates=False,
-               recycle_mode="last", num_recycles=0):
-    assert protocol in ["fixbb","partial","hallucination","binder"]
-    self.af = mk_af_model(protocol=protocol, use_templates=use_templates,
-                          recycle_mode=recycle_mode, num_recycles=num_recycles)
-    
-    if protocol == "binder":
-      def _prep_inputs(pdb_filename, chain, binder_len=50, binder_chain=None,
-                       ignore_missing=True, **kwargs):
-        self.af.prep_inputs(pdb_filename=pdb_filename, chain=chain,
-                            binder_len=binder_len, binder_chain=binder_chain,
-                            ignore_missing=ignore_missing, **kwargs)
-        flags = dict(ignore_missing=ignore_missing)
-        if binder_chain is None:
-          self.tr = mk_tr_model(protocol="hallucination")
-          self.tr.prep_inputs(length=binder_len, **flags)
-        else:
-          self.tr = mk_tr_model(protocol="fixbb")
-          self.tr.prep_inputs(pdb_filename=pdb_filename, chain=binder_chain, **flags)
-    else:
-      self.tr = mk_tr_model(protocol=protocol)
-
-    if protocol == "fixbb":
-      def _prep_inputs(pdb_filename, chain, fix_pos=None, 
-                       ignore_missing=True, **kwargs):
-        flags = dict(pdb_filename=pdb_filename, chain=chain,
-                     fix_pos=fix_pos, ignore_missing=ignore_missing)
-        self.af.prep_inputs(**flags, **kwargs)
-        self.tr.prep_inputs(**flags, chain=chain)
-
-    if protocol == "partial":
-      def _prep_inputs(pdb_filename, chain, pos=None, length=None,
-                       fix_pos=None, use_sidechains=False, atoms_to_exclude=None, 
-                       ignore_missing=True, **kwargs):
-        if use_sidechains: fix_seq = True
-        flags = dict(pdb_filename=pdb_filename, chain=chain, 
-                     length=length, pos=pos, fix_pos=fix_pos,
-                     ignore_missing=ignore_missing)
-        af_a2e = kwargs.pop("af_atoms_to_exclude",atoms_to_exclude)
-        tr_a2e = kwargs.pop("tr_atoms_to_exclude",atoms_to_exclude)
-        self.af.prep_inputs(**flags, use_sidechains=use_sidechains, atoms_to_exclude=af_a2e, **kwargs)
-        self.tr.prep_inputs(**flags, atoms_to_exclude=tr_a2e)
-      
-      def _rewire(order=None, offset=0, loops=0):
-        self.af.rewire(order=order, offset=offset, loops=loops)
-        self.tr.rewire(order=order, offset=offset, loops=loops)
-      
-      self.rewire = _rewire
-
-    if protocol == "hallucintion":
-      def _prep_inputs(length=None, **kwargs):
-        self.af.prep_inputs(length=length, **kwargs)
-        self.tr.prep_inputs(length=length)
-
-    self.prep_inputs = _prep_inputs
-
-  def set_opt(self,*args,**kwargs):
-    self.af.set_opt(*args,**kwargs)
-    self.tr.set_opt(*args,**kwargs)
-
-  def joint_design(self, iters=100, tr_weight=1.0, tr_seed=None, **kwargs):
-    self.af.design(iters, callback=self.tr.af_callback(weight=tr_weight, seed=tr_seed), **kwargs)
diff --git a/colabdesign/tr/legacy/README.md b/colabdesign/tr/legacy/README.md
deleted file mode 100644
index 4efb443..0000000
--- a/colabdesign/tr/legacy/README.md
+++ /dev/null
@@ -1 +0,0 @@
-These are old tensorflow script, we have since switched to jax for better integration with AfDesign!
diff --git a/colabdesign/tr/legacy/model.py b/colabdesign/tr/legacy/model.py
deleted file mode 100644
index 7506738..0000000
--- a/colabdesign/tr/legacy/model.py
+++ /dev/null
@@ -1,233 +0,0 @@
-# supressing warnings
-import warnings, logging, os
-warnings.filterwarnings('ignore',category=FutureWarning)
-logging.disable(logging.WARNING)
-os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
-
-import tensorflow as tf
-tf.compat.v1.disable_eager_execution()
-
-from tr.src.utils import split_feat
-
-def tr_clear_mem():
-  tf.compat.v1.reset_default_graph()
-  tf.compat.v1.keras.backend.clear_session()
-
-def tr_set_mem(frac=0.5):
-  tf_config = tf.compat.v1.ConfigProto()
-  tf_config.gpu_options.per_process_gpu_memory_fraction=frac
-  tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=tf_config))
-
-from tensorflow.keras.models import Model
-from tensorflow.keras.layers import Input, Conv2D, Activation, Dense, Lambda, Layer, Concatenate
-
-import numpy as np
-
-def get_TrR_weights(filename):
-  weights = [np.squeeze(w) for w in np.load(filename, allow_pickle=True)]
-  # remove weights for beta-beta pairing
-  del weights[-4:-2]
-  return weights
-
-def get_TrR(blocks=12, trainable=False, weights=None, name="TrR"):
-  ex = {"trainable":trainable}
-  # custom layer(s)
-  class PSSM(Layer):
-
-    # modified from MRF to only output tiled 1D features
-    def __init__(self, diag=0.4, use_entropy=False):
-      super(PSSM, self).__init__()
-      self.diag = diag
-      self.use_entropy = use_entropy
-    
-    def call(self, inputs):
-      x,y = inputs
-      _,_,L,A = [tf.shape(y)[k] for k in range(4)]
-      with tf.name_scope('1d_features'):
-        # sequence
-        x_i = x[0,0,:,:20]
-        # pssm
-        f_i = y[0,0]
-        # entropy
-        if self.use_entropy:
-          h_i = K.sum(-f_i * K.log(f_i + 1e-8), axis=-1, keepdims=True)
-        else:
-          h_i = tf.zeros((L,1))
-        # tile and combined 1D features
-        feat_1D = tf.concat([x_i,f_i,h_i], axis=-1)
-        feat_1D_tile_A = tf.tile(feat_1D[:,None,:], [1,L,1])
-        feat_1D_tile_B = tf.tile(feat_1D[None,:,:], [L,1,1])
-
-      with tf.name_scope('2d_features'):
-        ic = self.diag * tf.eye(L*A)
-        ic = tf.reshape(ic,(L,A,L,A))
-        ic = tf.transpose(ic,(0,2,1,3))
-        ic = tf.reshape(ic,(L,L,A*A))
-        i0 = tf.zeros([L,L,1])
-        feat_2D = tf.concat([ic,i0], axis=-1)
-
-      feat = tf.concat([feat_1D_tile_A, feat_1D_tile_B, feat_2D],axis=-1)
-      return tf.reshape(feat, [1,L,L,442+2*42])
-      
-  class instance_norm(Layer):
-    def __init__(self, axes=(1,2),trainable=True):
-      super(instance_norm, self).__init__()
-      self.axes = axes
-      self.trainable = trainable
-    def build(self, input_shape):
-      self.beta  = self.add_weight(name='beta',shape=(input_shape[-1],),
-                                  initializer='zeros',trainable=self.trainable)
-      self.gamma = self.add_weight(name='gamma',shape=(input_shape[-1],),
-                                  initializer='ones',trainable=self.trainable)
-    def call(self, inputs):
-      mean, variance = tf.nn.moments(inputs, self.axes, keepdims=True)
-      return tf.nn.batch_normalization(inputs, mean, variance, self.beta, self.gamma, 1e-6)
-
-  ## INPUT ##
-  inputs = Input((None,None,21),batch_size=1)
-  A = PSSM()([inputs,inputs])
-  A = Dense(64, **ex)(A)
-  A = instance_norm(**ex)(A)
-  A = Activation("elu")(A)
-
-  ## RESNET ##
-  def resnet(X, dilation=1, filters=64, win=3):
-    Y = Conv2D(filters, win, dilation_rate=dilation, padding='SAME', **ex)(X)
-    Y = instance_norm(**ex)(Y)
-    Y = Activation("elu")(Y)
-    Y = Conv2D(filters, win, dilation_rate=dilation, padding='SAME', **ex)(Y)
-    Y = instance_norm(**ex)(Y)
-    return Activation("elu")(X+Y)
-
-  for _ in range(blocks):
-    for dilation in [1,2,4,8,16]:
-      A = resnet(A, dilation)
-  A = resnet(A, dilation=1)
-  
-  ## OUTPUT ##
-  A_input   = Input((None,None,64))
-  p_theta   = Dense(25, activation="softmax", **ex)(A_input)
-  p_phi     = Dense(13, activation="softmax", **ex)(A_input)
-  A_sym     = Lambda(lambda x: (x + tf.transpose(x,[0,2,1,3]))/2)(A_input)
-  p_dist    = Dense(37, activation="softmax", **ex)(A_sym)
-  p_omega   = Dense(25, activation="softmax", **ex)(A_sym)
-  A_model   = Model(A_input,Concatenate()([p_theta,p_phi,p_dist,p_omega]))
-
-  ## MODEL ##
-  model = Model(inputs, A_model(A),name=name)
-  if weights is not None: model.set_weights(weights)
-  return model
-
-def get_TrR_model(protocol="fixbb", L=None, num_models=1, hard=True, use_theta=True):
-
-  def gather_idx(x):
-    idx = x[1][0]
-    return tf.gather(tf.gather(x[0],idx,axis=-2),idx,axis=-3)
-
-  def get_cce_loss(x, eps=1e-8):
-    if use_theta:
-      loss = -tf.reduce_sum(x[0]*tf.math.log(x[1] + eps),-1)
-      loss = tf.reduce_mean(loss)/4
-    else:
-      # remove theta
-      true_x = split_feat(x[0])
-      pred_x = split_feat(x[1])
-      true_x = tf.concat([true_x[k] for k in ["phi","dist","omega"]],-1)
-      pred_x = tf.concat([pred_x[k] for k in ["phi","dist","omega"]],-1)
-      loss = -tf.reduce_sum(true_x*tf.math.log(pred_x + eps),-1)
-      loss = tf.reduce_mean(loss)/3
-    return loss[None]
-  
-  def get_bkg_loss(x, eps=1e-8):
-    loss = -tf.reduce_sum(x[1]*(tf.math.log(x[1]+eps)-tf.math.log(x[0]+eps)),-1)
-    loss = tf.reduce_mean(loss)/4
-    return loss[None]
-
-  def prep_seq(x_logits):
-    x_soft = tf.nn.softmax(x_logits,-1)
-    if hard:
-      x_hard = tf.one_hot(tf.argmax(x_logits,-1),20)
-      x = tf.stop_gradient(x_hard - x_soft) + x_soft
-    else:
-      x = x_soft
-    x = tf.pad(x,[[0,0],[0,0],[0,1]])
-    return x[None]
-
-  I_seq_logits = Input((L,20),name="seq_logits")
-  seq = Lambda(prep_seq,name="seq")(I_seq_logits)
-  
-  if protocol in ["fixbb","partial"]:
-    I_true = Input((L,L,100),name="true")
-
-  if protocol in ["partial","hallucination"]:
-    I_bkg = Input((L,L,100),name="bkg")
-  
-  if protocol in ["partial"]:
-    I_idx = Input((None,),dtype=tf.int32,name="idx")
-    I_idx_true = Input((None,),dtype=tf.int32,name="idx_true")
-  
-  # TODO
-  pred = []
-  for nam in ["xaa","xab","xac","xad","xae"][:num_models]:
-    print(nam)
-    TrR = get_TrR(weights=get_TrR_weights(f"models/model_{nam}.npy"),name=nam)
-    pred.append(TrR(seq))
-  pred = sum(pred)/len(pred)
-
-  if protocol in ["partial"]:
-    pred_sub = Lambda(gather_idx, name="pred_sub")([pred,I_idx])
-    true_sub = Lambda(gather_idx, name="true_sub")([I_true,I_idx_true])
-    cce_loss = Lambda(get_cce_loss,name="cce_loss")([true_sub, pred_sub])
-  
-  if protocol in ["fixbb"]:
-    cce_loss = Lambda(get_cce_loss,name="cce_loss")([I_true, pred])
-  
-  if protocol in ["hallucination","partial"]:
-    bkg_loss = Lambda(get_bkg_loss,name="bkg_loss")([I_bkg, pred])
-
-  # define model, loss and gradients
-  inputs = [I_seq_logits]
-  outputs = []
-  if protocol == "partial":
-    inputs += [I_true, I_bkg, I_idx, I_idx_true]
-    outputs += [cce_loss, bkg_loss]
-    loss = Lambda(lambda x: x[0]+0.1*x[1])([cce_loss,bkg_loss])
-  if protocol == "hallucination":
-    inputs += [I_bkg]
-    outputs += [bkg_loss]
-    loss = bkg_loss
-  if protocol == "fixbb":
-    inputs += [I_true]
-    outputs += [cce_loss]
-    loss = cce_loss
-
-  grad = Lambda(lambda x: tf.gradients(x[0],x[1]), name="grad")([loss,I_seq_logits])  
-  outputs += [grad, pred]
-  model = Model(inputs, outputs, name="TrR_model")
-  
-  def _fixbb_model(seq, true):
-    cce_loss, grad, pred = model.predict([seq[None],true[None]])
-    return {"cce_loss":cce_loss[0],
-            "grad":grad[0],
-            "pred":pred[0]}
-  
-  def _hallucination_model(seq, bkg):
-    bkg_loss, grad, pred = model.predict([seq[None],bkg[None]])
-    return {"bkg_loss":bkg_loss[0],
-            "grad":grad[0],
-            "pred":pred[0]}
-
-  def _partial_model(seq, true, bkg, pos_idx, pos_idx_ref=None):
-    if pos_idx_ref is None: pos_idx_ref = pos_idx
-    cce_loss, bkg_loss, grad, pred = model.predict([seq[None],true[None],bkg[None],pos_idx[None],pos_idx_ref[None]])
-    return {"cce_loss":cce_loss[0],
-            "bkg_loss":bkg_loss[0],
-            "grad":grad[0],
-            "pred":pred[0]}
-  
-  if protocol == "fixbb":
-    return _fixbb_model
-  if protocol == "hallucination":
-    return _hallucination_model
-  if protocol == "partial":
-    return _partial_model
diff --git a/colabdesign/tr/legacy/utils.py b/colabdesign/tr/legacy/utils.py
deleted file mode 100644
index 7139897..0000000
--- a/colabdesign/tr/legacy/utils.py
+++ /dev/null
@@ -1,314 +0,0 @@
-# load libraries
-import numpy as np
-import string, sys, getopt
-
-DB_DIR = "/home/krypton/projects/TrR_for_design" # location of databases
-
-# ivan's natural AA composition
-AA_COMP = np.array([0.07892653, 0.04979037, 0.0451488 , 0.0603382 , 0.01261332,
-                    0.03783883, 0.06592534, 0.07122109, 0.02324815, 0.05647807,
-                    0.09311339, 0.05980368, 0.02072943, 0.04145316, 0.04631926,
-                    0.06123779, 0.0547427 , 0.01489194, 0.03705282, 0.0691271])
-
-# David Juergens' optimized AA reference weights
-# /home/norn/DL/200701_ref_weight_optimization/nelder_mead/scripts/nm_filtered/params_140
-AA_REF = np.array([-1.31161863, -0.44993051,  0.06198913, -0.81825899,  2.63941964,
-                    0.44087343, -0.93833546, -0.7374156 ,  1.54108622, -0.92757075,
-                   -1.70878817, -0.9461753 ,  1.77794612,  0.2156388 ,  0.3293717 ,
-                   -1.012154  , -0.60176806,  2.99381739,  0.84557686, -1.02749264])
-
-alpha_1 = list("ARNDCQEGHILKMFPSTWYV-")
-states = len(alpha_1)
-alpha_3 = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE',
-           'LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL','GAP']
-
-aa_1_N = {a:n for n,a in enumerate(alpha_1)}
-aa_3_N = {a:n for n,a in enumerate(alpha_3)}
-aa_N_1 = {n:a for n,a in enumerate(alpha_1)}
-aa_1_3 = {a:b for a,b in zip(alpha_1,alpha_3)}
-aa_3_1 = {b:a for a,b in zip(alpha_1,alpha_3)}
-
-def AA_to_N(x):
-  # ["ARND"] -> [[0,1,2,3]]
-  x = np.array(x);
-  if x.ndim == 0: x = x[None]
-  return [[aa_1_N.get(a, states-1) for a in y] for y in x]
-
-def N_to_AA(x):
-  # [[0,1,2,3]] -> ["ARND"]
-  x = np.array(x);
-  if x.ndim == 1: x = x[None]
-  return ["".join([aa_N_1.get(a,"-") for a in y]) for y in x]
-
-def parse_PDB(x, atoms=['N','CA','C'], chain=None):
-  '''
-  input:  x = PDB filename
-          atoms = atoms to extract (optional)
-  output: (length, atoms, coords=(x,y,z)), sequence
-  '''
-  xyz,seq,min_resn,max_resn = {},{},np.inf,-np.inf
-  for line in open(x,"rb"):
-    line = line.decode("utf-8","ignore").rstrip()
-
-    if line[:6] == "HETATM" and line[17:17+3] == "MSE":
-      line = line.replace("HETATM","ATOM  ")
-      line = line.replace("MSE","MET")
-
-    if line[:4] == "ATOM":
-      ch = line[21:22]
-      if ch == chain or chain is None:
-        atom = line[12:12+4].strip()
-        resi = line[17:17+3]
-        resn = line[22:22+5].strip()
-        x,y,z = [float(line[i:(i+8)]) for i in [30,38,46]]
-
-        if resn[-1].isalpha(): resa,resn = resn[-1],int(resn[:-1])-1
-        else: resa,resn = "",int(resn)-1
-        if resn < min_resn: min_resn = resn
-        if resn > max_resn: max_resn = resn
-        if resn not in xyz: xyz[resn] = {}
-        if resa not in xyz[resn]: xyz[resn][resa] = {}
-        if resn not in seq: seq[resn] = {}
-        if resa not in seq[resn]: seq[resn][resa] = resi
-
-        if atom not in xyz[resn][resa]:
-          xyz[resn][resa][atom] = np.array([x,y,z])
-
-  # convert to numpy arrays, fill in missing values
-  seq_,xyz_ = [],[]
-  for resn in range(min_resn,max_resn+1):
-    if resn in seq:
-      for k in sorted(seq[resn]): seq_.append(aa_3_N.get(seq[resn][k],20))
-    else: seq_.append(20)
-    if resn in xyz:
-      for k in sorted(xyz[resn]):
-        for atom in atoms:
-          if atom in xyz[resn][k]: xyz_.append(xyz[resn][k][atom])
-          else: xyz_.append(np.full(3,np.nan))
-    else:
-      for atom in atoms: xyz_.append(np.full(3,np.nan))
-  return np.array(xyz_).reshape(-1,len(atoms),3), np.array(seq_)
-
-def extend(a,b,c, L,A,D):
-  '''
-  input:  3 coords (a,b,c), (L)ength, (A)ngle, and (D)ihedral
-  output: 4th coord
-  '''
-  N = lambda x: x/np.sqrt(np.square(x).sum(-1,keepdims=True) + 1e-8)
-  bc = N(b-c)
-  n = N(np.cross(b-a, bc))
-  m = [bc,np.cross(n,bc),n]
-  d = [L*np.cos(A), L*np.sin(A)*np.cos(D), -L*np.sin(A)*np.sin(D)]
-  return c + sum([m*d for m,d in zip(m,d)])
-
-def to_len(a,b):
-  '''given coordinates a-b, return length or distance'''
-  return np.sqrt(np.sum(np.square(a-b),axis=-1))
-
-def to_len_pw(a,b=None):
-  '''given coordinates a-b return pairwise distance matrix'''
-  a_norm = np.square(a).sum(-1)
-  if b is None: b,b_norm = a,a_norm
-  else: b_norm = np.square(b).sum(-1)
-  return np.sqrt(np.abs(a_norm.reshape(-1,1) + b_norm - 2*(a@b.T)))
-
-def to_ang(a,b,c):
-  '''given coordinates a-b-c, return angle'''
-  D = lambda x,y: np.sum(x*y,axis=-1)
-  N = lambda x: x/np.sqrt(np.square(x).sum(-1,keepdims=True) + 1e-8)
-  return np.arccos(D(N(b-a),N(b-c)))
-
-def to_dih(a,b,c,d):
-  '''given coordinates a-b-c-d, return dihedral'''
-  D = lambda x,y: np.sum(x*y,axis=-1)
-  N = lambda x: x/np.sqrt(np.square(x).sum(-1,keepdims=True) + 1e-8)
-  bc = N(b-c)
-  n1 = np.cross(N(a-b),bc)
-  n2 = np.cross(bc,N(c-d))
-  return np.arctan2(D(np.cross(n1,bc),n2),D(n1,n2))
-
-def prep_input(pdb, chain=None, mask_gaps=False):
-  '''Parse PDB file and return features compatible with TrRosetta'''
-  ncac, seq = parse_PDB(pdb,["N","CA","C"], chain=chain)
-
-  # mask gap regions
-  if mask_gaps:
-    mask = seq != 20
-    ncac, seq = ncac[mask], seq[mask]
-
-  N,CA,C = ncac[:,0], ncac[:,1], ncac[:,2]
-  CB = extend(C, N, CA, 1.522, 1.927, -2.143)
-
-  dist_ref  = to_len(CB[:,None], CB[None,:])
-  omega_ref = to_dih(CA[:,None], CB[:,None], CB[None,:], CA[None,:])
-  theta_ref = to_dih( N[:,None], CA[:,None], CB[:,None], CB[None,:])
-  phi_ref   = to_ang(CA[:,None], CB[:,None], CB[None,:])
-
-  def mtx2bins(x_ref, start, end, nbins, mask):
-    bins = np.linspace(start, end, nbins)
-    x_true = np.digitize(x_ref, bins).astype(np.uint8)
-    x_true[mask] = 0
-    return np.eye(nbins+1)[x_true][...,:-1]
-
-  p_dist  = mtx2bins(dist_ref,     2.0,  20.0, 37, mask=(dist_ref > 20))
-  p_omega = mtx2bins(omega_ref, -np.pi, np.pi, 25, mask=(p_dist[...,0]==1))
-  p_theta = mtx2bins(theta_ref, -np.pi, np.pi, 25, mask=(p_dist[...,0]==1))
-  p_phi   = mtx2bins(phi_ref,      0.0, np.pi, 13, mask=(p_dist[...,0]==1))
-  feat    = np.concatenate([p_theta, p_phi, p_dist, p_omega],-1)
-  return {"seq":N_to_AA(seq), "feat":feat, "dist_ref":dist_ref}
-
-def split_feat(feat):
-  out = {}
-  for k,i,j in [["theta",0,25],["phi",25,38],["dist",38,75],["omega",75,100]]:
-    out[k] = feat[...,i:j]
-  return out
-
-def pairwise_id(x):
-  '''get pairwise sequence identity'''
-  x = np.array(x)
-  return (x[:,None] == x[None,:]).mean(-1)
-
-def arr2str(x, d=3):
-  return np.array2string(x,formatter={'float_kind':lambda x: f"%.{d}f" % x}).replace("\n","").replace(" ",",")
-
-#####################################################################
-# Working with multiple sequence alignments
-#####################################################################
-
-def parse_fasta(filename, a3m=False):
-  '''function to parse fasta file'''
-  if a3m:
-    # for a3m files the lowercase letters are removed
-    # as these do not align to the query sequence
-    rm_lc = str.maketrans(dict.fromkeys(string.ascii_lowercase))
-  header, sequence = [],[]
-  lines = open(filename, "r")
-  for line in lines:
-    line = line.rstrip()
-    if len(line) > 0:
-      if line[0] == ">":
-        header.append(line[1:])
-        sequence.append([])
-      else:
-        if a3m: line = line.translate(rm_lc)
-        else: line = line.upper()
-        sequence[-1].append(line)
-  lines.close()
-  sequence = [''.join(seq) for seq in sequence]
-  return header, sequence
-
-def mk_msa(seqs):
-  '''one hot encode msa'''
-  alphabet = list("ARNDCQEGHILKMFPSTWYV-")
-  states = len(alphabet)
-
-  alpha = np.array(alphabet, dtype='|S1').view(np.uint8)
-  msa = np.array([list(s) for s in seqs], dtype='|S1').view(np.uint8)
-  for n in range(states):
-    msa[msa == alpha[n]] = n
-  msa[msa > states] = states-1
-
-  return np.eye(states)[msa]
-
-def get_dist_acc(pred, true, true_mask=None,sep=5,eps=1e-8):
-  ## compute accuracy of CB features ##
-  pred,true = [x[...,39:51].sum(-1) for x in[pred,true]]
-  if true_mask is not None:
-    mask = true_mask[:,:,None] * true_mask[:,None,:]
-  else: mask = np.ones_like(pred)
-  i,j = np.triu_indices(pred.shape[-1],k=sep)
-  P,T,M = pred[...,i,j], true[...,i,j], mask[...,i,j]
-  ## give equal weighting to positive and negative predictions
-  pos = (T*P*M).sum(-1)/((M*T).sum(-1)+eps)
-  neg = ((1-T)*(1-P)*M).sum(-1)/((M*(1-T)).sum(-1)+eps)
-  return 2.0*(pos*neg)/(pos+neg+eps)
-
-def inv_cov(Y):
-  '''given MSA, return contacts'''
-
-  N,L = Y.shape
-  K = Y.max()+1
-  Y = np.eye(K)[Y]
-
-  # flatten msa (N,L,A) -> (N,L*A)
-  Y_flat = Y.reshape(N,-1)
-
-  # compute covariance matrix (L*A,L*A)
-  c = np.cov(Y_flat.T)
-  # compute shrinkage (l2 regularization)
-  shrink = 4.5/np.sqrt(N) * np.eye(c.shape[0])
-  # take the inverse to solve for w
-  ic = np.linalg.inv(c + shrink)
-  # (L,A,L,A)
-  ic = ic.reshape(L,K,L,K)
-
-  # take l2norm to reduce (L,A,L,A) to (L,L) matrix
-  ic_norm = np.sqrt(np.square(ic).sum((1,3)))
-  np.fill_diagonal(ic_norm,0)
-
-  #Average product correction (aka remove largest eigenvector)
-  ap = ic_norm.sum(0)
-  apc = ic_norm - (ap[:,None]*ap[None,:])/ap.sum()
-  np.fill_diagonal(apc,0.0)
-  return apc
-
-def to_dict(label, var_list):
-  return dict(zip(label,var_list))
-
-def to_list(label, var_dict, default=None):
-  return [var_dict.get(k, default) for k in label]
-
-# class for parsing arguments
-class parse_args:
-  def __init__(self):
-    self.long,self.short = [],[]
-    self.info,self.help = [],[]
-
-  def txt(self,help):
-    self.help.append(["txt",help])
-
-  def add(self, arg, default, type, help=None):
-    self.long.append(arg[0])
-    key = arg[0].replace("=","")
-    self.info.append({"key":key, "type":type,
-                      "value":default, "arg":[f"--{key}"]})
-    if len(arg) == 2:
-      self.short.append(arg[1])
-      s_key = arg[1].replace(":","")
-      self.info[-1]["arg"].append(f"-{s_key}")
-    if help is not None:
-      self.help.append(["opt",[arg,help]])
-
-  def parse(self,argv):
-    for opt, arg in getopt.getopt(argv,"".join(self.short),self.long)[0]:
-      for x in self.info:
-        if opt in x["arg"]:
-          if x["type"] is None: x["value"] = (x["value"] == False)
-          else: x["value"] = x["type"](arg)
-
-    opts = {x["key"]:x["value"] for x in self.info}
-    print(str(opts).replace(" ",""))
-    return dict2obj(opts)
-
-  def usage(self, err):
-    for type,info in self.help:
-      if type == "txt": print(info)
-      if type == "opt":
-        arg, helps = info
-        help = helps[0]
-        if len(arg) == 1: print("--%-15s : %s"     % (arg[0],help))
-        if len(arg) == 2: print("--%-10s -%-3s : %s" % (arg[0],arg[1].replace(":",""),help))
-        for help in helps[1:]: print("%19s %s" % ("",help))
-    print(f"< {err} >")
-    print(" "+"-"*(len(err)+2))
-    print("        \   ^__^               ")
-    print("         \  (oo)\_______       ")
-    print("            (__)\       )\/\   ")
-    print("                ||----w |      ")
-    print("                ||     ||      ")
-    sys.exit()
-
-class dict2obj():
-  def __init__(self, dictionary):
-    for key in dictionary:
-      setattr(self, key, dictionary[key])
diff --git a/colabdesign/tr/model.py b/colabdesign/tr/model.py
deleted file mode 100644
index 8262c92..0000000
--- a/colabdesign/tr/model.py
+++ /dev/null
@@ -1,341 +0,0 @@
-import random, os
-import numpy as np
-import jax
-import jax.numpy as jnp
-import matplotlib.pyplot as plt
-
-from colabdesign.shared.utils import copy_dict, update_dict, Key, dict_to_str
-from colabdesign.shared.prep import prep_pos
-from colabdesign.shared.protein import _np_get_6D_binned
-from colabdesign.shared.model import design_model, soft_seq
-
-from .trrosetta import TrRosetta, get_model_params
-
-# borrow some stuff from AfDesign
-from colabdesign.af.prep import prep_pdb
-from colabdesign.af.alphafold.common import protein
-
-class mk_tr_model(design_model):
-  def __init__(self, protocol="fixbb", num_models=1,
-               sample_models=True, data_dir="params/tr",
-               optimizer="sgd", learning_rate=0.1,
-               loss_callback=None):
-    
-    assert protocol in ["fixbb","hallucination","partial"]
-
-    self.protocol = protocol
-    self._data_dir = "." if os.path.isfile(os.path.join("models",f"model_xaa.npy")) else data_dir
-    self._loss_callback = loss_callback
-    self._num = 1
-
-    # set default options
-    self.opt = {"temp":1.0, "soft":1.0, "hard":1.0, "dropout":False,
-                "num_models":num_models,"sample_models":sample_models,
-                "weights":{}, "lr":1.0, "alpha":1.0,
-                "learning_rate":learning_rate, "use_pssm":False,
-                "norm_seq_grad":True}
-                
-    self._args = {"optimizer":optimizer}
-    self._params = {}
-    self._inputs = {}
-
-    # setup model
-    self._model = self._get_model()
-    self._model_params = []
-    for k in list("abcde"):
-      p = os.path.join(self._data_dir,os.path.join("models",f"model_xa{k}.npy"))
-      self._model_params.append(get_model_params(p))
-
-    if protocol in ["hallucination","partial"]:
-      self._bkg_model = TrRosetta(bkg_model=True)
-  
-  def _get_model(self):
-    runner = TrRosetta()    
-    def _get_loss(inputs, outputs):
-      opt = inputs["opt"]
-      aux = {"outputs":outputs, "losses":{}}
-      log_p = jax.tree_map(jax.nn.log_softmax, outputs)
-
-      # bkg loss
-      if self.protocol in ["hallucination","partial"]:
-        p = jax.tree_map(jax.nn.softmax, outputs)
-        log_q = jax.tree_map(jax.nn.log_softmax, inputs["6D_bkg"])
-        aux["losses"]["bkg"] = {}
-        for k in ["dist","omega","theta","phi"]:
-          aux["losses"]["bkg"][k] = -(p[k]*(log_p[k]-log_q[k])).sum(-1).mean()
-
-      # cce loss
-      if self.protocol in ["fixbb","partial"]:
-        if "pos" in opt:
-          pos = opt["pos"]
-          log_p = jax.tree_map(lambda x:x[:,pos][pos,:], log_p)
-
-        q = inputs["6D"]
-        aux["losses"]["cce"] = {}
-        for k in ["dist","omega","theta","phi"]:
-          aux["losses"]["cce"][k] = -(q[k]*log_p[k]).sum(-1).mean()
-
-      if self._loss_callback is not None:
-        aux["losses"].update(self._loss_callback(outputs))
-
-      # weighted loss
-      w = opt["weights"]
-      tree_multi = lambda x,y: jax.tree_map(lambda a,b:a*b, x,y)
-      losses = {k:(tree_multi(v,w[k]) if k in w else v) for k,v in aux["losses"].items()}
-      loss = sum(jax.tree_leaves(losses))
-      return loss, aux
-
-    def _model(params, model_params, inputs, key):
-      inputs["params"] = params
-      opt = inputs["opt"]
-      seq = soft_seq(params["seq"], inputs["bias"], opt)
-      if "fix_pos" in opt:
-        if "pos" in self.opt:
-          seq_ref = jax.nn.one_hot(inputs["batch"]["aatype_sub"],20)
-          p = opt["pos"][opt["fix_pos"]]
-          fix_seq = lambda x:x.at[...,p,:].set(seq_ref)
-        else:
-          seq_ref = jax.nn.one_hot(inputs["batch"]["aatype"],20)
-          p = opt["fix_pos"]
-          fix_seq = lambda x:x.at[...,p,:].set(seq_ref[...,p,:])
-        seq = jax.tree_map(fix_seq, seq)
-
-      inputs.update({"seq":seq["pseudo"][0],
-                     "prf":jnp.where(opt["use_pssm"],seq["pssm"],seq["pseudo"])[0]})
-      rate = jnp.where(opt["dropout"],0.15,0.0)
-      outputs = runner(inputs, model_params, key, rate)
-      loss, aux = _get_loss(inputs, outputs)
-      aux.update({"seq":seq,"opt":opt})
-      return loss, aux
-
-    return {"grad_fn":jax.jit(jax.value_and_grad(_model, has_aux=True, argnums=0)),
-            "fn":jax.jit(_model)}
-  
-  def prep_inputs(self, pdb_filename=None, chain=None, length=None,
-                  pos=None, fix_pos=None, atoms_to_exclude=None, ignore_missing=True,
-                  **kwargs):
-    '''
-    prep inputs for TrDesign
-    '''    
-    if self.protocol in ["fixbb", "partial"]:
-      # parse PDB file and return features compatible with TrRosetta
-      pdb = prep_pdb(pdb_filename, chain, ignore_missing=ignore_missing)
-      self._inputs["batch"] = pdb["batch"]
-
-      if fix_pos is not None:
-        self.opt["fix_pos"] = prep_pos(fix_pos, **pdb["idx"])["pos"]
-      
-      if self.protocol == "partial" and pos is not None:
-        self._pos_info = prep_pos(pos, **pdb["idx"])
-        p = self._pos_info["pos"]
-        aatype = self._inputs["batch"]["aatype"]
-        self._inputs["batch"] = jax.tree_map(lambda x:x[p], self._inputs["batch"])
-        self.opt["pos"] = p
-        if "fix_pos" in self.opt:
-          sub_i,sub_p = [],[]
-          p = p.tolist()
-          for i in self.opt["fix_pos"].tolist():
-            if i in p:
-              sub_i.append(i)
-              sub_p.append(p.index(i))
-          self.opt["fix_pos"] = np.array(sub_p)
-          self._inputs["batch"]["aatype_sub"] = aatype[sub_i]
-
-      self._inputs["6D"] = _np_get_6D_binned(self._inputs["batch"]["all_atom_positions"],
-                                             self._inputs["batch"]["all_atom_mask"])
-
-      self._len = len(self._inputs["batch"]["aatype"])
-      self.opt["weights"]["cce"] = {"dist":1/6,"omega":1/6,"theta":2/6,"phi":2/6}
-      if atoms_to_exclude is not None:
-        if "N" in atoms_to_exclude:
-          # theta = [N]-CA-CB-CB
-          self.opt["weights"]["cce"] = dict(dist=1/4,omega=1/4,phi=1/2,theta=0)
-        if "CA" in atoms_to_exclude:
-          # theta = N-[CA]-CB-CB
-          # omega = [CA]-CB-CB-[CA]
-          # phi = [CA]-CB-CB
-          self.opt["weights"]["cce"] = dict(dist=1,omega=0,phi=0,theta=0)
-
-    if self.protocol in ["hallucination", "partial"]:
-      # compute background distribution
-      if length is not None: self._len = length
-      self._inputs["6D_bkg"] = []
-      key = jax.random.PRNGKey(0)
-      for n in range(1,6):
-        p = os.path.join(self._data_dir,os.path.join("bkgr_models",f"bkgr0{n}.npy"))
-        self._inputs["6D_bkg"].append(self._bkg_model(get_model_params(p), key, self._len))
-      self._inputs["6D_bkg"] = jax.tree_map(lambda *x:np.stack(x).mean(0), *self._inputs["6D_bkg"])
-
-      # reweight the background
-      self.opt["weights"]["bkg"] = dict(dist=1/6,omega=1/6,phi=2/6,theta=2/6)
-
-
-    self._opt = copy_dict(self.opt)
-    self.restart(**kwargs)
-
-  def set_opt(self, *args, **kwargs):
-    '''
-    set [opt]ions
-    -------------------
-    note: model.restart() resets the [opt]ions to their defaults
-    use model.set_opt(..., set_defaults=True) 
-    or model.restart(..., reset_opt=False) to avoid this
-    -------------------    
-    model.set_opt(num_models=1)
-    model.set_opt(con=dict(num=1)) or set_opt({"con":{"num":1}})
-    model.set_opt(lr=1, set_defaults=True)
-    '''
-    if kwargs.pop("set_defaults", False):
-      update_dict(self._opt, *args, **kwargs)
-
-    update_dict(self.opt, *args, **kwargs)
-
-  
-  def restart(self, seed=None, opt=None, weights=None,
-              seq=None, reset_opt=True, **kwargs):
-
-    if reset_opt:
-      self.opt = copy_dict(self._opt)
-
-    self.set_opt(opt)
-    self.set_weights(weights)
-    self.set_seed(seed)
-    
-    # set sequence
-    self.set_seq(seq, **kwargs)
-    
-    # setup optimizer
-    self._k = 0
-    self.set_optimizer()
-
-    # clear previous best
-    self._tmp = {"best":{}}
-    
-  def run(self, backprop=True):
-    '''run model to get outputs, losses and gradients'''
-    
-    # decide which model params to use
-    ns = np.arange(5)
-    m = min(self.opt["num_models"],len(ns))
-    if self.opt["sample_models"] and m != len(ns):
-      model_num = np.random.choice(ns,(m,),replace=False)
-    else:
-      model_num = ns[:m]
-    model_num = np.array(model_num).tolist()
-
-    # run in serial
-    aux_all = []
-    for n in model_num:
-      model_params = self._model_params[n]
-      self._inputs["opt"] = self.opt
-      flags = [self._params, model_params, self._inputs, self.key()]
-      if backprop:
-        (loss,aux),grad = self._model["grad_fn"](*flags)
-      else:
-        loss,aux = self._model["fn"](*flags)
-        grad = jax.tree_map(np.zeros_like, self._params)
-      aux.update({"loss":loss, "grad":grad})
-      aux_all.append(aux)
-    
-    # average results
-    self.aux = jax.tree_map(lambda *x:np.stack(x).mean(0), *aux_all)
-    self.aux["model_num"] = model_num
-
-
-  def step(self, backprop=True, callback=None, save_best=True, verbose=1):
-    self.run(backprop=backprop)
-    if callback is not None: callback(self)
-
-    # modify gradients    
-    if self.opt["norm_seq_grad"]: self._norm_seq_grad()
-    self._state, self.aux["grad"] = self._optimizer(self._state, self.aux["grad"], self._params)
-
-    # apply gradients
-    lr = self.opt["learning_rate"]
-    self._params = jax.tree_map(lambda x,g:x-lr*g, self._params, self.aux["grad"])
-
-    # increment
-    self._k += 1
-
-    # save results
-    if save_best:
-      if "aux" not in self._tmp["best"] or self.aux["loss"] < self._tmp["best"]["aux"]["loss"]:
-        self._tmp["best"]["aux"] = self.aux
-
-    # print
-    if verbose and (self._k % verbose) == 0:
-      x = self.get_loss(get_best=False)
-      x["models"] = self.aux["model_num"]
-      print(dict_to_str(x, print_str=f"{self._k}", keys=["models"]))
-
-  def predict(self, seq=None, models=0):
-    self.set_opt(dropout=False)
-    if seq is not None:
-      self.set_seq(seq=seq, set_state=False)
-    self.run(backprop=False)
-
-  def design(self, iters=100, opt=None, weights=None, save_best=True, verbose=1):
-    self.set_opt(opt)
-    self.set_weights(weights)
-    for _ in range(iters):
-      self.step(save_best=save_best, verbose=verbose)
-  
-  def plot(self, mode="preds", dpi=100, get_best=True):
-    '''plot predictions'''
-
-    assert mode in ["preds","feats","bkg_feats"]
-    if mode == "preds":
-      aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
-      x = aux["outputs"]
-    elif mode == "feats":
-      x = self._inputs["6D"]
-    elif mode == "bkg_feats":
-      x = self._inputs["6D_bkg"]
-
-    x = jax.tree_map(np.asarray, x)
-
-    plt.figure(figsize=(4*4,4), dpi=dpi)
-    for n,k in enumerate(["theta","phi","dist","omega"]):
-      v = x[k]
-      plt.subplot(1,4,n+1)
-      plt.title(k)
-      plt.imshow(v.argmax(-1),cmap="binary")
-    plt.show()
-    
-  def get_loss(self, k=None, get_best=True):
-    aux = self._tmp["best"]["aux"] if (get_best and "aux" in self._tmp["best"]) else self.aux
-    if k is None:
-      return {k:self.get_loss(k, get_best=get_best) for k in aux["losses"].keys()}
-    losses = aux["losses"][k]
-    weights = aux["opt"]["weights"][k]
-    weighted_losses = jax.tree_map(lambda l,w:l*w, losses, weights)
-    return float(sum(jax.tree_leaves(weighted_losses)))
-    
-  def af_callback(self, weight=1.0, seed=None):
-    
-    def callback(af_model):      
-      # copy [opt]ions from afdesign
-      for k,v in af_model.opt.items():
-        if k in self.opt and k not in ["weights"]:
-          self.opt[k] = af_model.opt[k]
-
-      # update sequence input
-      self._params["seq"] = af_model._params["seq"]
-      
-      # run trdesign
-      self.run(backprop = weight > 0)
-      
-      # add gradients
-      af_model.aux["grad"]["seq"] += weight * self.aux["grad"]["seq"]
-      
-      # add loss
-      af_model.aux["loss"] += weight * self.aux["loss"]
-        
-      # for verbose printout
-      if self.protocol in ["hallucination","partial"]:
-        af_model.aux["losses"]["TrD_bkg"] = self.get_loss("bkg", get_best=False)
-      if self.protocol in ["fixbb","partial"]:
-        af_model.aux["losses"]["TrD_cce"] = self.get_loss("cce", get_best=False)
-      
-    self.restart(seed=seed)
-    return callback
\ No newline at end of file
diff --git a/colabdesign/tr/trrosetta.py b/colabdesign/tr/trrosetta.py
deleted file mode 100644
index 8a27b8d..0000000
--- a/colabdesign/tr/trrosetta.py
+++ /dev/null
@@ -1,115 +0,0 @@
-import jax.numpy as jnp
-import jax
-import numpy as np
-
-def TrRosetta(bkg_model=False):  
-  
-  def pseudo_mrf(inputs, prf=None):
-    '''single sequence'''
-    seq,prf = inputs["seq"],inputs["prf"]
-    L,A = seq.shape[0],21
-    if prf.shape[1] == 20:
-      prf = jnp.pad(prf,[[0,0],[0,1]])
-    
-    # 1D features
-    x_1D = jnp.concatenate([seq, prf],-1)
-    x_1D = jnp.pad(x_1D,[[0,0],[0,1]])
-    x_1D = jnp.repeat(x_1D[None],L,0)
-
-    # 2D features
-    x_2D = jnp.diag(jnp.full(L*A,0.4))
-    x_2D = x_2D.reshape(L,A,L,A).swapaxes(1,2).reshape(L,L,-1)
-    x_2D = jnp.pad(x_2D,[[0,0],[0,0],[0,1]])
-    return jnp.concatenate([x_1D.swapaxes(0,1), x_1D, x_2D],-1)
-  
-  # layers
-  def instance_norm(x, params):
-    mu = x.mean((0,1),keepdims=True)
-    var = x.var((0,1),keepdims=True)
-    inv = jax.lax.rsqrt(var + 1e-6) * params["scale"]
-    return x * inv + params["offset"] - mu * inv
-
-  def conv_2D(x, params, dilation=1, stride=1, padding="SAME"):
-    flags = dict(window_strides=(stride,stride),
-                 rhs_dilation=(dilation,dilation),
-                 padding=padding)
-    x = x.transpose([2,0,1])
-    f = params["filters"].transpose([3,2,0,1])
-    x = jax.lax.conv_general_dilated(x[None], f, **flags)[0]
-    x = x.transpose([1,2,0])
-    return x + params["bias"]
-
-  def dense(x, params):
-    return x @ params["filters"] + params["bias"]
-
-  def dropout(x, key, rate):
-    keep_rate = 1.0 - rate
-    keep = jax.random.bernoulli(key, keep_rate, shape=x.shape)
-    return keep * x / keep_rate
-  
-  # meta layers
-  def encoder(x, params):
-    x = dense(x, params)
-    x = instance_norm(x, params)
-    return jax.nn.elu(x)
-      
-  def block(x, params, dilation, key, rate=0.15):
-    y = x
-    for n in [0,1]:
-      if n == 1: y = dropout(y, key, rate)
-      p = jax.tree_map(lambda x:x[n], params)
-      y = conv_2D(y, p, dilation)
-      y = instance_norm(y, p)
-      y = jax.nn.elu(y if n == 0 else (x+y))
-    return y  
-  
-  def resnet(x, params, key, rate=0.15):
-    def body(prev, sub_params):
-      (x,key) = prev
-      for n, dilation in enumerate([1,2,4,8,16]):
-        key, sub_key = jax.random.split(key)
-        p = jax.tree_map(lambda x:x[n], sub_params)
-        x = block(x, p, dilation, sub_key, rate)
-      return (x,key), None
-    return jax.lax.scan(body,(x,key),params)[0][0]
-  
-  def heads(x, params):
-    o = {k:dense(x,params[k]) for k in ["theta","phi"]}
-    x = (x + x.swapaxes(0,1)) / 2  
-    o.update({k:dense(x,params[k]) for k in ["dist","bb","omega"]})
-    return o    
-  
-  def trunk(x, params, key, rate=0.15):
-    key, sub_key = jax.random.split(key)
-    x = encoder(x, params["encoder"])
-    x = resnet(x, params["resnet"], sub_key, rate)
-    x = block(x, params["block"], 1, key, rate)  
-    return heads(x, params)
-  
-  # decide which model to use
-  if bkg_model:
-    def model(params, key, length=100):
-      key, sub_key = jax.random.split(key)
-      x = jax.random.normal(sub_key, (length, length, 64))
-      return trunk(x, params, key, 0.0)
-    return jax.jit(model, static_argnums=2)
-  else:
-    def model(inputs, params, key, rate=0.15):
-      x = pseudo_mrf(inputs)
-      return trunk(x, params, key, rate)
-    return jax.jit(model)
-
-def get_model_params(npy):
-  '''parse TrRosetta params into dictionary'''
-  xaa = np.load(npy,allow_pickle=True).tolist()
-  layers = ["encoder","resnet","block","theta","phi","dist","bb","omega"]
-  num = np.array([4,0,8,2,2,2,2,2])
-  num[1] = len(xaa) - num.sum()
-  idx = np.cumsum(num) - num
-  def split(params):
-    labels = ["filters","bias","offset","scale"]
-    steps = min(len(params),len(labels))
-    return {labels[n]:np.squeeze(params[n::steps]) for n in range(steps)}
-  params = {k:split(xaa[i:i+n]) for k,i,n in zip(layers,idx,num)}
-  params["resnet"] = jax.tree_map(lambda x:x.reshape(-1,5,2,*x.shape[1:]), params["resnet"])
-  return params 
\ No newline at end of file
diff --git a/setup.py b/setup.py
index 15ca9a0..6eeccc0 100644
--- a/setup.py
+++ b/setup.py
@@ -6,10 +6,22 @@ setup(
     long_description="Making Protein Design accessible to all via Google Colab!",
     long_description_content_type='text/markdown',
     packages=find_packages(include=['colabdesign*']),
-    install_requires=['py3Dmol','absl-py','biopython',
-                      'chex','dm-haiku','dm-tree',
-                      'immutabledict','jax','ml-collections',
-                      'numpy','pandas','scipy','optax','joblib',
-                      'matplotlib'],
-    include_package_data=True
+    install_requires=[
+    'py3Dmol==2.4.2',
+    'absl-py==2.2.2',
+    'biopython==1.85',
+    'chex==0.1.89',
+    'dm-haiku==0.0.13',
+    'dm-tree==0.1.9',
+    'immutabledict==4.2.1',
+    'jax==0.5.3',
+    'jaxlib==0.5.3',  # <-- ADD THIS LINE
+    'ml-collections==1.1.0',
+    'numpy==1.26.0',
+    'pandas==2.2.3',
+    'scipy==1.15.2',
+    'optax==0.2.4',
+    'joblib==1.4.2'
+],
+	include_package_data=True
 )
